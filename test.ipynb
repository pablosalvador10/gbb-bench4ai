{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory C:\\Users\\pablosal\\Desktop\\gbb-ai-upgrade does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the target directory (change yours)\n",
    "TARGET_DIRECTORY = r\"C:\\Users\\pablosal\\Desktop\\gbb-ai-upgrade\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(TARGET_DIRECTORY):\n",
    "    # Change the current working directory\n",
    "    os.chdir(TARGET_DIRECTORY)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {TARGET_DIRECTORY} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.performance.latencytest import AzureOpenAIBenchmarkStreaming, AzureOpenAIBenchmarkNonStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access the environment variables using os.getenv\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "DEPLOYMENT_ID = os.getenv(\"AZURE_AOAI_CHAT_MODEL_NAME_DEPLOYMENT_ID\")\n",
    "DEPLOYMENT_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_streaming = AzureOpenAIBenchmarkStreaming(api_key=OPENAI_API_KEY, \n",
    "                                                    azure_endpoint=AZURE_OPENAI_ENDPOINT, \n",
    "                                                    api_version=DEPLOYMENT_VERSION)\n",
    "\n",
    "benchmark_non_streaming = AzureOpenAIBenchmarkNonStreaming(api_key=OPENAI_API_KEY, \n",
    "                                                    azure_endpoint=AZURE_OPENAI_ENDPOINT, \n",
    "                                                    api_version=DEPLOYMENT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.performance.utils import read_prompts_from_csv_with_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = read_prompts_from_csv_with_pandas(r\"C:\\Users\\pablosal\\Desktop\\gbb-ai-upgrade-llm\\utils\\data\\contextual_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 13:56:33,978 - micro - MainProcess - INFO     Split list into 4 parts. (utils.py:split_list_into_variable_parts:165)\n",
      "2024-06-29 13:56:33,981 - micro - MainProcess - INFO     CPU usage: 11.9% (utils.py:log_system_info:233)\n",
      "2024-06-29 13:56:33,993 - micro - MainProcess - INFO     RAM usage: 88.1% (utils.py:log_system_info:235)\n",
      "2024-06-29 13:56:34,004 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "2024-06-29 13:56:34,009 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "2024-06-29 13:56:34,458 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687394.4587574 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687394.4587574 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:56:34,464 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:34,467 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:34.467470, (GMT): 2024-06-29 18:56:34.467470+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:34.467470, (GMT): 2024-06-29 18:56:34.467470+00:00\n",
      "2024-06-29 13:56:34,972 - micro - MainProcess - INFO     CPU usage: 19.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.4%\n",
      "2024-06-29 13:56:34,982 - micro - MainProcess - INFO     RAM usage: 88.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.4%\n",
      "2024-06-29 13:56:35,011 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:35,019 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,024 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.0242708 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.0242708 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:56:35,031 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,036 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:35.036024, (GMT): 2024-06-29 18:56:35.036024+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:35.036024, (GMT): 2024-06-29 18:56:35.036024+00:00\n",
      "2024-06-29 13:56:35,039 - micro - MainProcess - INFO     CPU usage: 22.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 22.6%\n",
      "2024-06-29 13:56:35,051 - micro - MainProcess - INFO     RAM usage: 88.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.4%\n",
      "2024-06-29 13:56:35,072 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:35,078 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,083 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.0834787 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.0834787 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:56:35,089 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,092 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:35.092627, (GMT): 2024-06-29 18:56:35.092627+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:35.092627, (GMT): 2024-06-29 18:56:35.092627+00:00\n",
      "2024-06-29 13:56:35,095 - micro - MainProcess - INFO     CPU usage: 8.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 8.3%\n",
      "2024-06-29 13:56:35,111 - micro - MainProcess - INFO     RAM usage: 88.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.4%\n",
      "2024-06-29 13:56:35,132 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:35,141 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,145 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.1440096 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.1440096 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:56:35,149 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,152 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:35.152619, (GMT): 2024-06-29 18:56:35.152619+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:35.152619, (GMT): 2024-06-29 18:56:35.152619+00:00\n",
      "2024-06-29 13:56:35,156 - micro - MainProcess - INFO     CPU usage: 21.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 21.3%\n",
      "2024-06-29 13:56:35,170 - micro - MainProcess - INFO     RAM usage: 88.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.5%\n",
      "2024-06-29 13:56:35,192 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:35,199 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,208 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.2087638 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.2087638 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:56:35,223 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,235 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.235308, (GMT): 2024-06-29 18:56:35.235308+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.235308, (GMT): 2024-06-29 18:56:35.235308+00:00\n",
      "2024-06-29 13:56:35,275 - micro - MainProcess - INFO     CPU usage: 45.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 45.7%\n",
      "2024-06-29 13:56:35,289 - micro - MainProcess - INFO     RAM usage: 88.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.6%\n",
      "2024-06-29 13:56:35,335 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:35,342 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,347 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.3462417 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.3462417 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:56:35,353 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,357 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.356065, (GMT): 2024-06-29 18:56:35.356065+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.356065, (GMT): 2024-06-29 18:56:35.356065+00:00\n",
      "2024-06-29 13:56:35,361 - micro - MainProcess - INFO     CPU usage: 40.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 40.2%\n",
      "2024-06-29 13:56:35,375 - micro - MainProcess - INFO     RAM usage: 88.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.5%\n",
      "2024-06-29 13:56:35,394 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:35,399 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,406 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.4050272 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.4050272 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:56:35,412 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,414 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.414369, (GMT): 2024-06-29 18:56:35.414369+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.414369, (GMT): 2024-06-29 18:56:35.414369+00:00\n",
      "2024-06-29 13:56:35,418 - micro - MainProcess - INFO     CPU usage: 2.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 2.5%\n",
      "2024-06-29 13:56:35,429 - micro - MainProcess - INFO     RAM usage: 88.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.4%\n",
      "2024-06-29 13:56:35,451 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:35,456 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,461 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.460812 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687395.460812 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:56:35,466 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:35,469 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.469475, (GMT): 2024-06-29 18:56:35.469475+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:35.469475, (GMT): 2024-06-29 18:56:35.469475+00:00\n",
      "2024-06-29 13:56:42,239 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:42,253 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:42,257 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:42,268 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.91 seconds or 6908.55 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.91 seconds or 6908.55 milliseconds.\n",
      "2024-06-29 13:56:42,979 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:42,986 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:42,990 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.75 seconds or 7751.27 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.75 seconds or 7751.27 milliseconds.\n",
      "2024-06-29 13:56:43,035 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:43,038 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.57 seconds or 7566.77 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.57 seconds or 7566.77 milliseconds.\n",
      "2024-06-29 13:56:43,088 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:43,092 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.67 seconds or 7674.16 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.67 seconds or 7674.16 milliseconds.\n",
      "2024-06-29 13:56:43,141 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:43,985 - micro - MainProcess - INFO     CPU usage: 8.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 8.4%\n",
      "2024-06-29 13:56:43,996 - micro - MainProcess - INFO     RAM usage: 88.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.3%\n",
      "2024-06-29 13:56:44,048 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:44,055 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,058 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.0580375 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 78 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.0580375 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 78\n",
      "2024-06-29 13:56:44,064 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,067 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.067847, (GMT): 2024-06-29 18:56:44.067847+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.067847, (GMT): 2024-06-29 18:56:44.067847+00:00\n",
      "2024-06-29 13:56:44,073 - micro - MainProcess - INFO     CPU usage: 25.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.4%\n",
      "2024-06-29 13:56:44,087 - micro - MainProcess - INFO     RAM usage: 88.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.3%\n",
      "2024-06-29 13:56:44,105 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:44,112 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,119 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.119585 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.119585 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:56:44,126 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,129 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.129595, (GMT): 2024-06-29 18:56:44.129595+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.129595, (GMT): 2024-06-29 18:56:44.129595+00:00\n",
      "2024-06-29 13:56:44,137 - micro - MainProcess - INFO     CPU usage: 18.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 18.4%\n",
      "2024-06-29 13:56:44,150 - micro - MainProcess - INFO     RAM usage: 88.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.3%\n",
      "2024-06-29 13:56:44,167 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:44,173 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,178 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.178543 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.178543 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:56:44,183 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,186 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.186667, (GMT): 2024-06-29 18:56:44.186667+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.186667, (GMT): 2024-06-29 18:56:44.186667+00:00\n",
      "2024-06-29 13:56:44,192 - micro - MainProcess - INFO     CPU usage: 12.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.5%\n",
      "2024-06-29 13:56:44,203 - micro - MainProcess - INFO     RAM usage: 88.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.3%\n",
      "2024-06-29 13:56:44,270 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:44,280 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,283 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.2836838 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687404.2836838 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:56:44,292 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:44,297 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.297972, (GMT): 2024-06-29 18:56:44.297972+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:44.297972, (GMT): 2024-06-29 18:56:44.297972+00:00\n",
      "2024-06-29 13:56:45,129 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:45,156 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:45,162 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.69 seconds or 10691.58 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.69 seconds or 10691.58 milliseconds.\n",
      "2024-06-29 13:56:45,242 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:45,248 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:45,253 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:45,260 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.1 seconds or 10102.4 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.1 seconds or 10102.4 milliseconds.\n",
      "2024-06-29 13:56:45,326 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:45,331 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.24 seconds or 10235.16 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.24 seconds or 10235.16 milliseconds.\n",
      "2024-06-29 13:56:45,452 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:45,459 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.42 seconds or 10417.49 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 10.42 seconds or 10417.49 milliseconds.\n",
      "2024-06-29 13:56:45,508 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:46,236 - micro - MainProcess - INFO     CPU usage: 21.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 21.0%\n",
      "2024-06-29 13:56:46,248 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:56:46,278 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:46,295 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,302 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.3028412 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.3028412 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:56:46,311 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,315 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.315142, (GMT): 2024-06-29 18:56:46.315142+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.315142, (GMT): 2024-06-29 18:56:46.315142+00:00\n",
      "2024-06-29 13:56:46,328 - micro - MainProcess - INFO     CPU usage: 45.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 45.8%\n",
      "2024-06-29 13:56:46,342 - micro - MainProcess - INFO     RAM usage: 87.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.4%\n",
      "2024-06-29 13:56:46,368 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:46,376 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,381 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.3808894 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.3808894 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:56:46,391 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,395 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.395170, (GMT): 2024-06-29 18:56:46.395170+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.395170, (GMT): 2024-06-29 18:56:46.395170+00:00\n",
      "2024-06-29 13:56:46,466 - micro - MainProcess - INFO     CPU usage: 36.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 36.6%\n",
      "2024-06-29 13:56:46,491 - micro - MainProcess - INFO     RAM usage: 87.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.5%\n",
      "2024-06-29 13:56:46,568 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:46,574 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,579 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.5794444 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.5794444 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:56:46,584 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,588 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.588974, (GMT): 2024-06-29 18:56:46.588974+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.588974, (GMT): 2024-06-29 18:56:46.588974+00:00\n",
      "2024-06-29 13:56:46,606 - micro - MainProcess - INFO     CPU usage: 79.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 79.5%\n",
      "2024-06-29 13:56:46,626 - micro - MainProcess - INFO     RAM usage: 87.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.5%\n",
      "2024-06-29 13:56:46,659 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:46,666 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,672 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.6709077 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 78 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687406.6709077 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 78\n",
      "2024-06-29 13:56:46,678 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:46,683 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.683342, (GMT): 2024-06-29 18:56:46.683342+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:46.683342, (GMT): 2024-06-29 18:56:46.683342+00:00\n",
      "2024-06-29 13:56:47,266 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:47,297 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.23 seconds or 3225.28 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.23 seconds or 3225.28 milliseconds.\n",
      "2024-06-29 13:56:47,365 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:47,645 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:47,656 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:47,666 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:47,674 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.54 seconds or 3541.68 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.54 seconds or 3541.68 milliseconds.\n",
      "2024-06-29 13:56:47,734 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:47,743 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.44 seconds or 3438.53 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.44 seconds or 3438.53 milliseconds.\n",
      "2024-06-29 13:56:47,792 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:47,796 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.6 seconds or 3604.73 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.6 seconds or 3604.73 milliseconds.\n",
      "2024-06-29 13:56:47,860 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:48,366 - micro - MainProcess - INFO     CPU usage: 27.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 27.2%\n",
      "2024-06-29 13:56:48,377 - micro - MainProcess - INFO     RAM usage: 86.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.9%\n",
      "2024-06-29 13:56:48,397 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:48,404 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:48,409 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687408.4086394 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 64 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687408.4086394 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 64\n",
      "2024-06-29 13:56:48,417 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:48,422 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:48.421184, (GMT): 2024-06-29 18:56:48.422179+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:48.421184, (GMT): 2024-06-29 18:56:48.422179+00:00\n",
      "2024-06-29 13:56:48,732 - micro - MainProcess - INFO     CPU usage: 15.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.7%\n",
      "2024-06-29 13:56:48,745 - micro - MainProcess - INFO     RAM usage: 87.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.2%\n",
      "2024-06-29 13:56:48,771 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:48,788 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:48,794 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687408.794077 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 79 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687408.794077 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 79\n",
      "2024-06-29 13:56:48,805 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:48,808 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:48.808641, (GMT): 2024-06-29 18:56:48.808641+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:48.808641, (GMT): 2024-06-29 18:56:48.808641+00:00\n",
      "2024-06-29 13:56:48,818 - micro - MainProcess - INFO     CPU usage: 46.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 46.5%\n",
      "2024-06-29 13:56:48,836 - micro - MainProcess - INFO     RAM usage: 87.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.2%\n",
      "2024-06-29 13:56:48,905 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:48,911 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:48,924 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687408.9238877 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687408.9238877 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:56:48,931 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:48,940 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:48.940685, (GMT): 2024-06-29 18:56:48.940685+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:48.940685, (GMT): 2024-06-29 18:56:48.940685+00:00\n",
      "2024-06-29 13:56:48,988 - micro - MainProcess - INFO     CPU usage: 93.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 93.7%\n",
      "2024-06-29 13:56:49,112 - micro - MainProcess - INFO     RAM usage: 87.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.3%\n",
      "2024-06-29 13:56:49,134 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:49,141 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:49,147 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687409.1468697 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687409.1468697 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:56:49,155 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:49,158 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:49.158063, (GMT): 2024-06-29 18:56:49.158063+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:49.158063, (GMT): 2024-06-29 18:56:49.158063+00:00\n",
      "2024-06-29 13:56:52,940 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:52,979 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.55 seconds or 4552.87 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.55 seconds or 4552.87 milliseconds.\n",
      "2024-06-29 13:56:53,048 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:53,054 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:53,060 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.74 seconds or 6740.06 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.74 seconds or 6740.06 milliseconds.\n",
      "2024-06-29 13:56:53,118 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:53,731 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:53,755 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:53,766 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.37 seconds or 7365.3 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.37 seconds or 7365.3 milliseconds.\n",
      "2024-06-29 13:56:53,821 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:53,828 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:53,834 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:53,840 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.89 seconds or 4894.73 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.89 seconds or 4894.73 milliseconds.\n",
      "2024-06-29 13:56:53,888 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:53,892 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.73 seconds or 4730.68 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.73 seconds or 4730.68 milliseconds.\n",
      "2024-06-29 13:56:53,939 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:53,944 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.13 seconds or 5130.67 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.13 seconds or 5130.67 milliseconds.\n",
      "2024-06-29 13:56:53,990 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:54,064 - micro - MainProcess - INFO     CPU usage: 13.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.3%\n",
      "2024-06-29 13:56:54,076 - micro - MainProcess - INFO     RAM usage: 87.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.5%\n",
      "2024-06-29 13:56:54,127 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:54,136 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,142 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.1426969 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.1426969 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:56:54,149 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,159 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:54.159288, (GMT): 2024-06-29 18:56:54.159288+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:54.159288, (GMT): 2024-06-29 18:56:54.159288+00:00\n",
      "2024-06-29 13:56:54,166 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:54,176 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.58 seconds or 7580.21 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.58 seconds or 7580.21 milliseconds.\n",
      "2024-06-29 13:56:54,246 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:54,249 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:54,254 - micro - MainProcess - INFO     CPU usage: 68.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 68.1%\n",
      "2024-06-29 13:56:54,267 - micro - MainProcess - INFO     RAM usage: 87.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.5%\n",
      "2024-06-29 13:56:54,285 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:54,292 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,299 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.2978377 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 80 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.2978377 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 80\n",
      "2024-06-29 13:56:54,306 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,310 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:54.310463, (GMT): 2024-06-29 18:56:54.310463+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:54.310463, (GMT): 2024-06-29 18:56:54.310463+00:00\n",
      "2024-06-29 13:56:54,315 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.63 seconds or 7626.81 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.63 seconds or 7626.81 milliseconds.\n",
      "2024-06-29 13:56:54,366 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:54,835 - micro - MainProcess - INFO     CPU usage: 15.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.6%\n",
      "2024-06-29 13:56:54,851 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:56:54,893 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:54,903 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,909 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.909834 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.909834 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:56:54,917 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,919 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:54.919416, (GMT): 2024-06-29 18:56:54.919416+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:54.919416, (GMT): 2024-06-29 18:56:54.919416+00:00\n",
      "2024-06-29 13:56:54,927 - micro - MainProcess - INFO     CPU usage: 13.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.5%\n",
      "2024-06-29 13:56:54,941 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:56:54,971 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:54,978 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,984 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.9830167 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 62 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687414.9830167 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 62\n",
      "2024-06-29 13:56:54,990 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:54,992 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:54.992903, (GMT): 2024-06-29 18:56:54.992903+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:54.992903, (GMT): 2024-06-29 18:56:54.992903+00:00\n",
      "2024-06-29 13:56:54,995 - micro - MainProcess - INFO     CPU usage: 23.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.1%\n",
      "2024-06-29 13:56:55,009 - micro - MainProcess - INFO     RAM usage: 87.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.7%\n",
      "2024-06-29 13:56:55,028 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:55,035 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,040 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.0403376 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.0403376 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:56:55,049 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,053 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:55.053778, (GMT): 2024-06-29 18:56:55.053778+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:55.053778, (GMT): 2024-06-29 18:56:55.053778+00:00\n",
      "2024-06-29 13:56:55,127 - micro - MainProcess - INFO     CPU usage: 42.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 42.6%\n",
      "2024-06-29 13:56:55,143 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:56:55,163 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:55,173 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,180 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.1789982 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.1789982 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:56:55,186 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,190 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:55.189775, (GMT): 2024-06-29 18:56:55.189775+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:55.189775, (GMT): 2024-06-29 18:56:55.189775+00:00\n",
      "2024-06-29 13:56:55,222 - micro - MainProcess - INFO     CPU usage: 53.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 53.2%\n",
      "2024-06-29 13:56:55,236 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:56:55,253 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:55,258 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,262 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.262393 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.262393 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:56:55,268 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,270 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:55.270515, (GMT): 2024-06-29 18:56:55.270515+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:55.270515, (GMT): 2024-06-29 18:56:55.270515+00:00\n",
      "2024-06-29 13:56:55,353 - micro - MainProcess - INFO     CPU usage: 9.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 9.4%\n",
      "2024-06-29 13:56:55,363 - micro - MainProcess - INFO     RAM usage: 87.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.8%\n",
      "2024-06-29 13:56:55,392 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:55,397 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,402 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.4028604 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 64 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687415.4028604 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 64\n",
      "2024-06-29 13:56:55,409 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:55,411 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:55.411333, (GMT): 2024-06-29 18:56:55.411333+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:56:55.411333, (GMT): 2024-06-29 18:56:55.411333+00:00\n",
      "2024-06-29 13:56:57,409 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:57,439 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.28 seconds or 3276.35 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.28 seconds or 3276.35 milliseconds.\n",
      "2024-06-29 13:56:57,491 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:58,437 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:58,469 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.27 seconds or 3272.84 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.27 seconds or 3272.84 milliseconds.\n",
      "2024-06-29 13:56:58,546 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:58,552 - micro - MainProcess - INFO     CPU usage: 15.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.4%\n",
      "2024-06-29 13:56:58,563 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:56:58,697 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:58,704 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:58,708 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687418.7075555 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687418.7075555 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:56:58,714 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:58,721 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:58.721330, (GMT): 2024-06-29 18:56:58.721330+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:58.721330, (GMT): 2024-06-29 18:56:58.721330+00:00\n",
      "2024-06-29 13:56:58,726 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:58,732 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.67 seconds or 3673.4 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.67 seconds or 3673.4 milliseconds.\n",
      "2024-06-29 13:56:58,829 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:59,383 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:56:59,415 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.42 seconds or 4418.92 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.42 seconds or 4418.92 milliseconds.\n",
      "2024-06-29 13:56:59,472 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:56:59,561 - micro - MainProcess - INFO     CPU usage: 37.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 37.1%\n",
      "2024-06-29 13:56:59,574 - micro - MainProcess - INFO     RAM usage: 87.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.5%\n",
      "2024-06-29 13:56:59,601 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:59,613 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:59,620 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687419.6192904 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687419.6192904 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:56:59,629 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:59,652 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:59.652568, (GMT): 2024-06-29 18:56:59.652568+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:59.652568, (GMT): 2024-06-29 18:56:59.652568+00:00\n",
      "2024-06-29 13:56:59,835 - micro - MainProcess - INFO     CPU usage: 50.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 50.5%\n",
      "2024-06-29 13:56:59,849 - micro - MainProcess - INFO     RAM usage: 87.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.4%\n",
      "2024-06-29 13:56:59,871 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:56:59,878 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:59,886 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687419.8862855 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687419.8862855 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:56:59,892 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:56:59,897 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:59.897316, (GMT): 2024-06-29 18:56:59.897316+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:56:59.897316, (GMT): 2024-06-29 18:56:59.897316+00:00\n",
      "2024-06-29 13:57:00,217 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:00,223 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.91 seconds or 5908.24 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.91 seconds or 5908.24 milliseconds.\n",
      "2024-06-29 13:57:00,274 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:00,483 - micro - MainProcess - INFO     CPU usage: 37.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 37.8%\n",
      "2024-06-29 13:57:00,494 - micro - MainProcess - INFO     RAM usage: 87.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.5%\n",
      "2024-06-29 13:57:00,546 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:00,554 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:00,567 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687420.5672305 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687420.5672305 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:00,575 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:00,579 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:00.579777, (GMT): 2024-06-29 18:57:00.579777+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:00.579777, (GMT): 2024-06-29 18:57:00.579777+00:00\n",
      "2024-06-29 13:57:01,004 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:01,010 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.6 seconds or 5595.09 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.6 seconds or 5595.09 milliseconds.\n",
      "2024-06-29 13:57:01,059 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:01,063 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:01,091 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.82 seconds or 5816.3 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.82 seconds or 5816.3 milliseconds.\n",
      "2024-06-29 13:57:01,143 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:01,276 - micro - MainProcess - INFO     CPU usage: 35.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 35.8%\n",
      "2024-06-29 13:57:01,289 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:57:01,309 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:01,317 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:01,325 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687421.3246195 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687421.3246195 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:01,331 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:01,335 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:01.335174, (GMT): 2024-06-29 18:57:01.335174+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:01.335174, (GMT): 2024-06-29 18:57:01.335174+00:00\n",
      "2024-06-29 13:57:01,407 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:01,442 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.52 seconds or 6517.55 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.52 seconds or 6517.55 milliseconds.\n",
      "2024-06-29 13:57:01,499 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:02,020 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:02,049 - micro - MainProcess - INFO     CPU usage: 19.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.6%\n",
      "2024-06-29 13:57:02,068 - micro - MainProcess - INFO     RAM usage: 87.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.4%\n",
      "2024-06-29 13:57:02,094 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:02,101 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:02,107 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687422.1070077 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687422.1070077 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:57:02,114 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:02,118 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:02.117208, (GMT): 2024-06-29 18:57:02.117208+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:02.117208, (GMT): 2024-06-29 18:57:02.117208+00:00\n",
      "2024-06-29 13:57:02,125 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.4 seconds or 3400.13 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.4 seconds or 3400.13 milliseconds.\n",
      "2024-06-29 13:57:02,172 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:02,179 - micro - MainProcess - INFO     CPU usage: 40.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 40.9%\n",
      "2024-06-29 13:57:02,281 - micro - MainProcess - INFO     RAM usage: 87.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.5%\n",
      "2024-06-29 13:57:02,326 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:02,332 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:02,339 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687422.3382893 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 62 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687422.3382893 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 62\n",
      "2024-06-29 13:57:02,346 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:02,350 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:02.350096, (GMT): 2024-06-29 18:57:02.350096+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:02.350096, (GMT): 2024-06-29 18:57:02.350096+00:00\n",
      "2024-06-29 13:57:02,518 - micro - MainProcess - INFO     CPU usage: 75.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 75.1%\n",
      "2024-06-29 13:57:02,531 - micro - MainProcess - INFO     RAM usage: 87.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.7%\n",
      "2024-06-29 13:57:02,551 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:02,560 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:02,565 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687422.5641804 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687422.5641804 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:02,573 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:02,576 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:02.576503, (GMT): 2024-06-29 18:57:02.576503+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:02.576503, (GMT): 2024-06-29 18:57:02.576503+00:00\n",
      "2024-06-29 13:57:03,074 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:03,103 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.2 seconds or 3197.26 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.2 seconds or 3197.26 milliseconds.\n",
      "2024-06-29 13:57:03,191 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:03,198 - micro - MainProcess - INFO     CPU usage: 27.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 27.3%\n",
      "2024-06-29 13:57:03,210 - micro - MainProcess - INFO     RAM usage: 87.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.7%\n",
      "2024-06-29 13:57:03,238 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:03,244 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:03,251 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687423.251478 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687423.251478 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:57:03,259 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:03,263 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:03.263698, (GMT): 2024-06-29 18:57:03.263698+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:03.263698, (GMT): 2024-06-29 18:57:03.263698+00:00\n",
      "2024-06-29 13:57:03,762 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:03,790 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.21 seconds or 3206.24 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.21 seconds or 3206.24 milliseconds.\n",
      "2024-06-29 13:57:03,845 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:04,202 - micro - MainProcess - INFO     CPU usage: 20.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.1%\n",
      "2024-06-29 13:57:04,217 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:57:04,239 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:04,246 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:04,251 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687424.250013 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687424.250013 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:04,260 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:04,263 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:04.263368, (GMT): 2024-06-29 18:57:04.263368+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:04.263368, (GMT): 2024-06-29 18:57:04.263368+00:00\n",
      "2024-06-29 13:57:04,855 - micro - MainProcess - INFO     CPU usage: 23.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.1%\n",
      "2024-06-29 13:57:04,866 - micro - MainProcess - INFO     RAM usage: 87.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.7%\n",
      "2024-06-29 13:57:04,938 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:04,951 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:04,959 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687424.9596643 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687424.9596643 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:57:04,968 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:04,972 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:04.972373, (GMT): 2024-06-29 18:57:04.972373+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:04.972373, (GMT): 2024-06-29 18:57:04.972373+00:00\n",
      "2024-06-29 13:57:04,979 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:04,985 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.29 seconds or 5290.66 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.29 seconds or 5290.66 milliseconds.\n",
      "2024-06-29 13:57:05,039 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:06,038 - micro - MainProcess - INFO     CPU usage: 13.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.5%\n",
      "2024-06-29 13:57:06,049 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:57:06,102 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:06,109 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:06,114 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687426.114157 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687426.114157 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:06,119 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:06,122 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:06.122811, (GMT): 2024-06-29 18:57:06.122811+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:06.122811, (GMT): 2024-06-29 18:57:06.122811+00:00\n",
      "2024-06-29 13:57:06,971 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:07,003 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.73 seconds or 3734.94 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.73 seconds or 3734.94 milliseconds.\n",
      "2024-06-29 13:57:07,050 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:07,859 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:07,889 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.55 seconds or 6551.13 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.55 seconds or 6551.13 milliseconds.\n",
      "2024-06-29 13:57:07,941 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:08,014 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:08,047 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.78 seconds or 3779.15 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.78 seconds or 3779.15 milliseconds.\n",
      "2024-06-29 13:57:08,101 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:08,110 - micro - MainProcess - INFO     CPU usage: 15.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.4%\n",
      "2024-06-29 13:57:08,124 - micro - MainProcess - INFO     RAM usage: 87.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.7%\n",
      "2024-06-29 13:57:08,266 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:08,275 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:08,294 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687428.2940314 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687428.2940314 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:57:08,301 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:08,305 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:08.305583, (GMT): 2024-06-29 18:57:08.305583+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:08.305583, (GMT): 2024-06-29 18:57:08.305583+00:00\n",
      "2024-06-29 13:57:08,706 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:08,734 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.76 seconds or 3757.18 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.76 seconds or 3757.18 milliseconds.\n",
      "2024-06-29 13:57:08,786 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:08,808 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:08,847 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.72 seconds or 6723.88 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.72 seconds or 6723.88 milliseconds.\n",
      "2024-06-29 13:57:08,931 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:08,940 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:08,946 - micro - MainProcess - INFO     CPU usage: 50.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 50.4%\n",
      "2024-06-29 13:57:08,958 - micro - MainProcess - INFO     RAM usage: 87.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.8%\n",
      "2024-06-29 13:57:08,987 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:08,998 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:09,006 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.0060015 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.0060015 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:57:09,015 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:09,019 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:09.019036, (GMT): 2024-06-29 18:57:09.019036+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:09.019036, (GMT): 2024-06-29 18:57:09.019036+00:00\n",
      "2024-06-29 13:57:09,027 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.65 seconds or 6654.95 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.65 seconds or 6654.95 milliseconds.\n",
      "2024-06-29 13:57:09,116 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:09,123 - micro - MainProcess - INFO     CPU usage: 88.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 88.4%\n",
      "2024-06-29 13:57:09,145 - micro - MainProcess - INFO     RAM usage: 88.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.0%\n",
      "2024-06-29 13:57:09,176 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:09,190 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:09,196 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.19681 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 76 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.19681 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 76\n",
      "2024-06-29 13:57:09,212 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:09,231 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:09.231922, (GMT): 2024-06-29 18:57:09.231922+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:09.231922, (GMT): 2024-06-29 18:57:09.231922+00:00\n",
      "2024-06-29 13:57:09,266 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:09,292 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.71 seconds or 6710.98 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.71 seconds or 6710.98 milliseconds.\n",
      "2024-06-29 13:57:09,383 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:09,480 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:09,514 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.39 seconds or 3388.63 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.39 seconds or 3388.63 milliseconds.\n",
      "2024-06-29 13:57:09,577 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:09,810 - micro - MainProcess - INFO     CPU usage: 91.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 91.9%\n",
      "2024-06-29 13:57:09,822 - micro - MainProcess - INFO     RAM usage: 88.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.2%\n",
      "2024-06-29 13:57:09,854 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:09,864 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:09,874 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.873202 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.873202 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:57:09,884 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:09,889 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:09.889318, (GMT): 2024-06-29 18:57:09.889318+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:09.889318, (GMT): 2024-06-29 18:57:09.889318+00:00\n",
      "2024-06-29 13:57:09,933 - micro - MainProcess - INFO     CPU usage: 75.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 75.5%\n",
      "2024-06-29 13:57:09,949 - micro - MainProcess - INFO     RAM usage: 88.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.3%\n",
      "2024-06-29 13:57:09,978 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:09,991 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:09,997 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.9979382 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687429.9979382 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:57:10,008 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:10,012 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:10.012310, (GMT): 2024-06-29 18:57:10.012310+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:10.012310, (GMT): 2024-06-29 18:57:10.012310+00:00\n",
      "2024-06-29 13:57:10,135 - micro - MainProcess - INFO     CPU usage: 79.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 79.8%\n",
      "2024-06-29 13:57:10,148 - micro - MainProcess - INFO     RAM usage: 88.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.5%\n",
      "2024-06-29 13:57:10,181 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:10,192 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:10,199 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687430.198551 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687430.198551 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:57:10,207 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:10,214 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:10.214899, (GMT): 2024-06-29 18:57:10.214899+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:10.214899, (GMT): 2024-06-29 18:57:10.214899+00:00\n",
      "2024-06-29 13:57:10,393 - micro - MainProcess - INFO     CPU usage: 72.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 72.4%\n",
      "2024-06-29 13:57:10,409 - micro - MainProcess - INFO     RAM usage: 88.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.6%\n",
      "2024-06-29 13:57:10,444 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:10,461 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:10,467 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687430.465001 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687430.465001 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:57:10,474 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:10,479 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:10.479469, (GMT): 2024-06-29 18:57:10.479469+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:10.479469, (GMT): 2024-06-29 18:57:10.479469+00:00\n",
      "2024-06-29 13:57:10,580 - micro - MainProcess - INFO     CPU usage: 63.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 63.5%\n",
      "2024-06-29 13:57:10,597 - micro - MainProcess - INFO     RAM usage: 88.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.7%\n",
      "2024-06-29 13:57:10,628 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:10,639 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:10,645 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687430.645015 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687430.645015 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:57:10,654 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:10,662 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:10.662111, (GMT): 2024-06-29 18:57:10.662111+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:10.662111, (GMT): 2024-06-29 18:57:10.662111+00:00\n",
      "2024-06-29 13:57:11,589 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:11,630 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.32 seconds or 3320.79 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.32 seconds or 3320.79 milliseconds.\n",
      "2024-06-29 13:57:11,676 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:12,687 - micro - MainProcess - INFO     CPU usage: 17.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 17.2%\n",
      "2024-06-29 13:57:12,698 - micro - MainProcess - INFO     RAM usage: 88.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.6%\n",
      "2024-06-29 13:57:12,721 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:12,726 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:12,730 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687432.7309084 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687432.7309084 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:57:12,740 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:12,743 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:12.743963, (GMT): 2024-06-29 18:57:12.743963+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:12.743963, (GMT): 2024-06-29 18:57:12.743963+00:00\n",
      "2024-06-29 13:57:12,933 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:12,964 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3702.55 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3702.55 milliseconds.\n",
      "2024-06-29 13:57:13,041 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:13,734 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:13,766 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.87 seconds or 3870.32 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.87 seconds or 3870.32 milliseconds.\n",
      "2024-06-29 13:57:13,822 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:14,042 - micro - MainProcess - INFO     CPU usage: 18.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 18.2%\n",
      "2024-06-29 13:57:14,054 - micro - MainProcess - INFO     RAM usage: 88.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.6%\n",
      "2024-06-29 13:57:14,078 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:14,085 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:14,090 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687434.0902743 Sustainable development seeks to meet the needs of the present without compromising the ability of future generations to meet their own needs. What are some challenges and solutions in implementing sustainable development practices? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687434.0902743 Sustainable development seeks to meet the needs of the present without compromising the ability of future generations to meet their own needs. What are some challenges and solutions in implementing sustainable development practices? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:14,096 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:14,098 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:14.098269, (GMT): 2024-06-29 18:57:14.098269+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:14.098269, (GMT): 2024-06-29 18:57:14.098269+00:00\n",
      "2024-06-29 13:57:14,833 - micro - MainProcess - INFO     CPU usage: 13.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.9%\n",
      "2024-06-29 13:57:14,842 - micro - MainProcess - INFO     RAM usage: 88.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.5%\n",
      "2024-06-29 13:57:14,868 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:14,871 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:14,877 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687434.877217 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687434.877217 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:14,883 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:14,887 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:14.887879, (GMT): 2024-06-29 18:57:14.887879+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:14.887879, (GMT): 2024-06-29 18:57:14.887879+00:00\n",
      "2024-06-29 13:57:16,360 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:16,407 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.74 seconds or 5741.04 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.74 seconds or 5741.04 milliseconds.\n",
      "2024-06-29 13:57:16,481 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:16,489 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:16,497 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.47 seconds or 7471.94 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.47 seconds or 7471.94 milliseconds.\n",
      "2024-06-29 13:57:16,548 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:16,591 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:16,620 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.87 seconds or 3873.88 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.87 seconds or 3873.88 milliseconds.\n",
      "2024-06-29 13:57:16,689 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:16,695 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:16,740 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.52 seconds or 6518.9 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.52 seconds or 6518.9 milliseconds.\n",
      "2024-06-29 13:57:16,800 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:17,482 - micro - MainProcess - INFO     CPU usage: 9.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 9.9%\n",
      "2024-06-29 13:57:17,491 - micro - MainProcess - INFO     RAM usage: 87.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.9%\n",
      "2024-06-29 13:57:17,514 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:17,522 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,526 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.5266864 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.5266864 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:17,535 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,538 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:17.538599, (GMT): 2024-06-29 18:57:17.538599+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:17.538599, (GMT): 2024-06-29 18:57:17.538599+00:00\n",
      "2024-06-29 13:57:17,543 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:17,548 - micro - MainProcess - INFO     CPU usage: 21.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 21.4%\n",
      "2024-06-29 13:57:17,561 - micro - MainProcess - INFO     RAM usage: 87.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.9%\n",
      "2024-06-29 13:57:17,581 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:17,594 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,600 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.6009803 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 75 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.6009803 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 75\n",
      "2024-06-29 13:57:17,617 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,619 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:17.619267, (GMT): 2024-06-29 18:57:17.619267+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:17.619267, (GMT): 2024-06-29 18:57:17.619267+00:00\n",
      "2024-06-29 13:57:17,626 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.6 seconds or 7601.47 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.6 seconds or 7601.47 milliseconds.\n",
      "2024-06-29 13:57:17,695 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:17,701 - micro - MainProcess - INFO     CPU usage: 33.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 33.1%\n",
      "2024-06-29 13:57:17,715 - micro - MainProcess - INFO     RAM usage: 88.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.0%\n",
      "2024-06-29 13:57:17,755 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:17,761 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,778 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.7785141 Natural language processing (NLP) enables computers to understand and generate human language. How does natural language processing enable computers to understand human language? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.7785141 Natural language processing (NLP) enables computers to understand and generate human language. How does natural language processing enable computers to understand human language? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:57:17,786 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,791 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:17.791587, (GMT): 2024-06-29 18:57:17.791587+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:17.791587, (GMT): 2024-06-29 18:57:17.791587+00:00\n",
      "2024-06-29 13:57:17,808 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:17,823 - micro - MainProcess - INFO     CPU usage: 79.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 79.8%\n",
      "2024-06-29 13:57:17,839 - micro - MainProcess - INFO     RAM usage: 88.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.1%\n",
      "2024-06-29 13:57:17,859 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:17,864 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,870 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.870125 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687437.870125 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:57:17,877 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:17,880 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:17.880352, (GMT): 2024-06-29 18:57:17.880352+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:17.880352, (GMT): 2024-06-29 18:57:17.880352+00:00\n",
      "2024-06-29 13:57:17,890 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.0 seconds or 2999.75 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.0 seconds or 2999.75 milliseconds.\n",
      "2024-06-29 13:57:17,942 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:17,950 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:17,957 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.47 seconds or 7468.61 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.47 seconds or 7468.61 milliseconds.\n",
      "2024-06-29 13:57:18,018 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:18,026 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:18,046 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.94 seconds or 3942.86 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.94 seconds or 3942.86 milliseconds.\n",
      "2024-06-29 13:57:18,155 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:18,701 - micro - MainProcess - INFO     CPU usage: 31.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 31.9%\n",
      "2024-06-29 13:57:18,712 - micro - MainProcess - INFO     RAM usage: 87.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.8%\n",
      "2024-06-29 13:57:18,738 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:18,776 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:18,781 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687438.7812877 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687438.7812877 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:57:18,790 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:18,796 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:18.796434, (GMT): 2024-06-29 18:57:18.796434+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:18.796434, (GMT): 2024-06-29 18:57:18.796434+00:00\n",
      "2024-06-29 13:57:18,939 - micro - MainProcess - INFO     CPU usage: 51.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 51.9%\n",
      "2024-06-29 13:57:18,952 - micro - MainProcess - INFO     RAM usage: 88.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.0%\n",
      "2024-06-29 13:57:18,972 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:18,979 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:18,987 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687438.986335 The circular economy aims to minimize waste and make the most of resources by creating closed-loop systems. What are the benefits of a circular economy, and how does it promote sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687438.986335 The circular economy aims to minimize waste and make the most of resources by creating closed-loop systems. What are the benefits of a circular economy, and how does it promote sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:57:18,994 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:18,997 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:18.997783, (GMT): 2024-06-29 18:57:18.997783+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:18.997783, (GMT): 2024-06-29 18:57:18.997783+00:00\n",
      "2024-06-29 13:57:19,005 - micro - MainProcess - INFO     CPU usage: 51.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 51.8%\n",
      "2024-06-29 13:57:19,018 - micro - MainProcess - INFO     RAM usage: 87.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.9%\n",
      "2024-06-29 13:57:19,036 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:19,044 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:19,053 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687439.052966 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687439.052966 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:19,060 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:19,064 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:19.064983, (GMT): 2024-06-29 18:57:19.064983+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:19.064983, (GMT): 2024-06-29 18:57:19.064983+00:00\n",
      "2024-06-29 13:57:19,164 - micro - MainProcess - INFO     CPU usage: 15.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.1%\n",
      "2024-06-29 13:57:19,175 - micro - MainProcess - INFO     RAM usage: 87.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.9%\n",
      "2024-06-29 13:57:19,198 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:19,205 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:19,210 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687439.2106853 Social media trends influence consumer behavior and marketing strategies, shaping how brands interact with their audiences. Can you discuss the impact of social media trends on consumer behavior and marketing? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687439.2106853 Social media trends influence consumer behavior and marketing strategies, shaping how brands interact with their audiences. Can you discuss the impact of social media trends on consumer behavior and marketing? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:57:19,218 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:19,221 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:19.221231, (GMT): 2024-06-29 18:57:19.221231+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:19.221231, (GMT): 2024-06-29 18:57:19.221231+00:00\n",
      "2024-06-29 13:57:22,215 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:22,246 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.7 seconds or 4704.4 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.7 seconds or 4704.4 milliseconds.\n",
      "2024-06-29 13:57:22,298 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:22,306 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:22,318 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.52 seconds or 4518.59 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.52 seconds or 4518.59 milliseconds.\n",
      "2024-06-29 13:57:22,371 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:23,320 - micro - MainProcess - INFO     CPU usage: 13.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.2%\n",
      "2024-06-29 13:57:23,339 - micro - MainProcess - INFO     RAM usage: 88.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.1%\n",
      "2024-06-29 13:57:23,363 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:23,375 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:23,381 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687443.3816466 The Internet of Things (IoT) refers to a network of interconnected devices that communicate and share data. Can you explain the concept of the Internet of Things and provide some real-world examples? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687443.3816466 The Internet of Things (IoT) refers to a network of interconnected devices that communicate and share data. Can you explain the concept of the Internet of Things and provide some real-world examples? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:57:23,408 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:23,411 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:23.411946, (GMT): 2024-06-29 18:57:23.411946+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:23.411946, (GMT): 2024-06-29 18:57:23.411946+00:00\n",
      "2024-06-29 13:57:23,420 - micro - MainProcess - INFO     CPU usage: 32.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 32.9%\n",
      "2024-06-29 13:57:23,433 - micro - MainProcess - INFO     RAM usage: 88.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.0%\n",
      "2024-06-29 13:57:23,457 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:23,464 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:23,471 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687443.4718094 Drones are being used in various industries, from agriculture to logistics, due to their versatility. What are the potential uses of drones in agriculture and other industries? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687443.4718094 Drones are being used in various industries, from agriculture to logistics, due to their versatility. What are the potential uses of drones in agriculture and other industries? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:57:23,479 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:23,485 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:23.485521, (GMT): 2024-06-29 18:57:23.485521+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:23.485521, (GMT): 2024-06-29 18:57:23.485521+00:00\n",
      "2024-06-29 13:57:23,554 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:23,562 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.56 seconds or 4560.16 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.56 seconds or 4560.16 milliseconds.\n",
      "2024-06-29 13:57:23,621 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:23,688 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:23,726 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.5 seconds or 4501.46 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.5 seconds or 4501.46 milliseconds.\n",
      "2024-06-29 13:57:23,774 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:24,624 - micro - MainProcess - INFO     CPU usage: 29.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 29.5%\n",
      "2024-06-29 13:57:24,683 - micro - MainProcess - INFO     RAM usage: 88.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.2%\n",
      "2024-06-29 13:57:24,707 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:24,713 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:24,719 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687444.7172976 Agritech innovations are improving agricultural productivity and sustainability through the use of technology. How are advancements in agritech improving food production and sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687444.7172976 Agritech innovations are improving agricultural productivity and sustainability through the use of technology. How are advancements in agritech improving food production and sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:57:24,727 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:24,730 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:24.730137, (GMT): 2024-06-29 18:57:24.730137+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:24.730137, (GMT): 2024-06-29 18:57:24.730137+00:00\n",
      "2024-06-29 13:57:24,781 - micro - MainProcess - INFO     CPU usage: 30.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 30.6%\n",
      "2024-06-29 13:57:24,795 - micro - MainProcess - INFO     RAM usage: 88.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.1%\n",
      "2024-06-29 13:57:24,808 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:24,815 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:24,823 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687444.8201725 Artificial intelligence (AI) is being used to improve the accuracy and efficiency of medical diagnoses. How is artificial intelligence being used to improve the accuracy of medical diagnoses? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687444.8201725 Artificial intelligence (AI) is being used to improve the accuracy and efficiency of medical diagnoses. How is artificial intelligence being used to improve the accuracy of medical diagnoses? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:57:24,829 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:24,832 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:24.832744, (GMT): 2024-06-29 18:57:24.832744+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:24.832744, (GMT): 2024-06-29 18:57:24.832744+00:00\n",
      "2024-06-29 13:57:26,412 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:26,440 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.82 seconds or 8818.0 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.82 seconds or 8818.0 milliseconds.\n",
      "2024-06-29 13:57:26,496 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:26,504 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:26,509 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.62 seconds or 8624.54 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.62 seconds or 8624.54 milliseconds.\n",
      "2024-06-29 13:57:26,591 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:27,210 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:27,243 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.44 seconds or 8441.8 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.44 seconds or 8441.8 milliseconds.\n",
      "2024-06-29 13:57:27,305 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:27,504 - micro - MainProcess - INFO     CPU usage: 18.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 18.9%\n",
      "2024-06-29 13:57:27,515 - micro - MainProcess - INFO     RAM usage: 88.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.3%\n",
      "2024-06-29 13:57:27,537 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:27,545 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:27,551 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687447.5509071 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687447.5509071 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:27,558 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:27,562 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:27.562933, (GMT): 2024-06-29 18:57:27.562933+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:27.562933, (GMT): 2024-06-29 18:57:27.562933+00:00\n",
      "2024-06-29 13:57:27,591 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:27,596 - micro - MainProcess - INFO     CPU usage: 26.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 26.2%\n",
      "2024-06-29 13:57:27,610 - micro - MainProcess - INFO     RAM usage: 88.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.3%\n",
      "2024-06-29 13:57:27,634 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:27,640 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:27,647 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687447.6454334 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687447.6454334 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:57:27,652 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:27,655 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:27.655758, (GMT): 2024-06-29 18:57:27.655758+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:27.655758, (GMT): 2024-06-29 18:57:27.655758+00:00\n",
      "2024-06-29 13:57:27,665 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.59 seconds or 8594.4 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.59 seconds or 8594.4 milliseconds.\n",
      "2024-06-29 13:57:27,728 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:27,785 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:27,810 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.32 seconds or 4320.12 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.32 seconds or 4320.12 milliseconds.\n",
      "2024-06-29 13:57:27,862 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:27,945 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:27,994 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.58 seconds or 4579.06 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.58 seconds or 4579.06 milliseconds.\n",
      "2024-06-29 13:57:28,048 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:28,317 - micro - MainProcess - INFO     CPU usage: 42.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 42.0%\n",
      "2024-06-29 13:57:28,335 - micro - MainProcess - INFO     RAM usage: 88.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.4%\n",
      "2024-06-29 13:57:28,359 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:28,365 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:28,373 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687448.3724744 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687448.3724744 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:57:28,378 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:28,383 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:28.381699, (GMT): 2024-06-29 18:57:28.383214+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:28.381699, (GMT): 2024-06-29 18:57:28.383214+00:00\n",
      "2024-06-29 13:57:28,733 - micro - MainProcess - INFO     CPU usage: 14.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.6%\n",
      "2024-06-29 13:57:28,747 - micro - MainProcess - INFO     RAM usage: 88.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.4%\n",
      "2024-06-29 13:57:28,770 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:28,840 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:28,848 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687448.8478615 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687448.8478615 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:57:28,855 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:28,859 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:28.859898, (GMT): 2024-06-29 18:57:28.859898+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:28.859898, (GMT): 2024-06-29 18:57:28.859898+00:00\n",
      "2024-06-29 13:57:28,866 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:28,877 - micro - MainProcess - INFO     CPU usage: 77.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 77.4%\n",
      "2024-06-29 13:57:28,896 - micro - MainProcess - INFO     RAM usage: 88.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.4%\n",
      "2024-06-29 13:57:28,942 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:28,950 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:28,958 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687448.9587448 Cloud computing provides on-demand access to computing resources over the internet. How does cloud computing enhance business operations and data management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 63 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687448.9587448 Cloud computing provides on-demand access to computing resources over the internet. How does cloud computing enhance business operations and data management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 63\n",
      "2024-06-29 13:57:28,970 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:28,974 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:28.974179, (GMT): 2024-06-29 18:57:28.974179+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:28.974179, (GMT): 2024-06-29 18:57:28.974179+00:00\n",
      "2024-06-29 13:57:28,984 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:29,019 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.28 seconds or 4278.05 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.28 seconds or 4278.05 milliseconds.\n",
      "2024-06-29 13:57:29,102 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:29,107 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.27 seconds or 4268.19 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.27 seconds or 4268.19 milliseconds.\n",
      "2024-06-29 13:57:29,192 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:29,208 - micro - MainProcess - INFO     CPU usage: 80.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 80.3%\n",
      "2024-06-29 13:57:29,230 - micro - MainProcess - INFO     RAM usage: 88.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 88.7%\n",
      "2024-06-29 13:57:29,256 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:29,264 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:29,270 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687449.27079 Cybersecurity is crucial in protecting our personal and professional data from malicious attacks. How does cybersecurity protect our personal data, and what measures can individuals take? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687449.27079 Cybersecurity is crucial in protecting our personal and professional data from malicious attacks. How does cybersecurity protect our personal data, and what measures can individuals take? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:57:29,279 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:29,297 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:29.297305, (GMT): 2024-06-29 18:57:29.297305+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:29.297305, (GMT): 2024-06-29 18:57:29.297305+00:00\n",
      "2024-06-29 13:57:30,103 - micro - MainProcess - INFO     CPU usage: 36.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 36.2%\n",
      "2024-06-29 13:57:30,114 - micro - MainProcess - INFO     RAM usage: 87.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.6%\n",
      "2024-06-29 13:57:30,139 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:30,147 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:30,151 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687450.1519327 Urban planning involves designing and organizing urban spaces to improve the quality of life for residents. What is the role of urban planning in creating more livable and sustainable cities? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687450.1519327 Urban planning involves designing and organizing urban spaces to improve the quality of life for residents. What is the role of urban planning in creating more livable and sustainable cities? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:57:30,160 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:30,163 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:30.163343, (GMT): 2024-06-29 18:57:30.163343+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:30.163343, (GMT): 2024-06-29 18:57:30.163343+00:00\n",
      "2024-06-29 13:57:30,195 - micro - MainProcess - INFO     CPU usage: 25.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.4%\n",
      "2024-06-29 13:57:30,207 - micro - MainProcess - INFO     RAM usage: 87.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.8%\n",
      "2024-06-29 13:57:30,231 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:30,241 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:30,245 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687450.245661 Blockchain technology offers transparent and secure ways to track products through the supply chain. What are the key benefits of using blockchain for supply chain management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687450.245661 Blockchain technology offers transparent and secure ways to track products through the supply chain. What are the key benefits of using blockchain for supply chain management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:57:30,253 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:30,256 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:30.256431, (GMT): 2024-06-29 18:57:30.256431+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:30.256431, (GMT): 2024-06-29 18:57:30.256431+00:00\n",
      "2024-06-29 13:57:32,740 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:32,772 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.79 seconds or 3791.33 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.79 seconds or 3791.33 milliseconds.\n",
      "2024-06-29 13:57:32,839 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:33,830 - micro - MainProcess - INFO     CPU usage: 10.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.6%\n",
      "2024-06-29 13:57:33,843 - micro - MainProcess - INFO     RAM usage: 87.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.0%\n",
      "2024-06-29 13:57:33,867 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:33,872 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:33,875 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687453.8758519 Digital marketing is evolving with new trends and technologies that help businesses reach their audiences. What are some emerging trends in digital marketing that businesses should be aware of? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687453.8758519 Digital marketing is evolving with new trends and technologies that help businesses reach their audiences. What are some emerging trends in digital marketing that businesses should be aware of? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:57:33,881 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:33,886 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:33.886822, (GMT): 2024-06-29 18:57:33.886822+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:33.886822, (GMT): 2024-06-29 18:57:33.886822+00:00\n",
      "2024-06-29 13:57:33,909 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:33,950 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.69 seconds or 3689.33 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.69 seconds or 3689.33 milliseconds.\n",
      "2024-06-29 13:57:34,003 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:34,842 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:34,872 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.21 seconds or 7212.32 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.21 seconds or 7212.32 milliseconds.\n",
      "2024-06-29 13:57:34,924 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:35,014 - micro - MainProcess - INFO     CPU usage: 22.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 22.4%\n",
      "2024-06-29 13:57:35,027 - micro - MainProcess - INFO     RAM usage: 87.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.2%\n",
      "2024-06-29 13:57:35,066 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:35,091 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:35,097 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687455.0974312 Smart grids use digital technology to manage the production and distribution of electricity more efficiently. How is the development of smart grids contributing to more efficient energy distribution? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687455.0974312 Smart grids use digital technology to manage the production and distribution of electricity more efficiently. How is the development of smart grids contributing to more efficient energy distribution? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69\n",
      "2024-06-29 13:57:35,110 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:35,115 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:35.115246, (GMT): 2024-06-29 18:57:35.115246+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:35.115246, (GMT): 2024-06-29 18:57:35.115246+00:00\n",
      "2024-06-29 13:57:35,833 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:35,863 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.47 seconds or 7474.74 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.47 seconds or 7474.74 milliseconds.\n",
      "2024-06-29 13:57:35,908 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:35,921 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:35,925 - micro - MainProcess - INFO     CPU usage: 14.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.2%\n",
      "2024-06-29 13:57:35,938 - micro - MainProcess - INFO     RAM usage: 87.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.0%\n",
      "2024-06-29 13:57:35,960 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:35,969 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:35,977 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687455.9754593 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 75 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687455.9754593 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 75\n",
      "2024-06-29 13:57:35,985 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:35,992 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:35.992499, (GMT): 2024-06-29 18:57:35.992499+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:35.992499, (GMT): 2024-06-29 18:57:35.992499+00:00\n",
      "2024-06-29 13:57:36,005 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.68 seconds or 6681.87 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.68 seconds or 6681.87 milliseconds.\n",
      "2024-06-29 13:57:36,075 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:36,924 - micro - MainProcess - INFO     CPU usage: 31.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 31.8%\n",
      "2024-06-29 13:57:36,936 - micro - MainProcess - INFO     RAM usage: 87.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.1%\n",
      "2024-06-29 13:57:36,950 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:36,958 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:36,962 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687456.9628851 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687456.9628851 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:57:36,971 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:36,982 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:36.982148, (GMT): 2024-06-29 18:57:36.982148+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:36.982148, (GMT): 2024-06-29 18:57:36.982148+00:00\n",
      "2024-06-29 13:57:37,089 - micro - MainProcess - INFO     CPU usage: 25.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.2%\n",
      "2024-06-29 13:57:37,100 - micro - MainProcess - INFO     RAM usage: 87.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.1%\n",
      "2024-06-29 13:57:37,133 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:37,141 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:37,148 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687457.14809 Renewable energy sources, such as solar and wind power, are essential in reducing our reliance on fossil fuels. What are the environmental benefits of renewable energy sources compared to fossil fuels? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687457.14809 Renewable energy sources, such as solar and wind power, are essential in reducing our reliance on fossil fuels. What are the environmental benefits of renewable energy sources compared to fossil fuels? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:57:37,152 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:37,156 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:37.156352, (GMT): 2024-06-29 18:57:37.156352+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:37.156352, (GMT): 2024-06-29 18:57:37.156352+00:00\n",
      "2024-06-29 13:57:37,281 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:37,321 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.15 seconds or 7153.55 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.15 seconds or 7153.55 milliseconds.\n",
      "2024-06-29 13:57:37,376 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:38,388 - micro - MainProcess - INFO     CPU usage: 23.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.4%\n",
      "2024-06-29 13:57:38,400 - micro - MainProcess - INFO     RAM usage: 87.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.4%\n",
      "2024-06-29 13:57:38,424 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:38,431 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:38,439 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687458.4399142 Autonomous drones can operate without human intervention, offering various applications in different industries. How do autonomous drones operate, and what are their potential applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687458.4399142 Autonomous drones can operate without human intervention, offering various applications in different industries. How do autonomous drones operate, and what are their potential applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:57:38,445 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:38,447 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:38.447959, (GMT): 2024-06-29 18:57:38.447959+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:38.447959, (GMT): 2024-06-29 18:57:38.447959+00:00\n",
      "2024-06-29 13:57:39,177 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:39,209 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 11.64 seconds or 11639.49 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 11.64 seconds or 11639.49 milliseconds.\n",
      "2024-06-29 13:57:39,257 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:39,262 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:39,277 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.16 seconds or 4156.42 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.16 seconds or 4156.42 milliseconds.\n",
      "2024-06-29 13:57:39,342 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:40,190 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:40,220 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.33 seconds or 6328.29 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.33 seconds or 6328.29 milliseconds.\n",
      "2024-06-29 13:57:40,267 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:40,275 - micro - MainProcess - INFO     CPU usage: 25.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.5%\n",
      "2024-06-29 13:57:40,288 - micro - MainProcess - INFO     RAM usage: 87.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.3%\n",
      "2024-06-29 13:57:40,309 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:40,314 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:40,320 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687460.3195121 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687460.3195121 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:57:40,328 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:40,331 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:40.331658, (GMT): 2024-06-29 18:57:40.331658+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:57:40.331658, (GMT): 2024-06-29 18:57:40.331658+00:00\n",
      "2024-06-29 13:57:40,932 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:57:40,959 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.8 seconds or 3798.53 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.8 seconds or 3798.53 milliseconds.\n",
      "2024-06-29 13:57:41,015 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:57:41,270 - micro - MainProcess - INFO     CPU usage: 21.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 21.0%\n",
      "2024-06-29 13:57:41,282 - micro - MainProcess - INFO     RAM usage: 87.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 87.4%\n",
      "2024-06-29 13:57:41,307 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:57:41,315 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:41,322 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687461.3218503 Genetic modification in crops aims to improve yield, resistance to pests, and nutritional value. Can you explain the principles of genetic modification in crops and its impact on agriculture? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719687461.3218503 Genetic modification in crops aims to improve yield, resistance to pests, and nutritional value. Can you explain the principles of genetic modification in crops and its impact on agriculture? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:57:41,330 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:76)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:57:41,333 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:41.333588, (GMT): 2024-06-29 18:57:41.333588+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:57:41.333588, (GMT): 2024-06-29 18:57:41.333588+00:00\n"
     ]
    }
   ],
   "source": [
    "await benchmark_streaming.run_latency_benchmark_bulk(deployment_names=[DEPLOYMENT_ID], max_tokens_list=[500,250], byop=prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_streaming.calculate_and_show_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 13:26:24,285 - micro - MainProcess - INFO     Split list into 4 parts. (utils.py:split_list_into_variable_parts:174)\n",
      "INFO:micro:Split list into 4 parts.\n",
      "2024-06-29 13:26:24,290 - micro - MainProcess - INFO     CPU usage: 10.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.3%\n",
      "2024-06-29 13:26:24,301 - micro - MainProcess - INFO     RAM usage: 85.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.1%\n",
      "2024-06-29 13:26:24,323 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,328 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,332 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.3317206 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.3317206 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:26:24,341 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,345 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.345031, (GMT): 2024-06-29 18:26:24.345031+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.345031, (GMT): 2024-06-29 18:26:24.345031+00:00\n",
      "2024-06-29 13:26:24,349 - micro - MainProcess - INFO     CPU usage: 5.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 5.0%\n",
      "2024-06-29 13:26:24,363 - micro - MainProcess - INFO     RAM usage: 85.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.1%\n",
      "2024-06-29 13:26:24,381 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,388 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,392 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.39125 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.39125 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:26:24,399 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,403 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.403603, (GMT): 2024-06-29 18:26:24.403603+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.403603, (GMT): 2024-06-29 18:26:24.403603+00:00\n",
      "2024-06-29 13:26:24,407 - micro - MainProcess - INFO     CPU usage: 4.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 4.4%\n",
      "2024-06-29 13:26:24,422 - micro - MainProcess - INFO     RAM usage: 85.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.1%\n",
      "2024-06-29 13:26:24,440 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,445 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,450 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.4491832 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.4491832 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:26:24,456 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,459 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.458406, (GMT): 2024-06-29 18:26:24.458406+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.458406, (GMT): 2024-06-29 18:26:24.458406+00:00\n",
      "2024-06-29 13:26:24,461 - micro - MainProcess - INFO     CPU usage: 2.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 2.6%\n",
      "2024-06-29 13:26:24,472 - micro - MainProcess - INFO     RAM usage: 85.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.1%\n",
      "2024-06-29 13:26:24,495 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,506 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,510 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.5097642 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.5097642 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:26:24,520 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,525 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.525404, (GMT): 2024-06-29 18:26:24.525404+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:24.525404, (GMT): 2024-06-29 18:26:24.525404+00:00\n",
      "2024-06-29 13:26:24,530 - micro - MainProcess - INFO     CPU usage: 23.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.1%\n",
      "2024-06-29 13:26:24,548 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:26:24,587 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,594 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,598 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.5981498 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.5981498 Quantum computing is a new frontier in technology, promising to solve problems that are currently intractable for classical computers. Can you explain the concept of quantum computing and its potential impact on technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:26:24,608 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,611 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.611850, (GMT): 2024-06-29 18:26:24.611850+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.611850, (GMT): 2024-06-29 18:26:24.611850+00:00\n",
      "2024-06-29 13:26:24,661 - micro - MainProcess - INFO     CPU usage: 29.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 29.8%\n",
      "2024-06-29 13:26:24,679 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:26:24,721 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,730 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,735 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.735036 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.735036 Virtual reality (VR) creates immersive environments that can be used for entertainment, education, and therapy. How is virtual reality being used in therapeutic settings to treat mental health conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:26:24,743 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,747 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.747631, (GMT): 2024-06-29 18:26:24.747631+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.747631, (GMT): 2024-06-29 18:26:24.747631+00:00\n",
      "2024-06-29 13:26:24,756 - micro - MainProcess - INFO     CPU usage: 73.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 73.7%\n",
      "2024-06-29 13:26:24,768 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:26:24,787 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,794 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,800 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.7987113 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.7987113 Wearable technologies, such as fitness trackers and smartwatches, monitor various health metrics. How are wearable technologies transforming personal health and fitness monitoring? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:26:24,805 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,808 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.808942, (GMT): 2024-06-29 18:26:24.808942+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.808942, (GMT): 2024-06-29 18:26:24.808942+00:00\n",
      "2024-06-29 13:26:24,813 - micro - MainProcess - INFO     CPU usage: 5.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 5.9%\n",
      "2024-06-29 13:26:24,825 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:26:24,846 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:24,853 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,858 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.8583302 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685584.8583302 Biotechnology raises ethical questions, especially when it comes to modifying human genes. What are some ethical considerations in the use of biotechnology in humans? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:26:24,866 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:24,869 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.869887, (GMT): 2024-06-29 18:26:24.869887+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:24.869887, (GMT): 2024-06-29 18:26:24.869887+00:00\n",
      "2024-06-29 13:26:27,729 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:27,762 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.14 seconds or 3138.05 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.14 seconds or 3138.05 milliseconds.\n",
      "2024-06-29 13:26:27,905 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:28,922 - micro - MainProcess - INFO     CPU usage: 12.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.2%\n",
      "2024-06-29 13:26:28,937 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:26:28,982 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:28,992 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:28,996 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685588.99647 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685588.99647 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:26:29,003 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:29,009 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:29.009141, (GMT): 2024-06-29 18:26:29.009141+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:29.009141, (GMT): 2024-06-29 18:26:29.009141+00:00\n",
      "2024-06-29 13:26:29,016 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:29,021 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:29,028 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.15 seconds or 4151.9 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.15 seconds or 4151.9 milliseconds.\n",
      "2024-06-29 13:26:29,077 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:29,081 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.27 seconds or 4268.45 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.27 seconds or 4268.45 milliseconds.\n",
      "2024-06-29 13:26:29,140 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:29,225 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:29,258 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.5 seconds or 4504.36 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.5 seconds or 4504.36 milliseconds.\n",
      "2024-06-29 13:26:29,303 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:30,097 - micro - MainProcess - INFO     CPU usage: 31.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 31.1%\n",
      "2024-06-29 13:26:30,117 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:26:30,150 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:30,161 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:30,170 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685590.1709027 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685590.1709027 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:26:30,178 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:30,186 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:30.186770, (GMT): 2024-06-29 18:26:30.186770+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:30.186770, (GMT): 2024-06-29 18:26:30.186770+00:00\n",
      "2024-06-29 13:26:30,203 - micro - MainProcess - INFO     CPU usage: 13.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.8%\n",
      "2024-06-29 13:26:30,232 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:26:30,261 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:30,277 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:30,290 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685590.2903528 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685590.2903528 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:26:30,301 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:30,314 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:30.314551, (GMT): 2024-06-29 18:26:30.314551+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:30.314551, (GMT): 2024-06-29 18:26:30.314551+00:00\n",
      "2024-06-29 13:26:30,326 - micro - MainProcess - INFO     CPU usage: 55.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 55.8%\n",
      "2024-06-29 13:26:30,345 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:26:30,433 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:30,445 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:30,450 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685590.4509313 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 78 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685590.4509313 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 78\n",
      "2024-06-29 13:26:30,458 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:30,474 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:30.474675, (GMT): 2024-06-29 18:26:30.474675+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:30.474675, (GMT): 2024-06-29 18:26:30.474675+00:00\n",
      "2024-06-29 13:26:32,039 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:32,045 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:32,052 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:32,083 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.62 seconds or 7618.87 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.62 seconds or 7618.87 milliseconds.\n",
      "2024-06-29 13:26:32,130 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:32,140 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.79 seconds or 7788.39 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.79 seconds or 7788.39 milliseconds.\n",
      "2024-06-29 13:26:32,186 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:32,195 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.66 seconds or 7661.89 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.66 seconds or 7661.89 milliseconds.\n",
      "2024-06-29 13:26:32,244 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:32,542 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:32,575 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.56 seconds or 3559.75 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.56 seconds or 3559.75 milliseconds.\n",
      "2024-06-29 13:26:32,626 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:33,129 - micro - MainProcess - INFO     CPU usage: 47.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 47.6%\n",
      "2024-06-29 13:26:33,140 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:26:33,159 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:33,169 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,177 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.1762571 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.1762571 Virtual reality (VR) and augmented reality (AR) offer different ways to enhance our perception of the world. What are the main differences between virtual reality and augmented reality? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:26:33,185 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,189 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:33.189280, (GMT): 2024-06-29 18:26:33.189280+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:33.189280, (GMT): 2024-06-29 18:26:33.189280+00:00\n",
      "2024-06-29 13:26:33,197 - micro - MainProcess - INFO     CPU usage: 20.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.8%\n",
      "2024-06-29 13:26:33,210 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:33,229 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:33,236 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,243 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.2412727 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.2412727 Blockchain technology, initially popularized by cryptocurrencies like Bitcoin, has far-reaching applications beyond digital currencies. How does blockchain technology work, and what are its main applications beyond cryptocurrencies? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:26:33,247 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,250 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:33.250941, (GMT): 2024-06-29 18:26:33.250941+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:33.250941, (GMT): 2024-06-29 18:26:33.250941+00:00\n",
      "2024-06-29 13:26:33,258 - micro - MainProcess - INFO     CPU usage: 12.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.5%\n",
      "2024-06-29 13:26:33,281 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:33,336 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:33,345 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,352 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.3514612 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.3514612 Artificial intelligence (AI) is becoming increasingly important in cybersecurity to detect and prevent threats. How is artificial intelligence being used to enhance cybersecurity measures? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:26:33,361 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,365 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:33.365877, (GMT): 2024-06-29 18:26:33.365877+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:33.365877, (GMT): 2024-06-29 18:26:33.365877+00:00\n",
      "2024-06-29 13:26:33,637 - micro - MainProcess - INFO     CPU usage: 31.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 31.2%\n",
      "2024-06-29 13:26:33,647 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:33,667 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:33,674 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,678 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.6784482 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 80 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685593.6784482 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 80\n",
      "2024-06-29 13:26:33,683 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:33,686 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:33.686981, (GMT): 2024-06-29 18:26:33.686981+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:33.686981, (GMT): 2024-06-29 18:26:33.686981+00:00\n",
      "2024-06-29 13:26:33,783 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:33,813 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.62 seconds or 3618.71 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.62 seconds or 3618.71 milliseconds.\n",
      "2024-06-29 13:26:33,860 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:33,871 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:33,895 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.41 seconds or 3408.66 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.41 seconds or 3408.66 milliseconds.\n",
      "2024-06-29 13:26:33,941 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:34,021 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:34,052 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.64 seconds or 9644.94 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.64 seconds or 9644.94 milliseconds.\n",
      "2024-06-29 13:26:34,108 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:34,337 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:34,370 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.05 seconds or 4049.77 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.05 seconds or 4049.77 milliseconds.\n",
      "2024-06-29 13:26:34,417 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:34,875 - micro - MainProcess - INFO     CPU usage: 19.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.5%\n",
      "2024-06-29 13:26:34,885 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:26:34,906 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:34,912 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:34,915 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685594.9159925 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685594.9159925 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:26:34,928 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:34,931 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:34.931549, (GMT): 2024-06-29 18:26:34.931549+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:34.931549, (GMT): 2024-06-29 18:26:34.931549+00:00\n",
      "2024-06-29 13:26:34,939 - micro - MainProcess - INFO     CPU usage: 27.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 27.3%\n",
      "2024-06-29 13:26:34,949 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:26:34,974 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:34,980 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:34,985 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685594.9844363 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 64 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685594.9844363 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 64\n",
      "2024-06-29 13:26:34,994 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:34,997 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:34.997792, (GMT): 2024-06-29 18:26:34.997792+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:34.997792, (GMT): 2024-06-29 18:26:34.997792+00:00\n",
      "2024-06-29 13:26:35,122 - micro - MainProcess - INFO     CPU usage: 60.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 60.1%\n",
      "2024-06-29 13:26:35,136 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:35,153 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:35,160 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:35,164 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685595.1643531 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 78 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685595.1643531 E-commerce has grown rapidly, especially during the COVID-19 pandemic, changing the way we shop. What are the key factors driving the growth of e-commerce, and how is it changing retail? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 78\n",
      "2024-06-29 13:26:35,171 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:35,174 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:35.174126, (GMT): 2024-06-29 18:26:35.174126+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:35.174126, (GMT): 2024-06-29 18:26:35.174126+00:00\n",
      "2024-06-29 13:26:35,423 - micro - MainProcess - INFO     CPU usage: 11.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 11.4%\n",
      "2024-06-29 13:26:35,435 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:35,452 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:35,460 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:35,464 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685595.4640648 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685595.4640648 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:26:35,472 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:35,475 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:35.475607, (GMT): 2024-06-29 18:26:35.475607+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:35.475607, (GMT): 2024-06-29 18:26:35.475607+00:00\n",
      "2024-06-29 13:26:37,176 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:37,208 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.52 seconds or 3518.32 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.52 seconds or 3518.32 milliseconds.\n",
      "2024-06-29 13:26:37,263 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:38,280 - micro - MainProcess - INFO     CPU usage: 12.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.0%\n",
      "2024-06-29 13:26:38,291 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:38,316 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:38,325 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:38,332 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685598.3289857 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685598.3289857 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:26:38,340 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:38,344 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:38.344655, (GMT): 2024-06-29 18:26:38.344655+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:38.344655, (GMT): 2024-06-29 18:26:38.344655+00:00\n",
      "2024-06-29 13:26:38,824 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:38,852 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.85 seconds or 3849.65 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.85 seconds or 3849.65 milliseconds.\n",
      "2024-06-29 13:26:38,930 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:39,310 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:39,344 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.87 seconds or 3866.48 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.87 seconds or 3866.48 milliseconds.\n",
      "2024-06-29 13:26:39,393 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:39,925 - micro - MainProcess - INFO     CPU usage: 25.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.2%\n",
      "2024-06-29 13:26:39,937 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:39,963 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:39,972 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:39,975 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685599.9758494 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685599.9758494 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:26:39,981 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:39,986 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:39.986385, (GMT): 2024-06-29 18:26:39.986385+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:39.986385, (GMT): 2024-06-29 18:26:39.986385+00:00\n",
      "2024-06-29 13:26:40,203 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:40,237 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.04 seconds or 7043.02 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.04 seconds or 7043.02 milliseconds.\n",
      "2024-06-29 13:26:40,284 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:40,289 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:40,292 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:40,301 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:40,306 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.94 seconds or 6935.93 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.94 seconds or 6935.93 milliseconds.\n",
      "2024-06-29 13:26:40,357 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:40,364 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.11 seconds or 7107.95 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.11 seconds or 7107.95 milliseconds.\n",
      "2024-06-29 13:26:40,412 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:40,419 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.48 seconds or 5483.08 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.48 seconds or 5483.08 milliseconds.\n",
      "2024-06-29 13:26:40,473 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:40,477 - micro - MainProcess - INFO     CPU usage: 29.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 29.1%\n",
      "2024-06-29 13:26:40,489 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:40,511 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:40,516 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:40,521 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685600.519687 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 61 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685600.519687 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 61\n",
      "2024-06-29 13:26:40,527 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:40,530 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:40.530983, (GMT): 2024-06-29 18:26:40.530983+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:40.530983, (GMT): 2024-06-29 18:26:40.530983+00:00\n",
      "2024-06-29 13:26:41,303 - micro - MainProcess - INFO     CPU usage: 19.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.6%\n",
      "2024-06-29 13:26:41,314 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:41,366 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:41,373 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,377 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.3775523 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.3775523 Data privacy is a growing concern as more personal information is collected and stored online. Can you discuss the importance of data privacy and how individuals can protect their information? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:26:41,384 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,389 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:41.389078, (GMT): 2024-06-29 18:26:41.389078+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:41.389078, (GMT): 2024-06-29 18:26:41.389078+00:00\n",
      "2024-06-29 13:26:41,395 - micro - MainProcess - INFO     CPU usage: 30.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 30.3%\n",
      "2024-06-29 13:26:41,407 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:41,441 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:41,448 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,453 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.4533148 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.4533148 Smart contracts are self-executing contracts with the terms of the agreement directly written into code. Can you explain the concept of smart contracts and their use in blockchain technology? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:26:41,460 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,463 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:41.463852, (GMT): 2024-06-29 18:26:41.463852+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:41.463852, (GMT): 2024-06-29 18:26:41.463852+00:00\n",
      "2024-06-29 13:26:41,474 - micro - MainProcess - INFO     CPU usage: 16.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.4%\n",
      "2024-06-29 13:26:41,488 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:41,515 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:41,523 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,530 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.5291536 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 80 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.5291536 Climate change is one of the most pressing issues of our time, affecting ecosystems, weather patterns, and sea levels. What are the major challenges in combating climate change, and what can individuals do to help? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 80\n",
      "2024-06-29 13:26:41,537 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,555 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:41.555436, (GMT): 2024-06-29 18:26:41.555436+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:41.555436, (GMT): 2024-06-29 18:26:41.555436+00:00\n",
      "2024-06-29 13:26:41,563 - micro - MainProcess - INFO     CPU usage: 19.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.4%\n",
      "2024-06-29 13:26:41,576 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:41,596 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:41,603 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,609 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.6083193 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685601.6083193 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:26:41,615 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:41,618 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:41.618553, (GMT): 2024-06-29 18:26:41.618553+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:41.618553, (GMT): 2024-06-29 18:26:41.618553+00:00\n",
      "2024-06-29 13:26:42,106 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:42,147 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.8 seconds or 3799.51 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.8 seconds or 3799.51 milliseconds.\n",
      "2024-06-29 13:26:42,197 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:42,303 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:42,332 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.15 seconds or 7154.46 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.15 seconds or 7154.46 milliseconds.\n",
      "2024-06-29 13:26:42,475 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:43,195 - micro - MainProcess - INFO     CPU usage: 21.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 21.6%\n",
      "2024-06-29 13:26:43,207 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:43,225 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:43,232 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:43,236 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685603.236134 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 65 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685603.236134 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 65\n",
      "2024-06-29 13:26:43,249 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:43,251 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:43.251915, (GMT): 2024-06-29 18:26:43.251915+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:43.251915, (GMT): 2024-06-29 18:26:43.251915+00:00\n",
      "2024-06-29 13:26:43,473 - micro - MainProcess - INFO     CPU usage: 11.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 11.8%\n",
      "2024-06-29 13:26:43,484 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:43,506 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:43,514 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:43,518 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685603.5184133 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 64 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685603.5184133 Big data analytics involves examining large datasets to uncover hidden patterns and insights. How do big data analytics help businesses make better decisions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 64\n",
      "2024-06-29 13:26:43,525 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:43,528 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:43.528638, (GMT): 2024-06-29 18:26:43.528638+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:43.528638, (GMT): 2024-06-29 18:26:43.528638+00:00\n",
      "2024-06-29 13:26:43,696 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:43,727 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.74 seconds or 3735.42 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.74 seconds or 3735.42 milliseconds.\n",
      "2024-06-29 13:26:43,781 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:44,322 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:44,351 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.81 seconds or 3809.84 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.81 seconds or 3809.84 milliseconds.\n",
      "2024-06-29 13:26:44,449 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:44,785 - micro - MainProcess - INFO     CPU usage: 27.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 27.9%\n",
      "2024-06-29 13:26:44,810 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:44,852 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:44,861 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:44,866 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685604.8663273 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685604.8663273 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:26:44,872 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:44,876 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:44.875762, (GMT): 2024-06-29 18:26:44.875762+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:44.875762, (GMT): 2024-06-29 18:26:44.875762+00:00\n",
      "2024-06-29 13:26:45,188 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:45,218 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.6 seconds or 3595.87 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.6 seconds or 3595.87 milliseconds.\n",
      "2024-06-29 13:26:45,352 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:45,452 - micro - MainProcess - INFO     CPU usage: 33.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 33.3%\n",
      "2024-06-29 13:26:45,462 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:45,479 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:45,485 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:45,490 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685605.4904156 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685605.4904156 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:26:45,496 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:45,499 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:45.499735, (GMT): 2024-06-29 18:26:45.499735+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:45.499735, (GMT): 2024-06-29 18:26:45.499735+00:00\n",
      "2024-06-29 13:26:46,359 - micro - MainProcess - INFO     CPU usage: 13.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.9%\n",
      "2024-06-29 13:26:46,369 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:46,395 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:46,407 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:46,411 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685606.4101374 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685606.4101374 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:26:46,415 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:46,418 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:46.418423, (GMT): 2024-06-29 18:26:46.418423+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:46.418423, (GMT): 2024-06-29 18:26:46.418423+00:00\n",
      "2024-06-29 13:26:46,652 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:46,682 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.43 seconds or 3426.43 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.43 seconds or 3426.43 milliseconds.\n",
      "2024-06-29 13:26:46,730 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:47,193 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:47,226 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.67 seconds or 5665.12 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.67 seconds or 5665.12 milliseconds.\n",
      "2024-06-29 13:26:47,287 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:47,737 - micro - MainProcess - INFO     CPU usage: 14.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.9%\n",
      "2024-06-29 13:26:47,749 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:47,769 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:47,777 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:47,786 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685607.7863562 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685607.7863562 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75\n",
      "2024-06-29 13:26:47,795 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:47,820 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:47.820913, (GMT): 2024-06-29 18:26:47.820913+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:47.820913, (GMT): 2024-06-29 18:26:47.820913+00:00\n",
      "2024-06-29 13:26:48,296 - micro - MainProcess - INFO     CPU usage: 50.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 50.1%\n",
      "2024-06-29 13:26:48,334 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:48,360 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:48,368 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:48,374 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685608.374412 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685608.374412 Machine learning is a subset of artificial intelligence where algorithms learn from data to make predictions or decisions. How do machine learning algorithms improve over time, and what are some common applications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:26:48,382 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:48,385 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:48.385779, (GMT): 2024-06-29 18:26:48.385779+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:48.385779, (GMT): 2024-06-29 18:26:48.385779+00:00\n",
      "2024-06-29 13:26:48,435 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:48,444 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.05 seconds or 7050.35 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.05 seconds or 7050.35 milliseconds.\n",
      "2024-06-29 13:26:48,493 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:48,503 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:48,509 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:48,516 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.05 seconds or 7047.98 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.05 seconds or 7047.98 milliseconds.\n",
      "2024-06-29 13:26:49,577 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:49,581 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.7 seconds or 4699.19 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.7 seconds or 4699.19 milliseconds.\n",
      "2024-06-29 13:26:49,634 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:49,649 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:49,657 - micro - MainProcess - INFO     CPU usage: 10.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.9%\n",
      "2024-06-29 13:26:49,669 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:49,685 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:49,694 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:49,701 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685609.7014658 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 62 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685609.7014658 Quantum cryptography uses the principles of quantum mechanics to secure communication. How is quantum cryptography enhancing the security of digital communications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 62\n",
      "2024-06-29 13:26:49,709 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:49,720 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:49.720392, (GMT): 2024-06-29 18:26:49.720392+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:49.720392, (GMT): 2024-06-29 18:26:49.720392+00:00\n",
      "2024-06-29 13:26:49,742 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.24 seconds or 4240.11 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.24 seconds or 4240.11 milliseconds.\n",
      "2024-06-29 13:26:49,821 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:49,911 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:49,943 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.52 seconds or 3521.56 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.52 seconds or 3521.56 milliseconds.\n",
      "2024-06-29 13:26:50,002 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:50,605 - micro - MainProcess - INFO     CPU usage: 24.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 24.6%\n",
      "2024-06-29 13:26:50,615 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:50,647 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:50,655 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:50,659 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685610.6597655 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685610.6597655 3D printing, also known as additive manufacturing, creates objects layer by layer from digital models. What are some innovative uses of 3D printing in manufacturing and healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:26:50,665 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:50,669 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:50.669457, (GMT): 2024-06-29 18:26:50.669457+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:50.669457, (GMT): 2024-06-29 18:26:50.669457+00:00\n",
      "2024-06-29 13:26:50,676 - micro - MainProcess - INFO     CPU usage: 10.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.5%\n",
      "2024-06-29 13:26:50,689 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:50,705 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:50,711 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:50,715 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685610.714943 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685610.714943 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:26:50,722 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:50,726 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:50.726688, (GMT): 2024-06-29 18:26:50.726688+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:50.726688, (GMT): 2024-06-29 18:26:50.726688+00:00\n",
      "2024-06-29 13:26:50,839 - micro - MainProcess - INFO     CPU usage: 26.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 26.6%\n",
      "2024-06-29 13:26:50,849 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:50,869 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:50,875 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:50,881 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685610.880883 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685610.880883 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:26:50,889 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:50,893 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:50.893465, (GMT): 2024-06-29 18:26:50.893465+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:50.893465, (GMT): 2024-06-29 18:26:50.893465+00:00\n",
      "2024-06-29 13:26:50,910 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:50,932 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.4 seconds or 7400.72 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.4 seconds or 7400.72 milliseconds.\n",
      "2024-06-29 13:26:50,979 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:51,022 - micro - MainProcess - INFO     CPU usage: 10.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.6%\n",
      "2024-06-29 13:26:51,032 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:51,047 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:51,055 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:51,060 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685611.0605555 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685611.0605555 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75\n",
      "2024-06-29 13:26:51,066 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:51,070 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:51.070582, (GMT): 2024-06-29 18:26:51.070582+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:51.070582, (GMT): 2024-06-29 18:26:51.070582+00:00\n",
      "2024-06-29 13:26:51,077 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:51,097 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.24 seconds or 3243.05 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.24 seconds or 3243.05 milliseconds.\n",
      "2024-06-29 13:26:51,142 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:51,969 - micro - MainProcess - INFO     CPU usage: 10.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.7%\n",
      "2024-06-29 13:26:51,985 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:26:52,026 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:52,031 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:52,035 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685612.0358863 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685612.0358863 Precision medicine tailors medical treatment to the individual characteristics of each patient. Can you explain the concept of precision medicine and its advantages in treating diseases? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:26:52,043 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:52,046 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:52.046185, (GMT): 2024-06-29 18:26:52.046185+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:52.046185, (GMT): 2024-06-29 18:26:52.046185+00:00\n",
      "2024-06-29 13:26:52,154 - micro - MainProcess - INFO     CPU usage: 37.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 37.7%\n",
      "2024-06-29 13:26:52,163 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:52,183 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:52,188 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:52,192 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685612.1917856 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685612.1917856 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:26:52,198 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:52,202 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:52.202308, (GMT): 2024-06-29 18:26:52.202308+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:52.202308, (GMT): 2024-06-29 18:26:52.202308+00:00\n",
      "2024-06-29 13:26:53,884 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:53,915 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.19 seconds or 3186.71 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.19 seconds or 3186.71 milliseconds.\n",
      "2024-06-29 13:26:53,975 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:54,676 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:54,712 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.82 seconds or 3815.06 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.82 seconds or 3815.06 milliseconds.\n",
      "2024-06-29 13:26:54,774 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:54,789 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:54,824 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.75 seconds or 3750.4 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.75 seconds or 3750.4 milliseconds.\n",
      "2024-06-29 13:26:54,885 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:54,967 - micro - MainProcess - INFO     CPU usage: 13.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.5%\n",
      "2024-06-29 13:26:54,979 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:54,999 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:55,006 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:55,012 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685615.0092025 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685615.0092025 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:26:55,017 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:55,021 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:55.021669, (GMT): 2024-06-29 18:26:55.021669+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:55.021669, (GMT): 2024-06-29 18:26:55.021669+00:00\n",
      "2024-06-29 13:26:55,781 - micro - MainProcess - INFO     CPU usage: 9.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 9.7%\n",
      "2024-06-29 13:26:55,792 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:55,818 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:55,823 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:55,826 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685615.8263905 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685615.8263905 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:26:55,834 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:55,837 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:55.837086, (GMT): 2024-06-29 18:26:55.837086+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:55.837086, (GMT): 2024-06-29 18:26:55.837086+00:00\n",
      "2024-06-29 13:26:55,862 - micro - MainProcess - INFO     CPU usage: 17.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 17.2%\n",
      "2024-06-29 13:26:55,874 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:55,889 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:55,895 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:55,899 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685615.8991365 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685615.8991365 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:26:55,905 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:55,910 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:55.910338, (GMT): 2024-06-29 18:26:55.910338+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:55.910338, (GMT): 2024-06-29 18:26:55.910338+00:00\n",
      "2024-06-29 13:26:55,983 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:56,016 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.81 seconds or 3808.45 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.81 seconds or 3808.45 milliseconds.\n",
      "2024-06-29 13:26:56,066 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:56,626 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:56,677 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.29 seconds or 8286.96 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.29 seconds or 8286.96 milliseconds.\n",
      "2024-06-29 13:26:56,721 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:56,852 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:56,884 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.14 seconds or 7141.66 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.14 seconds or 7141.66 milliseconds.\n",
      "2024-06-29 13:26:56,932 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:57,080 - micro - MainProcess - INFO     CPU usage: 14.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.7%\n",
      "2024-06-29 13:26:57,107 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:26:57,125 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:57,133 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:57,141 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685617.1405592 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685617.1405592 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:26:57,147 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:57,150 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:57.150545, (GMT): 2024-06-29 18:26:57.150545+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:57.150545, (GMT): 2024-06-29 18:26:57.150545+00:00\n",
      "2024-06-29 13:26:57,741 - micro - MainProcess - INFO     CPU usage: 20.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.8%\n",
      "2024-06-29 13:26:57,752 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:57,773 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:57,780 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:57,784 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685617.7847874 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685617.7847874 As artificial intelligence becomes more integrated into our daily lives, ethical considerations become increasingly important. Can you discuss the ethical considerations surrounding artificial intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:26:57,791 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:57,794 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:57.794950, (GMT): 2024-06-29 18:26:57.794950+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:57.794950, (GMT): 2024-06-29 18:26:57.794950+00:00\n",
      "2024-06-29 13:26:57,801 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:57,809 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.14 seconds or 7135.6 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.14 seconds or 7135.6 milliseconds.\n",
      "2024-06-29 13:26:57,898 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:57,908 - micro - MainProcess - INFO     CPU usage: 16.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.1%\n",
      "2024-06-29 13:26:57,918 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:57,936 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:57,943 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:57,949 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685617.9494102 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685617.9494102 Artificial general intelligence (AGI) aims to create machines that can perform any intellectual task that a human can. What are some current challenges in the development of artificial general intelligence? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:26:57,955 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:57,959 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:57.959942, (GMT): 2024-06-29 18:26:57.959942+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:57.959942, (GMT): 2024-06-29 18:26:57.959942+00:00\n",
      "2024-06-29 13:26:58,108 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:58,141 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.11 seconds or 3114.76 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.11 seconds or 3114.76 milliseconds.\n",
      "2024-06-29 13:26:58,186 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:58,910 - micro - MainProcess - INFO     CPU usage: 16.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.4%\n",
      "2024-06-29 13:26:58,920 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:26:58,946 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:58,952 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:58,959 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685618.9590955 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685618.9590955 Augmented reality (AR) is being used in retail to create interactive shopping experiences for customers. How is augmented reality being integrated into retail experiences to enhance customer engagement? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:26:58,966 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:58,969 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:58.969620, (GMT): 2024-06-29 18:26:58.969620+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:26:58.969620, (GMT): 2024-06-29 18:26:58.969620+00:00\n",
      "2024-06-29 13:26:59,043 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:59,075 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.03 seconds or 7025.37 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.03 seconds or 7025.37 milliseconds.\n",
      "2024-06-29 13:26:59,290 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:59,305 - micro - MainProcess - INFO     CPU usage: 42.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 42.8%\n",
      "2024-06-29 13:26:59,319 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:26:59,346 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:26:59,354 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:59,360 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685619.3601453 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685619.3601453 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:26:59,368 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:26:59,371 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:59.371326, (GMT): 2024-06-29 18:26:59.371326+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:26:59.371326, (GMT): 2024-06-29 18:26:59.371326+00:00\n",
      "2024-06-29 13:26:59,381 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:59,390 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.55 seconds or 3548.31 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.55 seconds or 3548.31 milliseconds.\n",
      "2024-06-29 13:26:59,435 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:26:59,439 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:26:59,447 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.53 seconds or 3533.97 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.53 seconds or 3533.97 milliseconds.\n",
      "2024-06-29 13:26:59,499 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:00,282 - micro - MainProcess - INFO     CPU usage: 18.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 18.4%\n",
      "2024-06-29 13:27:00,297 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:00,321 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:00,327 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:00,330 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685620.3300219 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685620.3300219 Smart cities use technology to improve urban living conditions, making cities more efficient and sustainable. What are some examples of how smart cities are improving urban living conditions? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:27:00,338 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:00,341 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:00.341219, (GMT): 2024-06-29 18:27:00.341219+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:00.341219, (GMT): 2024-06-29 18:27:00.341219+00:00\n",
      "2024-06-29 13:27:00,451 - micro - MainProcess - INFO     CPU usage: 13.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.8%\n",
      "2024-06-29 13:27:00,462 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:27:00,504 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:00,516 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:00,521 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685620.5218508 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685620.5218508 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75\n",
      "2024-06-29 13:27:00,528 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:00,531 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:00.531787, (GMT): 2024-06-29 18:27:00.531787+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:00.531787, (GMT): 2024-06-29 18:27:00.531787+00:00\n",
      "2024-06-29 13:27:00,538 - micro - MainProcess - INFO     CPU usage: 28.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 28.8%\n",
      "2024-06-29 13:27:00,551 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:27:00,596 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:00,607 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:00,611 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685620.6113238 Sustainable development seeks to meet the needs of the present without compromising the ability of future generations to meet their own needs. What are some challenges and solutions in implementing sustainable development practices? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685620.6113238 Sustainable development seeks to meet the needs of the present without compromising the ability of future generations to meet their own needs. What are some challenges and solutions in implementing sustainable development practices? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:27:00,617 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:00,623 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:00.623052, (GMT): 2024-06-29 18:27:00.623052+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:00.623052, (GMT): 2024-06-29 18:27:00.623052+00:00\n",
      "2024-06-29 13:27:00,728 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:00,857 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3702.21 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3702.21 milliseconds.\n",
      "2024-06-29 13:27:00,916 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:01,923 - micro - MainProcess - INFO     CPU usage: 21.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 21.5%\n",
      "2024-06-29 13:27:01,935 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:01,962 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:01,970 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:01,974 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685621.9734912 The Internet of Things (IoT) refers to a network of interconnected devices that communicate and share data. Can you explain the concept of the Internet of Things and provide some real-world examples? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685621.9734912 The Internet of Things (IoT) refers to a network of interconnected devices that communicate and share data. Can you explain the concept of the Internet of Things and provide some real-world examples? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:27:01,982 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:01,986 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:01.986537, (GMT): 2024-06-29 18:27:01.986537+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:01.986537, (GMT): 2024-06-29 18:27:01.986537+00:00\n",
      "2024-06-29 13:27:02,975 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:02,993 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.62 seconds or 3619.09 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.62 seconds or 3619.09 milliseconds.\n",
      "2024-06-29 13:27:03,048 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:04,055 - micro - MainProcess - INFO     CPU usage: 14.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.5%\n",
      "2024-06-29 13:27:04,066 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:27:04,085 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:04,092 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:04,095 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685624.0954204 Natural language processing (NLP) enables computers to understand and generate human language. How does natural language processing enable computers to understand human language? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685624.0954204 Natural language processing (NLP) enables computers to understand and generate human language. How does natural language processing enable computers to understand human language? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:27:04,104 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:04,109 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:04.108181, (GMT): 2024-06-29 18:27:04.108181+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:04.108181, (GMT): 2024-06-29 18:27:04.108181+00:00\n",
      "2024-06-29 13:27:04,563 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:04,596 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:04,600 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.97 seconds or 3971.6 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.97 seconds or 3971.6 milliseconds.\n",
      "2024-06-29 13:27:04,653 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:04,657 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.12 seconds or 4120.91 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.12 seconds or 4120.91 milliseconds.\n",
      "2024-06-29 13:27:04,705 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:04,752 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:04,784 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.98 seconds or 6983.96 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.98 seconds or 6983.96 milliseconds.\n",
      "2024-06-29 13:27:04,862 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:05,549 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:05,582 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.59 seconds or 3592.32 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.59 seconds or 3592.32 milliseconds.\n",
      "2024-06-29 13:27:05,640 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:05,645 - micro - MainProcess - INFO     CPU usage: 19.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.6%\n",
      "2024-06-29 13:27:05,656 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:05,672 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:05,680 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:05,694 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685625.6931374 Social media trends influence consumer behavior and marketing strategies, shaping how brands interact with their audiences. Can you discuss the impact of social media trends on consumer behavior and marketing? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685625.6931374 Social media trends influence consumer behavior and marketing strategies, shaping how brands interact with their audiences. Can you discuss the impact of social media trends on consumer behavior and marketing? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:27:05,701 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:05,704 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:05.704883, (GMT): 2024-06-29 18:27:05.704883+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:05.704883, (GMT): 2024-06-29 18:27:05.704883+00:00\n",
      "2024-06-29 13:27:05,743 - micro - MainProcess - INFO     CPU usage: 22.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 22.2%\n",
      "2024-06-29 13:27:05,760 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:05,814 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:05,829 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:05,834 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685625.834339 The circular economy aims to minimize waste and make the most of resources by creating closed-loop systems. What are the benefits of a circular economy, and how does it promote sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685625.834339 The circular economy aims to minimize waste and make the most of resources by creating closed-loop systems. What are the benefits of a circular economy, and how does it promote sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:27:05,842 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:05,845 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:05.845706, (GMT): 2024-06-29 18:27:05.845706+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:05.845706, (GMT): 2024-06-29 18:27:05.845706+00:00\n",
      "2024-06-29 13:27:05,862 - micro - MainProcess - INFO     CPU usage: 70.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 70.0%\n",
      "2024-06-29 13:27:05,984 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:06,007 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:06,015 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:06,020 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685626.020871 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685626.020871 Autonomous vehicles, or self-driving cars, have the potential to revolutionize transportation, but they also pose significant challenges. What are the benefits and risks of autonomous vehicles on our roads? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:27:06,027 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:06,030 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:06.030969, (GMT): 2024-06-29 18:27:06.030969+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:06.030969, (GMT): 2024-06-29 18:27:06.030969+00:00\n",
      "2024-06-29 13:27:06,044 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:06,051 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.08 seconds or 7076.91 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.08 seconds or 7076.91 milliseconds.\n",
      "2024-06-29 13:27:06,113 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:06,666 - micro - MainProcess - INFO     CPU usage: 24.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 24.6%\n",
      "2024-06-29 13:27:06,680 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:06,701 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:06,708 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:06,713 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685626.7121258 Cybersecurity is crucial in protecting our personal and professional data from malicious attacks. How does cybersecurity protect our personal data, and what measures can individuals take? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685626.7121258 Cybersecurity is crucial in protecting our personal and professional data from malicious attacks. How does cybersecurity protect our personal data, and what measures can individuals take? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69\n",
      "2024-06-29 13:27:06,717 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:06,719 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:06.719653, (GMT): 2024-06-29 18:27:06.719653+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:06.719653, (GMT): 2024-06-29 18:27:06.719653+00:00\n",
      "2024-06-29 13:27:07,127 - micro - MainProcess - INFO     CPU usage: 9.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 9.4%\n",
      "2024-06-29 13:27:07,138 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:07,157 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:07,162 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:07,167 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685627.166653 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685627.166653 Electric vehicles (EVs) offer a cleaner alternative to traditional gasoline-powered cars, but their widespread adoption poses challenges. What are the potential environmental impacts of widespread adoption of electric vehicles? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:27:07,175 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:07,187 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:07.187544, (GMT): 2024-06-29 18:27:07.187544+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:07.187544, (GMT): 2024-06-29 18:27:07.187544+00:00\n",
      "2024-06-29 13:27:07,453 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:07,457 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.35 seconds or 3345.47 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.35 seconds or 3345.47 milliseconds.\n",
      "2024-06-29 13:27:07,585 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:07,591 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:07,596 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.25 seconds or 7251.62 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.25 seconds or 7251.62 milliseconds.\n",
      "2024-06-29 13:27:07,642 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:07,868 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:07,901 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.94 seconds or 9936.33 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.94 seconds or 9936.33 milliseconds.\n",
      "2024-06-29 13:27:07,949 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:08,583 - micro - MainProcess - INFO     CPU usage: 17.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 17.9%\n",
      "2024-06-29 13:27:08,593 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:08,608 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:08,614 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:08,624 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685628.6248825 Drones are being used in various industries, from agriculture to logistics, due to their versatility. What are the potential uses of drones in agriculture and other industries? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685628.6248825 Drones are being used in various industries, from agriculture to logistics, due to their versatility. What are the potential uses of drones in agriculture and other industries? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:27:08,632 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:08,636 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:08.636938, (GMT): 2024-06-29 18:27:08.636938+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:08.636938, (GMT): 2024-06-29 18:27:08.636938+00:00\n",
      "2024-06-29 13:27:08,643 - micro - MainProcess - INFO     CPU usage: 6.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 6.1%\n",
      "2024-06-29 13:27:08,657 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:08,694 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:08,705 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:08,710 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685628.710316 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685628.710316 Fintech innovations are transforming the financial industry, making services more accessible and efficient. How is fintech innovation changing the banking and finance industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:27:08,716 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:08,719 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:08.719970, (GMT): 2024-06-29 18:27:08.719970+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:08.719970, (GMT): 2024-06-29 18:27:08.719970+00:00\n",
      "2024-06-29 13:27:08,951 - micro - MainProcess - INFO     CPU usage: 10.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.4%\n",
      "2024-06-29 13:27:08,963 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:27:08,984 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:08,990 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:08,995 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685628.9956481 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685628.9956481 Nanotechnology involves manipulating matter at the atomic and molecular scale for various applications. How is nanotechnology being used in medical treatments and drug delivery systems? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:27:09,002 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:09,007 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:09.007581, (GMT): 2024-06-29 18:27:09.007581+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:09.007581, (GMT): 2024-06-29 18:27:09.007581+00:00\n",
      "2024-06-29 13:27:09,471 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:09,474 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.76 seconds or 3763.07 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.76 seconds or 3763.07 milliseconds.\n",
      "2024-06-29 13:27:09,537 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:09,541 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:09,546 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3695.75 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3695.75 milliseconds.\n",
      "2024-06-29 13:27:09,595 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:10,146 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:10,178 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.45 seconds or 3454.79 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.45 seconds or 3454.79 milliseconds.\n",
      "2024-06-29 13:27:10,234 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:10,559 - micro - MainProcess - INFO     CPU usage: 15.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.6%\n",
      "2024-06-29 13:27:10,570 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:10,584 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:10,594 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:10,597 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685630.5975149 Artificial intelligence (AI) is being used to improve the accuracy and efficiency of medical diagnoses. How is artificial intelligence being used to improve the accuracy of medical diagnoses? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685630.5975149 Artificial intelligence (AI) is being used to improve the accuracy and efficiency of medical diagnoses. How is artificial intelligence being used to improve the accuracy of medical diagnoses? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:27:10,611 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:10,614 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:10.614309, (GMT): 2024-06-29 18:27:10.614309+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:10.614309, (GMT): 2024-06-29 18:27:10.614309+00:00\n",
      "2024-06-29 13:27:10,620 - micro - MainProcess - INFO     CPU usage: 15.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.2%\n",
      "2024-06-29 13:27:10,633 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:10,656 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:10,660 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:10,665 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685630.6656785 Agritech innovations are improving agricultural productivity and sustainability through the use of technology. How are advancements in agritech improving food production and sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685630.6656785 Agritech innovations are improving agricultural productivity and sustainability through the use of technology. How are advancements in agritech improving food production and sustainability? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:27:10,679 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:10,691 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:10.691251, (GMT): 2024-06-29 18:27:10.691251+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:10.691251, (GMT): 2024-06-29 18:27:10.691251+00:00\n",
      "2024-06-29 13:27:11,242 - micro - MainProcess - INFO     CPU usage: 12.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.4%\n",
      "2024-06-29 13:27:11,254 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:11,277 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:11,283 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:11,288 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685631.2877903 Renewable energy sources, such as solar and wind power, are essential in reducing our reliance on fossil fuels. What are the environmental benefits of renewable energy sources compared to fossil fuels? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685631.2877903 Renewable energy sources, such as solar and wind power, are essential in reducing our reliance on fossil fuels. What are the environmental benefits of renewable energy sources compared to fossil fuels? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:27:11,293 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:11,296 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:11.296307, (GMT): 2024-06-29 18:27:11.296307+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:11.296307, (GMT): 2024-06-29 18:27:11.296307+00:00\n",
      "2024-06-29 13:27:12,238 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:12,269 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.63 seconds or 3628.62 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.63 seconds or 3628.62 milliseconds.\n",
      "2024-06-29 13:27:12,439 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:12,741 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:12,773 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.74 seconds or 6737.26 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.74 seconds or 6737.26 milliseconds.\n",
      "2024-06-29 13:27:12,829 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:13,455 - micro - MainProcess - INFO     CPU usage: 17.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 17.3%\n",
      "2024-06-29 13:27:13,465 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:13,490 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:13,495 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:13,498 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685633.498373 Cloud computing provides on-demand access to computing resources over the internet. How does cloud computing enhance business operations and data management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 62 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685633.498373 Cloud computing provides on-demand access to computing resources over the internet. How does cloud computing enhance business operations and data management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 62\n",
      "2024-06-29 13:27:13,508 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:13,512 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:13.512118, (GMT): 2024-06-29 18:27:13.512118+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:13.512118, (GMT): 2024-06-29 18:27:13.512118+00:00\n",
      "2024-06-29 13:27:13,724 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:13,757 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.57 seconds or 6566.73 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.57 seconds or 6566.73 milliseconds.\n",
      "2024-06-29 13:27:13,814 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:13,856 - micro - MainProcess - INFO     CPU usage: 20.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.3%\n",
      "2024-06-29 13:27:13,866 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:27:13,907 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:13,912 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:13,916 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685633.916216 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685633.916216 Augmented reality (AR) overlays digital information onto the real world, providing new ways to interact with our environment. How is augmented reality being used in education and training programs? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:27:13,924 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:13,928 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:13.927913, (GMT): 2024-06-29 18:27:13.927913+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:13.927913, (GMT): 2024-06-29 18:27:13.927913+00:00\n",
      "2024-06-29 13:27:14,376 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:14,394 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:14,408 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.71 seconds or 3712.65 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.71 seconds or 3712.65 milliseconds.\n",
      "2024-06-29 13:27:14,462 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:14,470 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.85 seconds or 3854.04 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.85 seconds or 3854.04 milliseconds.\n",
      "2024-06-29 13:27:14,621 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:14,822 - micro - MainProcess - INFO     CPU usage: 23.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.2%\n",
      "2024-06-29 13:27:14,834 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:27:14,850 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:14,856 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:14,860 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685634.8604627 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685634.8604627 Robots are being programmed to perform increasingly complex tasks in various industries, from manufacturing to healthcare. How do robots learn to perform complex tasks, and what are some examples of their use in industry? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:27:14,866 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:14,870 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:14.870999, (GMT): 2024-06-29 18:27:14.870999+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:14.870999, (GMT): 2024-06-29 18:27:14.870999+00:00\n",
      "2024-06-29 13:27:14,880 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:14,908 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.61 seconds or 3609.57 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.61 seconds or 3609.57 milliseconds.\n",
      "2024-06-29 13:27:14,954 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:15,419 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:15,448 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.72 seconds or 6723.2 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.72 seconds or 6723.2 milliseconds.\n",
      "2024-06-29 13:27:15,502 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:15,508 - micro - MainProcess - INFO     CPU usage: 19.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.4%\n",
      "2024-06-29 13:27:15,521 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:15,555 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:15,561 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:15,564 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685635.5640342 Urban planning involves designing and organizing urban spaces to improve the quality of life for residents. What is the role of urban planning in creating more livable and sustainable cities? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685635.5640342 Urban planning involves designing and organizing urban spaces to improve the quality of life for residents. What is the role of urban planning in creating more livable and sustainable cities? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:27:15,570 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:15,573 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:15.573554, (GMT): 2024-06-29 18:27:15.573554+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:15.573554, (GMT): 2024-06-29 18:27:15.573554+00:00\n",
      "2024-06-29 13:27:15,619 - micro - MainProcess - INFO     CPU usage: 25.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.9%\n",
      "2024-06-29 13:27:15,632 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:15,658 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:15,668 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:15,672 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685635.672562 Blockchain technology offers transparent and secure ways to track products through the supply chain. What are the key benefits of using blockchain for supply chain management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685635.672562 Blockchain technology offers transparent and secure ways to track products through the supply chain. What are the key benefits of using blockchain for supply chain management? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:27:15,678 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:15,681 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:15.681813, (GMT): 2024-06-29 18:27:15.681813+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:15.681813, (GMT): 2024-06-29 18:27:15.681813+00:00\n",
      "2024-06-29 13:27:15,691 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:15,735 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.72 seconds or 6724.28 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.72 seconds or 6724.28 milliseconds.\n",
      "2024-06-29 13:27:15,790 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:15,960 - micro - MainProcess - INFO     CPU usage: 49.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 49.6%\n",
      "2024-06-29 13:27:15,972 - micro - MainProcess - INFO     RAM usage: 86.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.2%\n",
      "2024-06-29 13:27:15,995 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:16,005 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:16,011 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685636.009077 Deep learning, a subset of machine learning, uses neural networks with many layers to analyze complex data. Can you describe the process of deep learning and how it differs from traditional machine learning? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685636.009077 Deep learning, a subset of machine learning, uses neural networks with many layers to analyze complex data. Can you describe the process of deep learning and how it differs from traditional machine learning? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 75\n",
      "2024-06-29 13:27:16,018 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:16,024 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:16.024972, (GMT): 2024-06-29 18:27:16.024972+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:16.024972, (GMT): 2024-06-29 18:27:16.024972+00:00\n",
      "2024-06-29 13:27:16,511 - micro - MainProcess - INFO     CPU usage: 16.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.3%\n",
      "2024-06-29 13:27:16,522 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:27:16,539 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:16,547 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:16,550 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685636.5503407 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685636.5503407 Telemedicine allows healthcare providers to consult with patients remotely, increasing access to care. What are the benefits and challenges of telemedicine for both patients and healthcare providers? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:27:16,557 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:16,565 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:16.563387, (GMT): 2024-06-29 18:27:16.563387+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:16.563387, (GMT): 2024-06-29 18:27:16.563387+00:00\n",
      "2024-06-29 13:27:16,748 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:16,779 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.26 seconds or 3263.65 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.26 seconds or 3263.65 milliseconds.\n",
      "2024-06-29 13:27:16,839 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:16,847 - micro - MainProcess - INFO     CPU usage: 39.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 39.0%\n",
      "2024-06-29 13:27:16,867 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:27:16,887 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:16,896 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:16,904 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685636.903001 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685636.903001 Smart homes use connected devices to automate and control household systems, improving convenience and efficiency. What are the key components of a smart home, and how do they work together? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:27:16,910 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:16,913 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:16.913989, (GMT): 2024-06-29 18:27:16.913989+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:16.913989, (GMT): 2024-06-29 18:27:16.913989+00:00\n",
      "2024-06-29 13:27:17,847 - micro - MainProcess - INFO     CPU usage: 16.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.6%\n",
      "2024-06-29 13:27:17,857 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:27:17,877 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:17,885 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:17,966 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685637.9663312 Digital marketing is evolving with new trends and technologies that help businesses reach their audiences. What are some emerging trends in digital marketing that businesses should be aware of? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685637.9663312 Digital marketing is evolving with new trends and technologies that help businesses reach their audiences. What are some emerging trends in digital marketing that businesses should be aware of? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:27:17,973 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:17,975 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:17.975196, (GMT): 2024-06-29 18:27:17.975196+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:17.975196, (GMT): 2024-06-29 18:27:17.975196+00:00\n",
      "2024-06-29 13:27:18,805 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:18,837 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.15 seconds or 3148.58 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.15 seconds or 3148.58 milliseconds.\n",
      "2024-06-29 13:27:18,889 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:19,064 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:19,096 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.07 seconds or 3065.59 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.07 seconds or 3065.59 milliseconds.\n",
      "2024-06-29 13:27:19,151 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:19,862 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:19,913 - micro - MainProcess - INFO     CPU usage: 14.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.8%\n",
      "2024-06-29 13:27:19,923 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:27:19,943 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:19,951 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:19,955 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685639.9541903 Smart grids use digital technology to manage the production and distribution of electricity more efficiently. How is the development of smart grids contributing to more efficient energy distribution? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685639.9541903 Smart grids use digital technology to manage the production and distribution of electricity more efficiently. How is the development of smart grids contributing to more efficient energy distribution? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69\n",
      "2024-06-29 13:27:19,981 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:19,989 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:19.989391, (GMT): 2024-06-29 18:27:19.989391+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:19.989391, (GMT): 2024-06-29 18:27:19.989391+00:00\n",
      "2024-06-29 13:27:19,994 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.42 seconds or 4417.62 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.42 seconds or 4417.62 milliseconds.\n",
      "2024-06-29 13:27:20,052 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:20,159 - micro - MainProcess - INFO     CPU usage: 35.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 35.8%\n",
      "2024-06-29 13:27:20,170 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:27:20,202 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:20,208 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:20,212 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685640.2129142 The rollout of 5G technology promises faster internet speeds and more reliable connections. What are some potential applications of 5G technology in everyday life? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685640.2129142 The rollout of 5G technology promises faster internet speeds and more reliable connections. What are some potential applications of 5G technology in everyday life? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 69\n",
      "2024-06-29 13:27:20,220 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:20,225 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:20.225447, (GMT): 2024-06-29 18:27:20.225447+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:20.225447, (GMT): 2024-06-29 18:27:20.225447+00:00\n",
      "2024-06-29 13:27:20,847 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:20,876 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.94 seconds or 6942.9 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.94 seconds or 6942.9 milliseconds.\n",
      "2024-06-29 13:27:20,948 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:21,064 - micro - MainProcess - INFO     CPU usage: 12.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.6%\n",
      "2024-06-29 13:27:21,074 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:27:21,091 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:21,097 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:21,101 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685641.1010852 Autonomous drones can operate without human intervention, offering various applications in different industries. How do autonomous drones operate, and what are their potential applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685641.1010852 Autonomous drones can operate without human intervention, offering various applications in different industries. How do autonomous drones operate, and what are their potential applications? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:27:21,106 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:21,122 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:21.122362, (GMT): 2024-06-29 18:27:21.122362+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:21.122362, (GMT): 2024-06-29 18:27:21.122362+00:00\n",
      "2024-06-29 13:27:21,953 - micro - MainProcess - INFO     CPU usage: 20.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.7%\n",
      "2024-06-29 13:27:21,963 - micro - MainProcess - INFO     RAM usage: 86.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.2%\n",
      "2024-06-29 13:27:21,991 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:21,998 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:22,001 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685642.0011678 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685642.0011678 Genetic engineering allows scientists to modify the DNA of organisms, leading to advancements in medicine and agriculture. What are the latest advancements in genetic engineering, and how might they affect healthcare? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:27:22,007 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:22,011 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:22.011062, (GMT): 2024-06-29 18:27:22.011062+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:22.011062, (GMT): 2024-06-29 18:27:22.011062+00:00\n",
      "2024-06-29 13:27:22,201 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:22,230 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.36 seconds or 7356.37 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.36 seconds or 7356.37 milliseconds.\n",
      "2024-06-29 13:27:22,305 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:22,413 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:22,446 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.47 seconds or 4467.91 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.47 seconds or 4467.91 milliseconds.\n",
      "2024-06-29 13:27:22,504 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:23,315 - micro - MainProcess - INFO     CPU usage: 9.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 9.6%\n",
      "2024-06-29 13:27:23,329 - micro - MainProcess - INFO     RAM usage: 86.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.2%\n",
      "2024-06-29 13:27:23,371 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:23,375 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:23,383 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685643.3831103 Sustainable development seeks to meet the needs of the present without compromising the ability of future generations to meet their own needs. What are some challenges and solutions in implementing sustainable development practices? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685643.3831103 Sustainable development seeks to meet the needs of the present without compromising the ability of future generations to meet their own needs. What are some challenges and solutions in implementing sustainable development practices? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:27:23,389 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:23,394 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:23.394419, (GMT): 2024-06-29 18:27:23.394419+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:23.394419, (GMT): 2024-06-29 18:27:23.394419+00:00\n",
      "2024-06-29 13:27:23,515 - micro - MainProcess - INFO     CPU usage: 56.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 56.1%\n",
      "2024-06-29 13:27:23,551 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:27:23,569 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:23,578 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:23,584 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685643.5834563 Genetic modification in crops aims to improve yield, resistance to pests, and nutritional value. Can you explain the principles of genetic modification in crops and its impact on agriculture? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685643.5834563 Genetic modification in crops aims to improve yield, resistance to pests, and nutritional value. Can you explain the principles of genetic modification in crops and its impact on agriculture? Please write a response that should be at least 250 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:27:23,592 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:23,595 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:23.595905, (GMT): 2024-06-29 18:27:23.595905+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-29 13:27:23.595905, (GMT): 2024-06-29 18:27:23.595905+00:00\n",
      "2024-06-29 13:27:23,665 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:23,678 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:23,694 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3700.02 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.7 seconds or 3700.02 milliseconds.\n",
      "2024-06-29 13:27:23,738 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:23,751 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.52 seconds or 3522.3 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.52 seconds or 3522.3 milliseconds.\n",
      "2024-06-29 13:27:23,797 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:23,990 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:24,018 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.45 seconds or 7448.13 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.45 seconds or 7448.13 milliseconds.\n",
      "2024-06-29 13:27:24,070 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:24,229 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:24,256 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.34 seconds or 7338.56 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.34 seconds or 7338.56 milliseconds.\n",
      "2024-06-29 13:27:24,299 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:24,661 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:24,696 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.57 seconds or 3572.21 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.57 seconds or 3572.21 milliseconds.\n",
      "2024-06-29 13:27:24,756 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:25,087 - micro - MainProcess - INFO     CPU usage: 23.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.0%\n",
      "2024-06-29 13:27:25,098 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:27:25,120 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:25,127 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:25,131 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685645.1312191 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685645.1312191 Bioinformatics combines biology, computer science, and information technology to analyze biological data. Can you discuss the role of bioinformatics in modern biology and medical research? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:27:25,139 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:25,142 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:25.142315, (GMT): 2024-06-29 18:27:25.142315+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:25.142315, (GMT): 2024-06-29 18:27:25.142315+00:00\n",
      "2024-06-29 13:27:25,293 - micro - MainProcess - INFO     CPU usage: 46.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 46.9%\n",
      "2024-06-29 13:27:25,305 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:27:25,326 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:25,333 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:25,339 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685645.3395553 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 75 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685645.3395553 Advancements in neuroscience are providing new insights into how the brain functions and how we can treat neurological disorders. Can you describe the advancements in neuroscience that are helping us understand the brain? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 75\n",
      "2024-06-29 13:27:25,345 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:25,349 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:25.349087, (GMT): 2024-06-29 18:27:25.349161+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:25.349087, (GMT): 2024-06-29 18:27:25.349161+00:00\n",
      "2024-06-29 13:27:26,624 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:26,654 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.06 seconds or 3055.08 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.06 seconds or 3055.08 milliseconds.\n",
      "2024-06-29 13:27:26,799 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:29,266 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:29,310 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.3 seconds or 7295.74 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.3 seconds or 7295.74 milliseconds.\n",
      "2024-06-29 13:27:29,379 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:29,405 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:29,433 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.03 seconds or 6034.39 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.03 seconds or 6034.39 milliseconds.\n",
      "2024-06-29 13:27:29,493 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:30,379 - micro - MainProcess - INFO     CPU usage: 17.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 17.5%\n",
      "2024-06-29 13:27:30,390 - micro - MainProcess - INFO     RAM usage: 86.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.3%\n",
      "2024-06-29 13:27:30,412 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:30,420 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:30,425 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685650.4243302 The Internet of Things (IoT) refers to a network of interconnected devices that communicate and share data. Can you explain the concept of the Internet of Things and provide some real-world examples? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685650.4243302 The Internet of Things (IoT) refers to a network of interconnected devices that communicate and share data. Can you explain the concept of the Internet of Things and provide some real-world examples? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 77\n",
      "2024-06-29 13:27:30,431 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:30,433 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:30.433344, (GMT): 2024-06-29 18:27:30.433344+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:30.433344, (GMT): 2024-06-29 18:27:30.433344+00:00\n",
      "2024-06-29 13:27:30,487 - micro - MainProcess - INFO     CPU usage: 18.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 18.4%\n",
      "2024-06-29 13:27:30,497 - micro - MainProcess - INFO     RAM usage: 86.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.3%\n",
      "2024-06-29 13:27:30,521 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:30,528 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:30,531 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685650.5312898 Social media trends influence consumer behavior and marketing strategies, shaping how brands interact with their audiences. Can you discuss the impact of social media trends on consumer behavior and marketing? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685650.5312898 Social media trends influence consumer behavior and marketing strategies, shaping how brands interact with their audiences. Can you discuss the impact of social media trends on consumer behavior and marketing? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:27:30,542 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:30,548 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:30.548054, (GMT): 2024-06-29 18:27:30.548054+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:30.548054, (GMT): 2024-06-29 18:27:30.548054+00:00\n",
      "2024-06-29 13:27:31,147 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:31,152 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.01 seconds or 6006.0 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.01 seconds or 6006.0 milliseconds.\n",
      "2024-06-29 13:27:31,215 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:31,265 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:31,301 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.95 seconds or 5948.87 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.95 seconds or 5948.87 milliseconds.\n",
      "2024-06-29 13:27:31,436 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:32,233 - micro - MainProcess - INFO     CPU usage: 20.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.9%\n",
      "2024-06-29 13:27:32,263 - micro - MainProcess - INFO     RAM usage: 86.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.3%\n",
      "2024-06-29 13:27:32,297 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:32,304 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:32,310 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685652.3087704 Natural language processing (NLP) enables computers to understand and generate human language. How does natural language processing enable computers to understand human language? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685652.3087704 Natural language processing (NLP) enables computers to understand and generate human language. How does natural language processing enable computers to understand human language? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:27:32,317 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:32,322 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:32.322504, (GMT): 2024-06-29 18:27:32.322504+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:32.322504, (GMT): 2024-06-29 18:27:32.322504+00:00\n",
      "2024-06-29 13:27:32,447 - micro - MainProcess - INFO     CPU usage: 40.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 40.5%\n",
      "2024-06-29 13:27:32,457 - micro - MainProcess - INFO     RAM usage: 86.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.4%\n",
      "2024-06-29 13:27:32,497 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:32,502 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:32,508 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685652.5081227 The circular economy aims to minimize waste and make the most of resources by creating closed-loop systems. What are the benefits of a circular economy, and how does it promote sustainability? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685652.5081227 The circular economy aims to minimize waste and make the most of resources by creating closed-loop systems. What are the benefits of a circular economy, and how does it promote sustainability? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 74\n",
      "2024-06-29 13:27:32,515 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:32,519 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:32.519170, (GMT): 2024-06-29 18:27:32.519170+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:32.519170, (GMT): 2024-06-29 18:27:32.519170+00:00\n",
      "2024-06-29 13:27:36,707 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:36,711 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:36,739 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.3 seconds or 6301.74 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.3 seconds or 6301.74 milliseconds.\n",
      "2024-06-29 13:27:36,820 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:36,826 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.27 seconds or 6272.1 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.27 seconds or 6272.1 milliseconds.\n",
      "2024-06-29 13:27:36,876 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:37,823 - micro - MainProcess - INFO     CPU usage: 12.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.9%\n",
      "2024-06-29 13:27:37,833 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:37,851 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:37,857 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:37,861 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685657.8618524 Cybersecurity is crucial in protecting our personal and professional data from malicious attacks. How does cybersecurity protect our personal data, and what measures can individuals take? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 69 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685657.8618524 Cybersecurity is crucial in protecting our personal and professional data from malicious attacks. How does cybersecurity protect our personal data, and what measures can individuals take? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 69\n",
      "2024-06-29 13:27:37,869 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:37,874 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:37.874393, (GMT): 2024-06-29 18:27:37.874393+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:37.874393, (GMT): 2024-06-29 18:27:37.874393+00:00\n",
      "2024-06-29 13:27:37,882 - micro - MainProcess - INFO     CPU usage: 16.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.0%\n",
      "2024-06-29 13:27:37,895 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:27:37,930 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:37,939 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:37,943 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685657.9433637 Artificial intelligence (AI) is being used to improve the accuracy and efficiency of medical diagnoses. How is artificial intelligence being used to improve the accuracy of medical diagnoses? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685657.9433637 Artificial intelligence (AI) is being used to improve the accuracy and efficiency of medical diagnoses. How is artificial intelligence being used to improve the accuracy of medical diagnoses? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 71\n",
      "2024-06-29 13:27:37,948 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:37,952 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:37.952481, (GMT): 2024-06-29 18:27:37.952481+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:37.952481, (GMT): 2024-06-29 18:27:37.952481+00:00\n",
      "2024-06-29 13:27:39,007 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:39,037 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.71 seconds or 6709.28 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.71 seconds or 6709.28 milliseconds.\n",
      "2024-06-29 13:27:39,106 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:39,162 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:39,184 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.66 seconds or 6660.5 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.66 seconds or 6660.5 milliseconds.\n",
      "2024-06-29 13:27:39,295 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:40,108 - micro - MainProcess - INFO     CPU usage: 16.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.9%\n",
      "2024-06-29 13:27:40,118 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:40,132 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:40,139 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:40,142 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685660.142985 Drones are being used in various industries, from agriculture to logistics, due to their versatility. What are the potential uses of drones in agriculture and other industries? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685660.142985 Drones are being used in various industries, from agriculture to logistics, due to their versatility. What are the potential uses of drones in agriculture and other industries? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:27:40,150 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:40,153 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:40.153418, (GMT): 2024-06-29 18:27:40.153418+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:40.153418, (GMT): 2024-06-29 18:27:40.153418+00:00\n",
      "2024-06-29 13:27:40,308 - micro - MainProcess - INFO     CPU usage: 30.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 30.2%\n",
      "2024-06-29 13:27:40,318 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:40,337 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:40,343 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:40,347 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685660.3463156 Agritech innovations are improving agricultural productivity and sustainability through the use of technology. How are advancements in agritech improving food production and sustainability? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685660.3463156 Agritech innovations are improving agricultural productivity and sustainability through the use of technology. How are advancements in agritech improving food production and sustainability? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:27:40,353 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:40,358 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:40.358608, (GMT): 2024-06-29 18:27:40.358608+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:40.358608, (GMT): 2024-06-29 18:27:40.358608+00:00\n",
      "2024-06-29 13:27:44,314 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:44,346 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.47 seconds or 6467.4 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.47 seconds or 6467.4 milliseconds.\n",
      "2024-06-29 13:27:44,424 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:44,797 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:44,829 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.87 seconds or 6873.27 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.87 seconds or 6873.27 milliseconds.\n",
      "2024-06-29 13:27:44,880 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:45,423 - micro - MainProcess - INFO     CPU usage: 10.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.7%\n",
      "2024-06-29 13:27:45,433 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:45,461 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:45,467 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:45,471 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685665.470518 Renewable energy sources, such as solar and wind power, are essential in reducing our reliance on fossil fuels. What are the environmental benefits of renewable energy sources compared to fossil fuels? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685665.470518 Renewable energy sources, such as solar and wind power, are essential in reducing our reliance on fossil fuels. What are the environmental benefits of renewable energy sources compared to fossil fuels? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 73\n",
      "2024-06-29 13:27:45,528 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:45,539 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:45.539582, (GMT): 2024-06-29 18:27:45.539582+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:45.539582, (GMT): 2024-06-29 18:27:45.539582+00:00\n",
      "2024-06-29 13:27:45,889 - micro - MainProcess - INFO     CPU usage: 28.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 28.1%\n",
      "2024-06-29 13:27:45,899 - micro - MainProcess - INFO     RAM usage: 85.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.5%\n",
      "2024-06-29 13:27:45,916 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:45,923 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:45,927 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685665.9275467 Blockchain technology offers transparent and secure ways to track products through the supply chain. What are the key benefits of using blockchain for supply chain management? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685665.9275467 Blockchain technology offers transparent and secure ways to track products through the supply chain. What are the key benefits of using blockchain for supply chain management? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 67\n",
      "2024-06-29 13:27:45,934 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:45,937 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:45.937598, (GMT): 2024-06-29 18:27:45.937598+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:45.937598, (GMT): 2024-06-29 18:27:45.937598+00:00\n",
      "2024-06-29 13:27:46,809 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:46,819 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:46,841 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.68 seconds or 6683.64 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.68 seconds or 6683.64 milliseconds.\n",
      "2024-06-29 13:27:46,889 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:46,896 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.53 seconds or 6533.21 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.53 seconds or 6533.21 milliseconds.\n",
      "2024-06-29 13:27:46,966 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:47,906 - micro - MainProcess - INFO     CPU usage: 16.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.2%\n",
      "2024-06-29 13:27:47,916 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:27:47,942 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:47,950 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:47,953 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685667.9537215 Cloud computing provides on-demand access to computing resources over the internet. How does cloud computing enhance business operations and data management? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 63 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685667.9537215 Cloud computing provides on-demand access to computing resources over the internet. How does cloud computing enhance business operations and data management? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 63\n",
      "2024-06-29 13:27:47,959 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:47,962 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:47.962745, (GMT): 2024-06-29 18:27:47.962745+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:47.962745, (GMT): 2024-06-29 18:27:47.962745+00:00\n",
      "2024-06-29 13:27:47,965 - micro - MainProcess - INFO     CPU usage: 20.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.4%\n",
      "2024-06-29 13:27:47,975 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:27:48,015 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:48,022 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:48,026 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685668.0266707 Urban planning involves designing and organizing urban spaces to improve the quality of life for residents. What is the role of urban planning in creating more livable and sustainable cities? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685668.0266707 Urban planning involves designing and organizing urban spaces to improve the quality of life for residents. What is the role of urban planning in creating more livable and sustainable cities? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:27:48,034 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:48,037 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:48.037449, (GMT): 2024-06-29 18:27:48.037449+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:48.037449, (GMT): 2024-06-29 18:27:48.037449+00:00\n",
      "2024-06-29 13:27:52,571 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:52,577 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.02 seconds or 7020.16 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.02 seconds or 7020.16 milliseconds.\n",
      "2024-06-29 13:27:52,647 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:52,900 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:52,931 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.99 seconds or 6990.15 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 6.99 seconds or 6990.15 milliseconds.\n",
      "2024-06-29 13:27:53,102 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:53,646 - micro - MainProcess - INFO     CPU usage: 12.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.7%\n",
      "2024-06-29 13:27:53,657 - micro - MainProcess - INFO     RAM usage: 85.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.5%\n",
      "2024-06-29 13:27:53,673 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:53,681 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:53,684 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685673.6849089 Deep learning, a subset of machine learning, uses neural networks with many layers to analyze complex data. Can you describe the process of deep learning and how it differs from traditional machine learning? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 76 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685673.6849089 Deep learning, a subset of machine learning, uses neural networks with many layers to analyze complex data. Can you describe the process of deep learning and how it differs from traditional machine learning? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 76\n",
      "2024-06-29 13:27:53,692 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:53,695 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:53.695120, (GMT): 2024-06-29 18:27:53.695120+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:53.695120, (GMT): 2024-06-29 18:27:53.695120+00:00\n",
      "2024-06-29 13:27:54,095 - micro - MainProcess - INFO     CPU usage: 35.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 35.4%\n",
      "2024-06-29 13:27:54,105 - micro - MainProcess - INFO     RAM usage: 85.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.5%\n",
      "2024-06-29 13:27:54,122 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:54,127 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:54,132 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685674.1314588 Smart grids use digital technology to manage the production and distribution of electricity more efficiently. How is the development of smart grids contributing to more efficient energy distribution? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 69 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685674.1314588 Smart grids use digital technology to manage the production and distribution of electricity more efficiently. How is the development of smart grids contributing to more efficient energy distribution? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 69\n",
      "2024-06-29 13:27:54,139 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:54,143 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:54.141979, (GMT): 2024-06-29 18:27:54.141979+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:54.141979, (GMT): 2024-06-29 18:27:54.141979+00:00\n",
      "2024-06-29 13:27:55,336 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:55,368 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.4 seconds or 7403.07 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.4 seconds or 7403.07 milliseconds.\n",
      "2024-06-29 13:27:55,460 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:55,464 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:27:55,469 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.43 seconds or 7427.5 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.43 seconds or 7427.5 milliseconds.\n",
      "2024-06-29 13:27:55,562 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:27:56,469 - micro - MainProcess - INFO     CPU usage: 15.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.2%\n",
      "2024-06-29 13:27:56,480 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:56,495 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:56,505 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:56,510 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685676.5109882 Digital marketing is evolving with new trends and technologies that help businesses reach their audiences. What are some emerging trends in digital marketing that businesses should be aware of? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685676.5109882 Digital marketing is evolving with new trends and technologies that help businesses reach their audiences. What are some emerging trends in digital marketing that businesses should be aware of? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 70\n",
      "2024-06-29 13:27:56,515 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:56,519 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:56.519517, (GMT): 2024-06-29 18:27:56.519517+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:56.519517, (GMT): 2024-06-29 18:27:56.519517+00:00\n",
      "2024-06-29 13:27:56,570 - micro - MainProcess - INFO     CPU usage: 28.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 28.7%\n",
      "2024-06-29 13:27:56,582 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:27:56,632 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:27:56,644 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:56,658 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685676.658579 Autonomous drones can operate without human intervention, offering various applications in different industries. How do autonomous drones operate, and what are their potential applications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685676.658579 Autonomous drones can operate without human intervention, offering various applications in different industries. How do autonomous drones operate, and what are their potential applications? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 66\n",
      "2024-06-29 13:27:56,670 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:27:56,673 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:56.673110, (GMT): 2024-06-29 18:27:56.673110+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:27:56.673110, (GMT): 2024-06-29 18:27:56.673110+00:00\n",
      "2024-06-29 13:28:01,115 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:28:01,150 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.45 seconds or 7451.31 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.45 seconds or 7451.31 milliseconds.\n",
      "2024-06-29 13:28:01,219 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:28:01,521 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:28:01,548 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.4 seconds or 7402.42 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.4 seconds or 7402.42 milliseconds.\n",
      "2024-06-29 13:28:01,609 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:28:02,241 - micro - MainProcess - INFO     CPU usage: 15.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.1%\n",
      "2024-06-29 13:28:02,251 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:28:02,275 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:02,282 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:02,287 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685682.286015 The rollout of 5G technology promises faster internet speeds and more reliable connections. What are some potential applications of 5G technology in everyday life? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685682.286015 The rollout of 5G technology promises faster internet speeds and more reliable connections. What are some potential applications of 5G technology in everyday life? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 68\n",
      "2024-06-29 13:28:02,294 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:02,298 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:28:02.297279, (GMT): 2024-06-29 18:28:02.297279+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:28:02.297279, (GMT): 2024-06-29 18:28:02.297279+00:00\n",
      "2024-06-29 13:28:03,831 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:28:03,837 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.31 seconds or 7313.16 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.31 seconds or 7313.16 milliseconds.\n",
      "2024-06-29 13:28:03,896 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:28:03,901 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:28:03,905 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.23 seconds or 7227.89 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.23 seconds or 7227.89 milliseconds.\n",
      "2024-06-29 13:28:03,953 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:28:04,915 - micro - MainProcess - INFO     CPU usage: 12.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.5%\n",
      "2024-06-29 13:28:04,926 - micro - MainProcess - INFO     RAM usage: 85.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.5%\n",
      "2024-06-29 13:28:04,975 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:809)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:04,984 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:05,024 - micro - MainProcess - INFO     Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685685.0233626 Genetic modification in crops aims to improve yield, resistance to pests, and nutritional value. Can you explain the principles of genetic modification in crops and its impact on agriculture? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72 (latencytest.py:make_call:818)\n",
      "INFO:micro:Messages: [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '1719685685.0233626 Genetic modification in crops aims to improve yield, resistance to pests, and nutritional value. Can you explain the principles of genetic modification in crops and its impact on agriculture? Please write a response that should be at least 500 tokens long.'}] and Context Tokens: 72\n",
      "2024-06-29 13:28:05,032 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:05,044 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:28:05.044345, (GMT): 2024-06-29 18:28:05.044345+00:00 (latencytest.py:make_call:840)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-29 13:28:05.044345, (GMT): 2024-06-29 18:28:05.044345+00:00\n",
      "2024-06-29 13:28:09,306 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:28:09,333 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.03 seconds or 7032.52 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 7.03 seconds or 7032.52 milliseconds.\n",
      "2024-06-29 13:28:09,383 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:28:10,861 - micro - MainProcess - ERROR    Error decoding JSON: Expecting value: line 1 column 2 (char 1) (latencytest.py:make_call:868)\n",
      "ERROR:micro:Error decoding JSON: Expecting value: line 1 column 2 (char 1)\n",
      "2024-06-29 13:28:10,893 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.83 seconds or 5826.6 milliseconds. (latencytest.py:make_call:879)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.83 seconds or 5826.6 milliseconds.\n",
      "2024-06-29 13:28:10,940 - micro - MainProcess - INFO     Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region N/A not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-29 13:28:16,965 - micro - MainProcess - INFO     Split list into 4 parts. (utils.py:split_list_into_variable_parts:174)\n",
      "INFO:micro:Split list into 4 parts.\n",
      "2024-06-29 13:28:16,968 - micro - MainProcess - INFO     CPU usage: 13.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.5%\n",
      "2024-06-29 13:28:16,979 - micro - MainProcess - INFO     RAM usage: 85.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.2%\n",
      "2024-06-29 13:28:16,999 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,004 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,009 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:17,014 - micro - MainProcess - INFO     CPU usage: 16.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.7%\n",
      "2024-06-29 13:28:17,029 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:28:17,083 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,086 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,090 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:17,093 - micro - MainProcess - INFO     CPU usage: 45.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 45.2%\n",
      "2024-06-29 13:28:17,103 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:28:17,128 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,132 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,137 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:17,141 - micro - MainProcess - INFO     CPU usage: 8.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 8.1%\n",
      "2024-06-29 13:28:17,156 - micro - MainProcess - INFO     RAM usage: 85.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.3%\n",
      "2024-06-29 13:28:17,182 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,186 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,190 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:17,197 - micro - MainProcess - INFO     CPU usage: 19.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.6%\n",
      "2024-06-29 13:28:17,211 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:28:17,235 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,241 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,242 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:17,249 - micro - MainProcess - INFO     CPU usage: 20.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.0%\n",
      "2024-06-29 13:28:17,261 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:28:17,287 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,290 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,294 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:17,297 - micro - MainProcess - INFO     CPU usage: 12.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.5%\n",
      "2024-06-29 13:28:17,309 - micro - MainProcess - INFO     RAM usage: 85.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.5%\n",
      "2024-06-29 13:28:17,333 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,336 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,342 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:17,346 - micro - MainProcess - INFO     CPU usage: 11.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 11.4%\n",
      "2024-06-29 13:28:17,358 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:17,382 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:17,384 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:17,389 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:26,876 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.16 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.16 seconds.\n",
      "2024-06-29 13:28:26,882 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.54 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.54 seconds.\n",
      "2024-06-29 13:28:26,886 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.49 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.49 seconds.\n",
      "2024-06-29 13:28:26,891 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.75 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.75 seconds.\n",
      "2024-06-29 13:28:26,895 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.65 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.65 seconds.\n",
      "2024-06-29 13:28:26,899 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.70 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.70 seconds.\n",
      "2024-06-29 13:28:26,911 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.90 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.90 seconds.\n",
      "2024-06-29 13:28:26,917 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.82 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.82 seconds.\n",
      "2024-06-29 13:28:27,892 - micro - MainProcess - INFO     CPU usage: 14.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.4%\n",
      "2024-06-29 13:28:27,904 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:28:27,927 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:27,930 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:27,933 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:27,943 - micro - MainProcess - INFO     CPU usage: 18.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 18.2%\n",
      "2024-06-29 13:28:27,956 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:28:27,981 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:27,984 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:27,987 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:27,991 - micro - MainProcess - INFO     CPU usage: 42.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 42.5%\n",
      "2024-06-29 13:28:28,004 - micro - MainProcess - INFO     RAM usage: 85.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.5%\n",
      "2024-06-29 13:28:28,031 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:28,033 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:28,054 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:28,058 - micro - MainProcess - INFO     CPU usage: 25.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.5%\n",
      "2024-06-29 13:28:28,071 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:28,169 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:28,175 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:28,179 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:28,183 - micro - MainProcess - INFO     CPU usage: 75.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 75.8%\n",
      "2024-06-29 13:28:28,210 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:28,237 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:28,242 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:28,249 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:28,254 - micro - MainProcess - INFO     CPU usage: 76.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 76.8%\n",
      "2024-06-29 13:28:28,267 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:28:28,304 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:28,307 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:28,313 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:28,317 - micro - MainProcess - INFO     CPU usage: 49.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 49.0%\n",
      "2024-06-29 13:28:28,334 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:28:28,359 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:28,362 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:28,368 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:28,373 - micro - MainProcess - INFO     CPU usage: 24.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 24.4%\n",
      "2024-06-29 13:28:28,389 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:28:28,415 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:28,419 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:28,426 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:33,197 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.21 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.21 seconds.\n",
      "2024-06-29 13:28:33,287 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.92 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.92 seconds.\n",
      "2024-06-29 13:28:34,206 - micro - MainProcess - INFO     CPU usage: 13.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.8%\n",
      "2024-06-29 13:28:34,216 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:34,306 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:34,310 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:34,313 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:34,320 - micro - MainProcess - INFO     CPU usage: 29.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 29.7%\n",
      "2024-06-29 13:28:34,330 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:34,371 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:34,374 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:34,378 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:34,412 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.47 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.47 seconds.\n",
      "2024-06-29 13:28:34,441 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.38 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.38 seconds.\n",
      "2024-06-29 13:28:34,693 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.51 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.51 seconds.\n",
      "2024-06-29 13:28:35,421 - micro - MainProcess - INFO     CPU usage: 19.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.7%\n",
      "2024-06-29 13:28:35,431 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:35,455 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:35,460 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:35,463 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:35,473 - micro - MainProcess - INFO     CPU usage: 25.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.0%\n",
      "2024-06-29 13:28:35,487 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:35,512 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:35,514 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:35,520 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:35,690 - micro - MainProcess - INFO     CPU usage: 32.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 32.8%\n",
      "2024-06-29 13:28:35,702 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:35,730 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:35,735 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:35,739 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:36,745 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.49 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.49 seconds.\n",
      "2024-06-29 13:28:36,808 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.49 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.49 seconds.\n",
      "2024-06-29 13:28:36,827 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.40 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.40 seconds.\n",
      "2024-06-29 13:28:37,740 - micro - MainProcess - INFO     CPU usage: 16.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.3%\n",
      "2024-06-29 13:28:37,750 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:37,784 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:37,786 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:37,790 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:37,795 - micro - MainProcess - INFO     CPU usage: 9.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 9.3%\n",
      "2024-06-29 13:28:37,806 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:37,943 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:37,955 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:37,967 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:37,975 - micro - MainProcess - INFO     CPU usage: 62.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 62.1%\n",
      "2024-06-29 13:28:37,989 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:38,030 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:38,033 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:38,037 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:38,345 - micro - MainProcess - INFO     Succesful Run - Time taken: 2.88 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 2.88 seconds.\n",
      "2024-06-29 13:28:38,569 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.25 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.25 seconds.\n",
      "2024-06-29 13:28:38,585 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.20 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.20 seconds.\n",
      "2024-06-29 13:28:39,371 - micro - MainProcess - INFO     CPU usage: 15.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.7%\n",
      "2024-06-29 13:28:39,381 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:39,407 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:39,452 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:39,456 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:39,503 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.98 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.98 seconds.\n",
      "2024-06-29 13:28:39,572 - micro - MainProcess - INFO     CPU usage: 56.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 56.6%\n",
      "2024-06-29 13:28:39,630 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:39,656 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:39,662 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:39,667 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:39,681 - micro - MainProcess - INFO     CPU usage: 65.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 65.5%\n",
      "2024-06-29 13:28:39,697 - micro - MainProcess - INFO     RAM usage: 86.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.0%\n",
      "2024-06-29 13:28:39,733 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:39,738 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:39,746 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:40,514 - micro - MainProcess - INFO     CPU usage: 28.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 28.2%\n",
      "2024-06-29 13:28:40,525 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:40,552 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:40,555 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:40,559 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:42,761 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.02 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.02 seconds.\n",
      "2024-06-29 13:28:43,776 - micro - MainProcess - INFO     CPU usage: 8.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 8.0%\n",
      "2024-06-29 13:28:43,786 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:43,815 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:43,818 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:43,822 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:43,849 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.29 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.29 seconds.\n",
      "2024-06-29 13:28:44,121 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.66 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.66 seconds.\n",
      "2024-06-29 13:28:44,182 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.43 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.43 seconds.\n",
      "2024-06-29 13:28:44,189 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.52 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.52 seconds.\n",
      "2024-06-29 13:28:44,868 - micro - MainProcess - INFO     CPU usage: 27.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 27.7%\n",
      "2024-06-29 13:28:44,880 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:44,905 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:44,908 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:44,912 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:44,917 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.12 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.12 seconds.\n",
      "2024-06-29 13:28:45,129 - micro - MainProcess - INFO     CPU usage: 12.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.7%\n",
      "2024-06-29 13:28:45,139 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:45,165 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:45,169 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:45,172 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:45,176 - micro - MainProcess - INFO     CPU usage: 19.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.4%\n",
      "2024-06-29 13:28:45,186 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:45,215 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:45,219 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:45,222 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:45,227 - micro - MainProcess - INFO     CPU usage: 22.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 22.9%\n",
      "2024-06-29 13:28:45,241 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:45,384 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:45,387 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:45,392 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:45,565 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.59 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.59 seconds.\n",
      "2024-06-29 13:28:45,966 - micro - MainProcess - INFO     CPU usage: 23.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.8%\n",
      "2024-06-29 13:28:45,976 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:28:46,148 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:46,154 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:46,158 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:46,664 - micro - MainProcess - INFO     CPU usage: 23.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.1%\n",
      "2024-06-29 13:28:46,675 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:46,709 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:46,714 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:46,717 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:47,102 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.06 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.06 seconds.\n",
      "2024-06-29 13:28:48,108 - micro - MainProcess - INFO     CPU usage: 18.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 18.6%\n",
      "2024-06-29 13:28:48,117 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:48,146 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:48,149 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:48,156 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:48,787 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.56 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.56 seconds.\n",
      "2024-06-29 13:28:49,801 - micro - MainProcess - INFO     CPU usage: 13.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.1%\n",
      "2024-06-29 13:28:49,810 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:49,830 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:49,833 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:49,836 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:49,977 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.06 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.06 seconds.\n",
      "2024-06-29 13:28:50,282 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.89 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.89 seconds.\n",
      "2024-06-29 13:28:50,297 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.12 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.12 seconds.\n",
      "2024-06-29 13:28:50,978 - micro - MainProcess - INFO     CPU usage: 19.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.1%\n",
      "2024-06-29 13:28:50,987 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:51,002 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:51,005 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:51,009 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:51,287 - micro - MainProcess - INFO     CPU usage: 37.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 37.4%\n",
      "2024-06-29 13:28:51,297 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:51,321 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:51,323 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:51,327 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:51,330 - micro - MainProcess - INFO     CPU usage: 14.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.8%\n",
      "2024-06-29 13:28:51,343 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:28:51,360 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:51,363 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:51,367 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:52,462 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.30 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.30 seconds.\n",
      "2024-06-29 13:28:52,729 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.91 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.91 seconds.\n",
      "2024-06-29 13:28:53,483 - micro - MainProcess - INFO     CPU usage: 12.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.3%\n",
      "2024-06-29 13:28:53,493 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:53,508 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:53,511 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:53,514 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:53,546 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.71 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.71 seconds.\n",
      "2024-06-29 13:28:53,722 - micro - MainProcess - INFO     CPU usage: 41.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 41.9%\n",
      "2024-06-29 13:28:53,732 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:53,758 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:53,761 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:53,764 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:54,334 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.01 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.01 seconds.\n",
      "2024-06-29 13:28:54,566 - micro - MainProcess - INFO     CPU usage: 11.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 11.0%\n",
      "2024-06-29 13:28:54,577 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:54,601 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:54,634 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:54,639 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:54,642 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.92 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.92 seconds.\n",
      "2024-06-29 13:28:55,090 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.08 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.08 seconds.\n",
      "2024-06-29 13:28:55,352 - micro - MainProcess - INFO     CPU usage: 22.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 22.6%\n",
      "2024-06-29 13:28:55,362 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:28:55,495 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:55,507 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:55,511 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:55,519 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.15 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.15 seconds.\n",
      "2024-06-29 13:28:55,632 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.47 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.47 seconds.\n",
      "2024-06-29 13:28:55,666 - micro - MainProcess - INFO     CPU usage: 40.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 40.5%\n",
      "2024-06-29 13:28:55,677 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:28:55,699 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:55,702 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:55,706 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:56,086 - micro - MainProcess - INFO     CPU usage: 26.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 26.7%\n",
      "2024-06-29 13:28:56,096 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:28:56,122 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:56,124 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:56,128 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:56,521 - micro - MainProcess - INFO     CPU usage: 8.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 8.9%\n",
      "2024-06-29 13:28:56,531 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:28:56,553 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:56,560 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:56,563 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:28:56,629 - micro - MainProcess - INFO     CPU usage: 32.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 32.5%\n",
      "2024-06-29 13:28:56,640 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:28:56,665 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:28:56,669 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:28:56,673 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:28:58,978 - micro - MainProcess - INFO     Succesful Run - Time taken: 2.85 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 2.85 seconds.\n",
      "2024-06-29 13:28:59,075 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.55 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.55 seconds.\n",
      "2024-06-29 13:28:59,998 - micro - MainProcess - INFO     CPU usage: 13.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.9%\n",
      "2024-06-29 13:29:00,006 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:29:00,098 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:00,100 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:00,103 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:00,109 - micro - MainProcess - INFO     CPU usage: 32.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 32.6%\n",
      "2024-06-29 13:29:00,118 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:29:00,134 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:00,137 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:00,142 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:02,080 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.44 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.44 seconds.\n",
      "2024-06-29 13:29:02,464 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.76 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.76 seconds.\n",
      "2024-06-29 13:29:03,093 - micro - MainProcess - INFO     CPU usage: 17.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 17.2%\n",
      "2024-06-29 13:29:03,104 - micro - MainProcess - INFO     RAM usage: 85.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.6%\n",
      "2024-06-29 13:29:03,123 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:03,129 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:03,133 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:03,256 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.74 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.74 seconds.\n",
      "2024-06-29 13:29:03,464 - micro - MainProcess - INFO     CPU usage: 37.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 37.5%\n",
      "2024-06-29 13:29:03,475 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:29:03,507 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:03,510 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:03,517 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:03,772 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.20 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.20 seconds.\n",
      "2024-06-29 13:29:04,270 - micro - MainProcess - INFO     CPU usage: 25.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.8%\n",
      "2024-06-29 13:29:04,279 - micro - MainProcess - INFO     RAM usage: 85.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.8%\n",
      "2024-06-29 13:29:04,389 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:04,403 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:04,404 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:04,781 - micro - MainProcess - INFO     CPU usage: 23.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.7%\n",
      "2024-06-29 13:29:04,791 - micro - MainProcess - INFO     RAM usage: 85.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.7%\n",
      "2024-06-29 13:29:04,818 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:04,822 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:04,825 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:05,083 - micro - MainProcess - INFO     Succesful Run - Time taken: 11.31 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 11.31 seconds.\n",
      "2024-06-29 13:29:05,152 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.05 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.05 seconds.\n",
      "2024-06-29 13:29:06,100 - micro - MainProcess - INFO     CPU usage: 20.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.0%\n",
      "2024-06-29 13:29:06,110 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:29:06,131 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:06,135 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:06,141 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:06,146 - micro - MainProcess - INFO     CPU usage: 16.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.1%\n",
      "2024-06-29 13:29:06,164 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:29:06,219 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:06,228 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:06,233 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:06,996 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.86 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.86 seconds.\n",
      "2024-06-29 13:29:07,261 - micro - MainProcess - INFO     Succesful Run - Time taken: 10.58 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 10.58 seconds.\n",
      "2024-06-29 13:29:08,008 - micro - MainProcess - INFO     CPU usage: 20.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.3%\n",
      "2024-06-29 13:29:08,025 - micro - MainProcess - INFO     RAM usage: 85.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.9%\n",
      "2024-06-29 13:29:08,056 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:08,060 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:08,066 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:08,253 - micro - MainProcess - INFO     CPU usage: 84.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 84.8%\n",
      "2024-06-29 13:29:08,266 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:29:08,291 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:08,295 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:08,303 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:08,558 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.41 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.41 seconds.\n",
      "2024-06-29 13:29:08,669 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.84 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.84 seconds.\n",
      "2024-06-29 13:29:09,557 - micro - MainProcess - INFO     CPU usage: 31.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 31.0%\n",
      "2024-06-29 13:29:09,567 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:29:09,595 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:09,599 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:09,604 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:09,680 - micro - MainProcess - INFO     CPU usage: 27.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 27.8%\n",
      "2024-06-29 13:29:09,691 - micro - MainProcess - INFO     RAM usage: 86.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.3%\n",
      "2024-06-29 13:29:09,713 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:09,717 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:09,722 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:10,147 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.91 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.91 seconds.\n",
      "2024-06-29 13:29:10,907 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.39 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.39 seconds.\n",
      "2024-06-29 13:29:11,149 - micro - MainProcess - INFO     CPU usage: 25.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.7%\n",
      "2024-06-29 13:29:11,159 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:29:11,186 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:11,188 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:11,193 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:11,924 - micro - MainProcess - INFO     CPU usage: 22.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 22.4%\n",
      "2024-06-29 13:29:11,934 - micro - MainProcess - INFO     RAM usage: 86.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.2%\n",
      "2024-06-29 13:29:11,970 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:11,978 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:11,981 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:12,172 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.10 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.10 seconds.\n",
      "2024-06-29 13:29:13,178 - micro - MainProcess - INFO     CPU usage: 40.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 40.6%\n",
      "2024-06-29 13:29:13,189 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:13,216 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:13,220 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:13,222 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:13,820 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.09 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.09 seconds.\n",
      "2024-06-29 13:29:13,911 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.77 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.77 seconds.\n",
      "2024-06-29 13:29:14,337 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.03 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.03 seconds.\n",
      "2024-06-29 13:29:14,834 - micro - MainProcess - INFO     CPU usage: 14.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.5%\n",
      "2024-06-29 13:29:14,845 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:14,859 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:14,861 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:14,866 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:14,916 - micro - MainProcess - INFO     CPU usage: 8.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 8.1%\n",
      "2024-06-29 13:29:14,929 - micro - MainProcess - INFO     RAM usage: 86.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.6%\n",
      "2024-06-29 13:29:15,024 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:15,027 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:15,032 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:15,039 - micro - MainProcess - INFO     Succesful Run - Time taken: 10.63 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 10.63 seconds.\n",
      "2024-06-29 13:29:15,345 - micro - MainProcess - INFO     CPU usage: 28.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 28.1%\n",
      "2024-06-29 13:29:15,357 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:15,380 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:15,383 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:15,387 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:15,394 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.17 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.17 seconds.\n",
      "2024-06-29 13:29:16,058 - micro - MainProcess - INFO     CPU usage: 28.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 28.8%\n",
      "2024-06-29 13:29:16,070 - micro - MainProcess - INFO     RAM usage: 86.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.7%\n",
      "2024-06-29 13:29:16,089 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:16,101 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:16,106 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:16,414 - micro - MainProcess - INFO     CPU usage: 21.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 21.6%\n",
      "2024-06-29 13:29:16,425 - micro - MainProcess - INFO     RAM usage: 86.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.4%\n",
      "2024-06-29 13:29:16,447 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:16,449 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:16,454 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:17,436 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.21 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.21 seconds.\n",
      "2024-06-29 13:29:17,570 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.96 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.96 seconds.\n",
      "2024-06-29 13:29:18,453 - micro - MainProcess - INFO     CPU usage: 14.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.8%\n",
      "2024-06-29 13:29:18,463 - micro - MainProcess - INFO     RAM usage: 86.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.4%\n",
      "2024-06-29 13:29:18,499 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:18,518 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:18,520 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:18,591 - micro - MainProcess - INFO     CPU usage: 24.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 24.3%\n",
      "2024-06-29 13:29:18,601 - micro - MainProcess - INFO     RAM usage: 86.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.6%\n",
      "2024-06-29 13:29:18,631 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:18,639 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:18,643 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:18,650 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.78 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.78 seconds.\n",
      "2024-06-29 13:29:19,656 - micro - MainProcess - INFO     CPU usage: 20.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.8%\n",
      "2024-06-29 13:29:19,667 - micro - MainProcess - INFO     RAM usage: 86.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.6%\n",
      "2024-06-29 13:29:19,703 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:19,707 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:19,711 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:19,744 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.63 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.63 seconds.\n",
      "2024-06-29 13:29:20,143 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.16 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.16 seconds.\n",
      "2024-06-29 13:29:20,744 - micro - MainProcess - INFO     CPU usage: 15.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.6%\n",
      "2024-06-29 13:29:20,754 - micro - MainProcess - INFO     RAM usage: 86.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.4%\n",
      "2024-06-29 13:29:20,795 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:20,799 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:20,802 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:20,806 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.35 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.35 seconds.\n",
      "2024-06-29 13:29:21,165 - micro - MainProcess - INFO     CPU usage: 32.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 32.1%\n",
      "2024-06-29 13:29:21,176 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:21,208 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:21,213 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:21,218 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:21,825 - micro - MainProcess - INFO     CPU usage: 27.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 27.1%\n",
      "2024-06-29 13:29:21,837 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:21,861 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:21,864 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:21,870 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:22,839 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.32 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.32 seconds.\n",
      "2024-06-29 13:29:23,153 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.12 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.12 seconds.\n",
      "2024-06-29 13:29:23,549 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.16 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.16 seconds.\n",
      "2024-06-29 13:29:23,758 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.04 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.04 seconds.\n",
      "2024-06-29 13:29:23,850 - micro - MainProcess - INFO     CPU usage: 20.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.0%\n",
      "2024-06-29 13:29:23,862 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:23,927 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:23,934 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:23,937 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:24,153 - micro - MainProcess - INFO     CPU usage: 47.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 47.2%\n",
      "2024-06-29 13:29:24,165 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:24,191 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:24,194 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:24,199 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:24,561 - micro - MainProcess - INFO     CPU usage: 30.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 30.2%\n",
      "2024-06-29 13:29:24,572 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:24,599 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:24,601 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:24,607 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:24,777 - micro - MainProcess - INFO     CPU usage: 23.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.7%\n",
      "2024-06-29 13:29:24,789 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:24,812 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:24,815 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:24,819 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:26,007 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.13 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.13 seconds.\n",
      "2024-06-29 13:29:26,547 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.90 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.90 seconds.\n",
      "2024-06-29 13:29:27,016 - micro - MainProcess - INFO     CPU usage: 16.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.8%\n",
      "2024-06-29 13:29:27,027 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:27,055 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:27,058 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:27,061 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:27,213 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.41 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.41 seconds.\n",
      "2024-06-29 13:29:27,559 - micro - MainProcess - INFO     CPU usage: 26.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 26.0%\n",
      "2024-06-29 13:29:27,569 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:27,599 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:27,601 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:27,604 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:27,764 - micro - MainProcess - INFO     Succesful Run - Time taken: 3.82 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 3.82 seconds.\n",
      "2024-06-29 13:29:28,226 - micro - MainProcess - INFO     CPU usage: 25.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.1%\n",
      "2024-06-29 13:29:28,243 - micro - MainProcess - INFO     RAM usage: 86.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.6%\n",
      "2024-06-29 13:29:28,274 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:28,277 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:28,279 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:29,033 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.81 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.81 seconds.\n",
      "2024-06-29 13:29:29,036 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.21 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.21 seconds.\n",
      "2024-06-29 13:29:30,047 - micro - MainProcess - INFO     CPU usage: 16.0% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.0%\n",
      "2024-06-29 13:29:30,064 - micro - MainProcess - INFO     RAM usage: 86.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.7%\n",
      "2024-06-29 13:29:30,095 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:30,105 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:30,110 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:30,114 - micro - MainProcess - INFO     CPU usage: 33.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 33.3%\n",
      "2024-06-29 13:29:30,130 - micro - MainProcess - INFO     RAM usage: 86.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.7%\n",
      "2024-06-29 13:29:30,150 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:30,153 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:30,159 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:30,561 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.95 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.95 seconds.\n",
      "2024-06-29 13:29:31,573 - micro - MainProcess - INFO     CPU usage: 14.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.5%\n",
      "2024-06-29 13:29:31,582 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:31,613 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:31,616 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:31,617 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:31,966 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.90 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.90 seconds.\n",
      "2024-06-29 13:29:32,186 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.98 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.98 seconds.\n",
      "2024-06-29 13:29:32,719 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.44 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.44 seconds.\n",
      "2024-06-29 13:29:33,198 - micro - MainProcess - INFO     CPU usage: 19.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.1%\n",
      "2024-06-29 13:29:33,207 - micro - MainProcess - INFO     RAM usage: 86.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.6%\n",
      "2024-06-29 13:29:33,241 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:33,246 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:33,249 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:33,739 - micro - MainProcess - INFO     CPU usage: 23.4% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 23.4%\n",
      "2024-06-29 13:29:33,749 - micro - MainProcess - INFO     RAM usage: 86.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.4%\n",
      "2024-06-29 13:29:33,781 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:33,784 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:33,787 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 250\n",
      "2024-06-29 13:29:34,423 - micro - MainProcess - INFO     Succesful Run - Time taken: 4.25 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 4.25 seconds.\n",
      "2024-06-29 13:29:35,288 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.68 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.68 seconds.\n",
      "2024-06-29 13:29:36,295 - micro - MainProcess - INFO     CPU usage: 12.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.5%\n",
      "2024-06-29 13:29:36,306 - micro - MainProcess - INFO     RAM usage: 86.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.4%\n",
      "2024-06-29 13:29:36,324 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:36,328 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:36,332 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:37,731 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.62 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.62 seconds.\n",
      "2024-06-29 13:29:38,744 - micro - MainProcess - INFO     CPU usage: 13.3% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.3%\n",
      "2024-06-29 13:29:38,754 - micro - MainProcess - INFO     RAM usage: 86.6% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.6%\n",
      "2024-06-29 13:29:38,796 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:38,799 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:38,805 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:39,151 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.53 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.53 seconds.\n",
      "2024-06-29 13:29:39,527 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.74 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.74 seconds.\n",
      "2024-06-29 13:29:40,168 - micro - MainProcess - INFO     CPU usage: 14.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.1%\n",
      "2024-06-29 13:29:40,178 - micro - MainProcess - INFO     RAM usage: 86.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.7%\n",
      "2024-06-29 13:29:40,330 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:40,333 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:40,334 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:41,039 - micro - MainProcess - INFO     Succesful Run - Time taken: 7.79 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 7.79 seconds.\n",
      "2024-06-29 13:29:42,054 - micro - MainProcess - INFO     CPU usage: 16.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.1%\n",
      "2024-06-29 13:29:42,064 - micro - MainProcess - INFO     RAM usage: 86.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.7%\n",
      "2024-06-29 13:29:42,092 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:42,098 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:42,104 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:43,001 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.67 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.67 seconds.\n",
      "2024-06-29 13:29:44,023 - micro - MainProcess - INFO     CPU usage: 22.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 22.9%\n",
      "2024-06-29 13:29:44,031 - micro - MainProcess - INFO     RAM usage: 86.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.3%\n",
      "2024-06-29 13:29:44,051 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:44,059 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:44,063 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:45,139 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.33 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.33 seconds.\n",
      "2024-06-29 13:29:46,156 - micro - MainProcess - INFO     CPU usage: 9.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 9.9%\n",
      "2024-06-29 13:29:46,166 - micro - MainProcess - INFO     RAM usage: 86.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.3%\n",
      "2024-06-29 13:29:46,183 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:46,186 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:46,188 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:47,180 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.84 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.84 seconds.\n",
      "2024-06-29 13:29:48,190 - micro - MainProcess - INFO     CPU usage: 16.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 16.6%\n",
      "2024-06-29 13:29:48,200 - micro - MainProcess - INFO     RAM usage: 86.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.2%\n",
      "2024-06-29 13:29:48,237 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:48,243 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:48,245 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:50,742 - micro - MainProcess - INFO     Succesful Run - Time taken: 8.54 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 8.54 seconds.\n",
      "2024-06-29 13:29:51,759 - micro - MainProcess - INFO     CPU usage: 13.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.6%\n",
      "2024-06-29 13:29:51,779 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:29:51,813 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:51,817 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:51,820 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:53,807 - micro - MainProcess - INFO     Succesful Run - Time taken: 9.74 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 9.74 seconds.\n",
      "2024-06-29 13:29:54,169 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.91 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.91 seconds.\n",
      "2024-06-29 13:29:54,814 - micro - MainProcess - INFO     CPU usage: 19.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.1%\n",
      "2024-06-29 13:29:54,825 - micro - MainProcess - INFO     RAM usage: 86.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.3%\n",
      "2024-06-29 13:29:54,870 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:54,873 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:54,877 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:55,188 - micro - MainProcess - INFO     CPU usage: 41.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 41.5%\n",
      "2024-06-29 13:29:55,199 - micro - MainProcess - INFO     RAM usage: 86.1% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.1%\n",
      "2024-06-29 13:29:55,218 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:55,221 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:55,227 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:56,507 - micro - MainProcess - INFO     Succesful Run - Time taken: 10.32 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 10.32 seconds.\n",
      "2024-06-29 13:29:57,514 - micro - MainProcess - INFO     CPU usage: 15.2% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.2%\n",
      "2024-06-29 13:29:57,534 - micro - MainProcess - INFO     RAM usage: 86.2% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.2%\n",
      "2024-06-29 13:29:57,563 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:57,567 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:57,572 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:29:58,374 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.55 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.55 seconds.\n",
      "2024-06-29 13:29:59,387 - micro - MainProcess - INFO     CPU usage: 11.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 11.5%\n",
      "2024-06-29 13:29:59,398 - micro - MainProcess - INFO     RAM usage: 86.5% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.5%\n",
      "2024-06-29 13:29:59,578 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:29:59,582 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:29:59,585 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:30:05,599 - micro - MainProcess - INFO     Succesful Run - Time taken: 10.72 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 10.72 seconds.\n",
      "2024-06-29 13:30:05,897 - micro - MainProcess - INFO     Succesful Run - Time taken: 10.67 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 10.67 seconds.\n",
      "2024-06-29 13:30:06,620 - micro - MainProcess - INFO     CPU usage: 13.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 13.1%\n",
      "2024-06-29 13:30:06,630 - micro - MainProcess - INFO     RAM usage: 85.4% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 85.4%\n",
      "2024-06-29 13:30:06,648 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:30:06,652 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:30:06,655 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:30:08,171 - micro - MainProcess - INFO     Succesful Run - Time taken: 10.60 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 10.60 seconds.\n",
      "2024-06-29 13:30:09,973 - micro - MainProcess - INFO     Succesful Run - Time taken: 10.39 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 10.39 seconds.\n",
      "2024-06-29 13:30:10,985 - micro - MainProcess - INFO     CPU usage: 14.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 14.8%\n",
      "2024-06-29 13:30:10,995 - micro - MainProcess - INFO     RAM usage: 84.7% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 84.7%\n",
      "2024-06-29 13:30:11,018 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:82)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-29 13:30:11,045 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:generate_test_messages:651)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-29 13:30:11,049 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500 (latencytest.py:make_call:742)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 500\n",
      "2024-06-29 13:30:13,145 - micro - MainProcess - INFO     Succesful Run - Time taken: 6.49 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 6.49 seconds.\n",
      "2024-06-29 13:30:16,773 - micro - MainProcess - INFO     Succesful Run - Time taken: 5.72 seconds. (latencytest.py:make_call:771)\n",
      "INFO:micro:Succesful Run - Time taken: 5.72 seconds.\n"
     ]
    }
   ],
   "source": [
    "await benchmark_streaming.run_latency_benchmark_bulk(deployment_names=[DEPLOYMENT_ID], max_tokens_list=[500,250], byop=prompts)\n",
    "await benchmark_non_streaming.run_latency_benchmark_bulk(deployment_names=[DEPLOYMENT_ID], max_tokens_list=[500,250], byop=prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 13:30:33,062 - micro - MainProcess - INFO     Calculating statistics for data: [3.2, 3.41, 4.1, 4.05, 4.03, 3.95, 3.79, 4.01, 2.88, 4.52, 3.14, 4.15, 4.27, 4.5, 3.56, 3.62, 3.41, 4.05, 3.52, 3.85, 3.87, 5.48, 3.8, 3.74, 3.81, 3.6, 3.43, 4.7, 4.24, 3.52, 3.24, 3.19, 3.82, 3.75, 3.81, 3.11, 3.55, 3.53, 3.7, 3.62, 3.97, 4.12, 3.59, 3.35, 3.76, 3.7, 3.45, 3.63, 3.71, 3.85, 3.61, 3.26, 3.15, 3.07, 4.42, 4.47, 3.7, 3.52, 3.57, 3.06] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [3.2, 3.41, 4.1, 4.05, 4.03, 3.95, 3.79, 4.01, 2.88, 4.52, 3.14, 4.15, 4.27, 4.5, 3.56, 3.62, 3.41, 4.05, 3.52, 3.85, 3.87, 5.48, 3.8, 3.74, 3.81, 3.6, 3.43, 4.7, 4.24, 3.52, 3.24, 3.19, 3.82, 3.75, 3.81, 3.11, 3.55, 3.53, 3.7, 3.62, 3.97, 4.12, 3.59, 3.35, 3.76, 3.7, 3.45, 3.63, 3.71, 3.85, 3.61, 3.26, 3.15, 3.07, 4.42, 4.47, 3.7, 3.52, 3.57, 3.06]\n",
      "2024-06-29 13:30:33,066 - micro - MainProcess - INFO     Data converted to numpy array: [3.2  3.41 4.1  4.05 4.03 3.95 3.79 4.01 2.88 4.52 3.14 4.15 4.27 4.5\n",
      " 3.56 3.62 3.41 4.05 3.52 3.85 3.87 5.48 3.8  3.74 3.81 3.6  3.43 4.7\n",
      " 4.24 3.52 3.24 3.19 3.82 3.75 3.81 3.11 3.55 3.53 3.7  3.62 3.97 4.12\n",
      " 3.59 3.35 3.76 3.7  3.45 3.63 3.71 3.85 3.61 3.26 3.15 3.07 4.42 4.47\n",
      " 3.7  3.52 3.57 3.06] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [3.2  3.41 4.1  4.05 4.03 3.95 3.79 4.01 2.88 4.52 3.14 4.15 4.27 4.5\n",
      " 3.56 3.62 3.41 4.05 3.52 3.85 3.87 5.48 3.8  3.74 3.81 3.6  3.43 4.7\n",
      " 4.24 3.52 3.24 3.19 3.82 3.75 3.81 3.11 3.55 3.53 3.7  3.62 3.97 4.12\n",
      " 3.59 3.35 3.76 3.7  3.45 3.63 3.71 3.85 3.61 3.26 3.15 3.07 4.42 4.47\n",
      " 3.7  3.52 3.57 3.06]\n",
      "2024-06-29 13:30:33,069 - micro - MainProcess - INFO     Calculated median: 3.7 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 3.7\n",
      "2024-06-29 13:30:33,073 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.48 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.48\n",
      "2024-06-29 13:30:33,077 - micro - MainProcess - INFO     Calculated 95th percentile: 4.5 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 4.5\n",
      "2024-06-29 13:30:33,080 - micro - MainProcess - INFO     Calculated 99th percentile: 5.02 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 5.02\n",
      "2024-06-29 13:30:33,084 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.12 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.12\n",
      "2024-06-29 13:30:33,086 - micro - MainProcess - INFO     Result: (3.7, 0.48, 4.5, 5.02, 0.12) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (3.7, 0.48, 4.5, 5.02, 0.12)\n",
      "2024-06-29 13:30:33,088 - micro - MainProcess - INFO     Calculating statistics for data: [255, 252, 248, 246, 256, 250, 253, 250, 250, 251, 250, 253, 254, 250, 248, 251, 250, 252, 246, 250, 247, 249, 248, 252, 247, 252, 250, 247, 250, 252, 250, 257, 252, 253, 251, 251, 248, 250, 259, 252, 249, 251, 248, 253, 253, 251, 253, 252, 252, 252, 254, 255, 247, 247, 254, 249, 252, 252, 247, 249] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [255, 252, 248, 246, 256, 250, 253, 250, 250, 251, 250, 253, 254, 250, 248, 251, 250, 252, 246, 250, 247, 249, 248, 252, 247, 252, 250, 247, 250, 252, 250, 257, 252, 253, 251, 251, 248, 250, 259, 252, 249, 251, 248, 253, 253, 251, 253, 252, 252, 252, 254, 255, 247, 247, 254, 249, 252, 252, 247, 249]\n",
      "2024-06-29 13:30:33,091 - micro - MainProcess - INFO     Data converted to numpy array: [255 252 248 246 256 250 253 250 250 251 250 253 254 250 248 251 250 252\n",
      " 246 250 247 249 248 252 247 252 250 247 250 252 250 257 252 253 251 251\n",
      " 248 250 259 252 249 251 248 253 253 251 253 252 252 252 254 255 247 247\n",
      " 254 249 252 252 247 249] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [255 252 248 246 256 250 253 250 250 251 250 253 254 250 248 251 250 252\n",
      " 246 250 247 249 248 252 247 252 250 247 250 252 250 257 252 253 251 251\n",
      " 248 250 259 252 249 251 248 253 253 251 253 252 252 252 254 255 247 247\n",
      " 254 249 252 252 247 249]\n",
      "2024-06-29 13:30:33,095 - micro - MainProcess - INFO     Calculated median: 251.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 251.0\n",
      "2024-06-29 13:30:33,101 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 3.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 3.0\n",
      "2024-06-29 13:30:33,104 - micro - MainProcess - INFO     Calculated 95th percentile: 255.05 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 255.05\n",
      "2024-06-29 13:30:33,108 - micro - MainProcess - INFO     Calculated 99th percentile: 257.82 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 257.82\n",
      "2024-06-29 13:30:33,112 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.01 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.01\n",
      "2024-06-29 13:30:33,118 - micro - MainProcess - INFO     Result: (251.0, 3.0, 255.05, 257.82, 0.01) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (251.0, 3.0, 255.05, 257.82, 0.01)\n",
      "2024-06-29 13:30:33,123 - micro - MainProcess - INFO     Calculating statistics for data: [74, 77, 73, 76, 73, 73, 68, 66, 75, 80, 77, 68, 68, 73, 72, 67, 78, 73, 80, 64, 71, 72, 74, 68, 61, 74, 65, 70, 74, 72, 75, 67, 67, 75, 74, 71, 73, 77, 74, 70, 74, 75, 77, 67, 72, 73, 69, 71, 66, 71, 74, 62, 66, 75, 72, 70, 69, 69, 67, 72] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [74, 77, 73, 76, 73, 73, 68, 66, 75, 80, 77, 68, 68, 73, 72, 67, 78, 73, 80, 64, 71, 72, 74, 68, 61, 74, 65, 70, 74, 72, 75, 67, 67, 75, 74, 71, 73, 77, 74, 70, 74, 75, 77, 67, 72, 73, 69, 71, 66, 71, 74, 62, 66, 75, 72, 70, 69, 69, 67, 72]\n",
      "2024-06-29 13:30:33,126 - micro - MainProcess - INFO     Data converted to numpy array: [74 77 73 76 73 73 68 66 75 80 77 68 68 73 72 67 78 73 80 64 71 72 74 68\n",
      " 61 74 65 70 74 72 75 67 67 75 74 71 73 77 74 70 74 75 77 67 72 73 69 71\n",
      " 66 71 74 62 66 75 72 70 69 69 67 72] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [74 77 73 76 73 73 68 66 75 80 77 68 68 73 72 67 78 73 80 64 71 72 74 68\n",
      " 61 74 65 70 74 72 75 67 67 75 74 71 73 77 74 70 74 75 77 67 72 73 69 71\n",
      " 66 71 74 62 66 75 72 70 69 69 67 72]\n",
      "2024-06-29 13:30:33,132 - micro - MainProcess - INFO     Calculated median: 72.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 72.0\n",
      "2024-06-29 13:30:33,137 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 6.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 6.0\n",
      "2024-06-29 13:30:33,142 - micro - MainProcess - INFO     Calculated 95th percentile: 77.05 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 77.05\n",
      "2024-06-29 13:30:33,147 - micro - MainProcess - INFO     Calculated 99th percentile: 80.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 80.0\n",
      "2024-06-29 13:30:33,154 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.06 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.06\n",
      "2024-06-29 13:30:33,158 - micro - MainProcess - INFO     Result: (72.0, 6.0, 77.05, 80.0, 0.06) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (72.0, 6.0, 77.05, 80.0, 0.06)\n",
      "2024-06-29 13:30:33,169 - micro - MainProcess - INFO     Calculating statistics for data: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 233.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 242.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 205.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.73, 1, 1, 1, 1, 42.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 217.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 231.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 254.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 441.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 423.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 452.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 650.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.28, 1, 1, 1, 324.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 195.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 744.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 221.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 525.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 601.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 540.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 97.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 235.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 215.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 476.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 422.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 564.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.29, 1, 1, 1, 1, 1, 1, 49.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.32, 1, 1, 1, 44.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 209.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 230.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 418.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 432.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 485.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 165.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 687.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 597.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 215.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 68.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 289.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 161.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 198.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 494.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 494.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.48, 1, 1, 1, 1, 1, 1, 1, 1, 48.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 80.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 480.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 119.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 405.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1637.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 438.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.98, 1, 1, 1, 1, 1, 1, 358.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 76.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 451.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 426.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 153.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 431.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 228.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 163.95, 1, 1, 1, 1, 1, 1, 298.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 430.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 426.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 176.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 185.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1386.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1475.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 179.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 82.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1152.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 170.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 185.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 188.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 168.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 231.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 171.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 277.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 56.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 222.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 437.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.71, 1, 1, 1, 1, 1, 279.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 146.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 430.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 138.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 182.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 137.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 544.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 76.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 133.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 66.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 86.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 93.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 31.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 201.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 215.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 254.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 163.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 454.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 114.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 196.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 83.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 428.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 300.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 34.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 200.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 18.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 216.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 244.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 232.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 249.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 135.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.58, 1, 1, 1, 1, 1, 1, 1, 1, 47.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 67.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 464.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 469.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 159.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 229.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 66.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 186.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.79, 1, 1, 1, 1, 1, 268.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.16, 1, 1, 1, 1, 1, 1, 253.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.75, 1, 1, 1, 1, 1, 1, 43.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 66.34, 1, 1, 1, 1, 1, 1, 169.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1206.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 389.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 205.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 223.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 53.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 67.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 233.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 242.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 205.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.73, 1, 1, 1, 1, 42.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 217.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 231.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 254.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 441.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 423.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 452.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 650.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.28, 1, 1, 1, 324.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 195.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 744.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 221.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 525.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 601.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 540.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 97.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 235.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 215.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 476.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 422.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 564.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.29, 1, 1, 1, 1, 1, 1, 49.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.32, 1, 1, 1, 44.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 209.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 230.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 418.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 432.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 485.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 165.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 687.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 597.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 215.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 68.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 289.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 161.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 198.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 494.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 494.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 63.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.48, 1, 1, 1, 1, 1, 1, 1, 1, 48.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 80.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 480.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 119.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 405.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1637.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 438.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.98, 1, 1, 1, 1, 1, 1, 358.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 76.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 451.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 426.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 153.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 431.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 228.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 163.95, 1, 1, 1, 1, 1, 1, 298.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 430.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 426.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 176.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 185.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1386.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1475.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 179.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 82.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1152.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 170.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 185.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 188.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 168.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 231.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 171.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 277.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 56.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 222.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 437.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.71, 1, 1, 1, 1, 1, 279.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 146.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 430.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 138.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 182.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 137.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 544.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 76.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 133.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 66.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 86.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 93.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 31.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 201.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 215.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 254.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 163.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 454.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 114.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 196.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 83.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 428.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 300.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 34.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 200.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 18.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 216.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 244.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 232.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 249.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 135.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.58, 1, 1, 1, 1, 1, 1, 1, 1, 47.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 375.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 67.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 464.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 469.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 159.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 229.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 66.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 186.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.79, 1, 1, 1, 1, 1, 268.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.16, 1, 1, 1, 1, 1, 1, 253.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.75, 1, 1, 1, 1, 1, 1, 43.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 66.34, 1, 1, 1, 1, 1, 1, 169.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1206.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 389.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 205.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 223.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 53.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 67.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 264.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2024-06-29 13:30:33,178 - micro - MainProcess - INFO     Data converted to numpy array: [1. 1. 1. ... 1. 1. 1.] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [1. 1. 1. ... 1. 1. 1.]\n",
      "2024-06-29 13:30:33,183 - micro - MainProcess - INFO     Calculated median: 1.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 1.0\n",
      "2024-06-29 13:30:33,191 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-29 13:30:33,196 - micro - MainProcess - INFO     Calculated 95th percentile: 1.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 1.0\n",
      "2024-06-29 13:30:33,201 - micro - MainProcess - INFO     Calculated 99th percentile: 355.42 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 355.42\n",
      "2024-06-29 13:30:33,212 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 5.17 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 5.17\n",
      "2024-06-29 13:30:33,214 - micro - MainProcess - INFO     Result: (1.0, 0.0, 1.0, 355.42, 5.17) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (1.0, 0.0, 1.0, 355.42, 5.17)\n",
      "2024-06-29 13:30:33,217 - micro - MainProcess - INFO     Calculating statistics for data: [0.88, 1.01, 1.05, 0.97, 0.82, 0.75, 0.67, 0.7, 0.6, 0.81, 0.88, 0.77, 0.82, 1.18, 0.73, 1.01, 0.85, 0.98, 0.65, 0.69, 0.71, 0.8, 0.77, 0.62, 0.68, 0.7, 0.63, 0.63, 0.69, 0.69, 0.65, 0.62, 0.77, 0.69, 0.67, 0.6, 0.73, 0.72, 0.82, 0.73, 1.46, 1.03, 0.73, 0.67, 1.02, 0.88, 0.7, 0.76, 0.62, 0.73, 0.68, 0.66, 0.68, 0.57, 0.67, 1.51, 0.96, 0.73, 0.71, 0.65] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [0.88, 1.01, 1.05, 0.97, 0.82, 0.75, 0.67, 0.7, 0.6, 0.81, 0.88, 0.77, 0.82, 1.18, 0.73, 1.01, 0.85, 0.98, 0.65, 0.69, 0.71, 0.8, 0.77, 0.62, 0.68, 0.7, 0.63, 0.63, 0.69, 0.69, 0.65, 0.62, 0.77, 0.69, 0.67, 0.6, 0.73, 0.72, 0.82, 0.73, 1.46, 1.03, 0.73, 0.67, 1.02, 0.88, 0.7, 0.76, 0.62, 0.73, 0.68, 0.66, 0.68, 0.57, 0.67, 1.51, 0.96, 0.73, 0.71, 0.65]\n",
      "2024-06-29 13:30:33,222 - micro - MainProcess - INFO     Data converted to numpy array: [0.88 1.01 1.05 0.97 0.82 0.75 0.67 0.7  0.6  0.81 0.88 0.77 0.82 1.18\n",
      " 0.73 1.01 0.85 0.98 0.65 0.69 0.71 0.8  0.77 0.62 0.68 0.7  0.63 0.63\n",
      " 0.69 0.69 0.65 0.62 0.77 0.69 0.67 0.6  0.73 0.72 0.82 0.73 1.46 1.03\n",
      " 0.73 0.67 1.02 0.88 0.7  0.76 0.62 0.73 0.68 0.66 0.68 0.57 0.67 1.51\n",
      " 0.96 0.73 0.71 0.65] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [0.88 1.01 1.05 0.97 0.82 0.75 0.67 0.7  0.6  0.81 0.88 0.77 0.82 1.18\n",
      " 0.73 1.01 0.85 0.98 0.65 0.69 0.71 0.8  0.77 0.62 0.68 0.7  0.63 0.63\n",
      " 0.69 0.69 0.65 0.62 0.77 0.69 0.67 0.6  0.73 0.72 0.82 0.73 1.46 1.03\n",
      " 0.73 0.67 1.02 0.88 0.7  0.76 0.62 0.73 0.68 0.66 0.68 0.57 0.67 1.51\n",
      " 0.96 0.73 0.71 0.65]\n",
      "2024-06-29 13:30:33,227 - micro - MainProcess - INFO     Calculated median: 0.73 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.73\n",
      "2024-06-29 13:30:33,233 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.16 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.16\n",
      "2024-06-29 13:30:33,237 - micro - MainProcess - INFO     Calculated 95th percentile: 1.06 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 1.06\n",
      "2024-06-29 13:30:33,244 - micro - MainProcess - INFO     Calculated 99th percentile: 1.48 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 1.48\n",
      "2024-06-29 13:30:33,249 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.24 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.24\n",
      "2024-06-29 13:30:33,252 - micro - MainProcess - INFO     Result: (0.73, 0.16, 1.06, 1.48, 0.24) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.73, 0.16, 1.06, 1.48, 0.24)\n",
      "2024-06-29 13:30:33,260 - micro - MainProcess - INFO     Calculating statistics for data: [8.09, 8.06, 8.19, 9.01, 8.08, 7.68, 8.02, 8.25, 6.85, 8.85, 7.62, 7.79, 7.66, 9.64, 7.04, 6.94, 7.11, 7.15, 5.67, 7.05, 7.05, 7.4, 8.29, 7.14, 7.14, 7.03, 6.98, 7.08, 7.25, 9.94, 6.74, 6.57, 6.72, 6.72, 6.94, 7.36, 7.45, 7.34, 7.3, 6.03, 6.01, 5.95, 6.3, 6.27, 6.71, 6.66, 6.47, 6.87, 6.68, 6.53, 7.02, 6.99, 7.4, 7.43, 7.45, 7.4, 7.31, 7.23, 7.03, 5.83] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [8.09, 8.06, 8.19, 9.01, 8.08, 7.68, 8.02, 8.25, 6.85, 8.85, 7.62, 7.79, 7.66, 9.64, 7.04, 6.94, 7.11, 7.15, 5.67, 7.05, 7.05, 7.4, 8.29, 7.14, 7.14, 7.03, 6.98, 7.08, 7.25, 9.94, 6.74, 6.57, 6.72, 6.72, 6.94, 7.36, 7.45, 7.34, 7.3, 6.03, 6.01, 5.95, 6.3, 6.27, 6.71, 6.66, 6.47, 6.87, 6.68, 6.53, 7.02, 6.99, 7.4, 7.43, 7.45, 7.4, 7.31, 7.23, 7.03, 5.83]\n",
      "2024-06-29 13:30:33,265 - micro - MainProcess - INFO     Data converted to numpy array: [8.09 8.06 8.19 9.01 8.08 7.68 8.02 8.25 6.85 8.85 7.62 7.79 7.66 9.64\n",
      " 7.04 6.94 7.11 7.15 5.67 7.05 7.05 7.4  8.29 7.14 7.14 7.03 6.98 7.08\n",
      " 7.25 9.94 6.74 6.57 6.72 6.72 6.94 7.36 7.45 7.34 7.3  6.03 6.01 5.95\n",
      " 6.3  6.27 6.71 6.66 6.47 6.87 6.68 6.53 7.02 6.99 7.4  7.43 7.45 7.4\n",
      " 7.31 7.23 7.03 5.83] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [8.09 8.06 8.19 9.01 8.08 7.68 8.02 8.25 6.85 8.85 7.62 7.79 7.66 9.64\n",
      " 7.04 6.94 7.11 7.15 5.67 7.05 7.05 7.4  8.29 7.14 7.14 7.03 6.98 7.08\n",
      " 7.25 9.94 6.74 6.57 6.72 6.72 6.94 7.36 7.45 7.34 7.3  6.03 6.01 5.95\n",
      " 6.3  6.27 6.71 6.66 6.47 6.87 6.68 6.53 7.02 6.99 7.4  7.43 7.45 7.4\n",
      " 7.31 7.23 7.03 5.83]\n",
      "2024-06-29 13:30:33,268 - micro - MainProcess - INFO     Calculated median: 7.12 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 7.12\n",
      "2024-06-29 13:30:33,273 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.76 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.76\n",
      "2024-06-29 13:30:33,278 - micro - MainProcess - INFO     Calculated 95th percentile: 8.86 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 8.86\n",
      "2024-06-29 13:30:33,282 - micro - MainProcess - INFO     Calculated 99th percentile: 9.76 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 9.76\n",
      "2024-06-29 13:30:33,285 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.11 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.11\n",
      "2024-06-29 13:30:33,293 - micro - MainProcess - INFO     Result: (7.12, 0.76, 8.86, 9.76, 0.11) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (7.12, 0.76, 8.86, 9.76, 0.11)\n",
      "2024-06-29 13:30:33,300 - micro - MainProcess - INFO     Calculating statistics for data: [495, 494, 503, 496, 502, 493, 509, 501, 500, 498, 508, 500, 499, 502, 496, 503, 498, 497, 498, 501, 492, 503, 495, 499, 504, 503, 501, 503, 503, 500, 498, 505, 514, 509, 509, 502, 493, 505, 507, 496, 503, 503, 488, 505, 510, 501, 506, 503, 500, 507, 501, 495, 502, 505, 496, 504, 503, 500, 504, 495] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [495, 494, 503, 496, 502, 493, 509, 501, 500, 498, 508, 500, 499, 502, 496, 503, 498, 497, 498, 501, 492, 503, 495, 499, 504, 503, 501, 503, 503, 500, 498, 505, 514, 509, 509, 502, 493, 505, 507, 496, 503, 503, 488, 505, 510, 501, 506, 503, 500, 507, 501, 495, 502, 505, 496, 504, 503, 500, 504, 495]\n",
      "2024-06-29 13:30:33,310 - micro - MainProcess - INFO     Data converted to numpy array: [495 494 503 496 502 493 509 501 500 498 508 500 499 502 496 503 498 497\n",
      " 498 501 492 503 495 499 504 503 501 503 503 500 498 505 514 509 509 502\n",
      " 493 505 507 496 503 503 488 505 510 501 506 503 500 507 501 495 502 505\n",
      " 496 504 503 500 504 495] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [495 494 503 496 502 493 509 501 500 498 508 500 499 502 496 503 498 497\n",
      " 498 501 492 503 495 499 504 503 501 503 503 500 498 505 514 509 509 502\n",
      " 493 505 507 496 503 503 488 505 510 501 506 503 500 507 501 495 502 505\n",
      " 496 504 503 500 504 495]\n",
      "2024-06-29 13:30:33,314 - micro - MainProcess - INFO     Calculated median: 501.5 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 501.5\n",
      "2024-06-29 13:30:33,320 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 6.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 6.0\n",
      "2024-06-29 13:30:33,323 - micro - MainProcess - INFO     Calculated 95th percentile: 509.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 509.0\n",
      "2024-06-29 13:30:33,327 - micro - MainProcess - INFO     Calculated 99th percentile: 511.64 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 511.64\n",
      "2024-06-29 13:30:33,334 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.01 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.01\n",
      "2024-06-29 13:30:33,337 - micro - MainProcess - INFO     Result: (501.5, 6.0, 509.0, 511.64, 0.01) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (501.5, 6.0, 509.0, 511.64, 0.01)\n",
      "2024-06-29 13:30:33,341 - micro - MainProcess - INFO     Calculating statistics for data: [74, 77, 74, 77, 66, 73, 74, 69, 80, 75, 68, 77, 68, 73, 73, 67, 73, 78, 80, 71, 72, 64, 73, 62, 74, 68, 66, 72, 70, 74, 74, 74, 67, 68, 73, 77, 71, 72, 74, 74, 70, 75, 77, 72, 67, 74, 69, 71, 70, 66, 73, 67, 63, 72, 76, 69, 70, 66, 68, 72] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [74, 77, 74, 77, 66, 73, 74, 69, 80, 75, 68, 77, 68, 73, 73, 67, 73, 78, 80, 71, 72, 64, 73, 62, 74, 68, 66, 72, 70, 74, 74, 74, 67, 68, 73, 77, 71, 72, 74, 74, 70, 75, 77, 72, 67, 74, 69, 71, 70, 66, 73, 67, 63, 72, 76, 69, 70, 66, 68, 72]\n",
      "2024-06-29 13:30:33,346 - micro - MainProcess - INFO     Data converted to numpy array: [74 77 74 77 66 73 74 69 80 75 68 77 68 73 73 67 73 78 80 71 72 64 73 62\n",
      " 74 68 66 72 70 74 74 74 67 68 73 77 71 72 74 74 70 75 77 72 67 74 69 71\n",
      " 70 66 73 67 63 72 76 69 70 66 68 72] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [74 77 74 77 66 73 74 69 80 75 68 77 68 73 73 67 73 78 80 71 72 64 73 62\n",
      " 74 68 66 72 70 74 74 74 67 68 73 77 71 72 74 74 70 75 77 72 67 74 69 71\n",
      " 70 66 73 67 63 72 76 69 70 66 68 72]\n",
      "2024-06-29 13:30:33,352 - micro - MainProcess - INFO     Calculated median: 72.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 72.0\n",
      "2024-06-29 13:30:33,358 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 6.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 6.0\n",
      "2024-06-29 13:30:33,363 - micro - MainProcess - INFO     Calculated 95th percentile: 77.05 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 77.05\n",
      "2024-06-29 13:30:33,367 - micro - MainProcess - INFO     Calculated 99th percentile: 80.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 80.0\n",
      "2024-06-29 13:30:33,373 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.06 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.06\n",
      "2024-06-29 13:30:33,377 - micro - MainProcess - INFO     Result: (72.0, 6.0, 77.05, 80.0, 0.06) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (72.0, 6.0, 77.05, 80.0, 0.06)\n",
      "2024-06-29 13:30:33,384 - micro - MainProcess - INFO     Calculating statistics for data: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 119.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 546.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 410.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.27, 1, 1, 1, 1, 1, 1, 1, 1, 731.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 469.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 62.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 527.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 463.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 638.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 55.27, 1, 1, 1, 1, 1, 1, 1, 1, 273.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 457.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 435.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 580.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 107.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 57.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 438.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 599.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 554.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 405.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 426.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 484.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 411.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 407.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 449.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 393.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 502.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 75.51, 1, 1, 1, 521.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.65, 1, 1, 1, 1, 1, 44.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 441.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 441.74, 1, 1, 1, 42.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 500.95, 1, 1, 1, 44.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 76.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 440.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 435.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 559.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.13, 1, 1, 1, 1, 1, 1, 1, 318.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 473.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 429.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 134.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.29, 1, 1, 1, 1, 1, 1, 1, 1, 287.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 40.78, 1, 1, 1, 1, 259.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 40.43, 1, 1, 1, 1, 1, 1, 363.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1236.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 515.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 542.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 465.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 520.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 468.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 109.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 434.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 389.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 438.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 117.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 657.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 502.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 242.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 507.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 234.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 462.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 169.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 189.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 489.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 466.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 600.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 622.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 389.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 445.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 480.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 411.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 503.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 542.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 213.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 455.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 465.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 502.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 277.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.42, 273.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 436.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 203.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 277.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 434.98, 1, 1, 1, 1, 52.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.21, 1, 1, 1, 1, 1, 456.79, 1, 1, 1, 1, 1, 55.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 458.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 414.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 414.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 479.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 471.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 435.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 412.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 508.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 85.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 174.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 244.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 146.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 218.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 233.2, 1, 1, 1, 6.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 241.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 407.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 300.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 419.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 479.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 19.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1427.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 172.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 235.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 470.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 53.22, 1, 1, 1, 1, 1, 1, 1, 1, 430.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.79, 1, 1, 1, 1, 1, 1, 350.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 484.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 504.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 524.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 498.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 184.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 201.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 65.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 61.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 218.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 492.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 190.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 443.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.7, 1, 1, 1, 1, 1, 1.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 189.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 554.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 486.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 546.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 476.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 557.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 623.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 517.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 451.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 563.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3755.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 32.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 30.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 15.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 219.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 163.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 577.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 228.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 141.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 210.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 112.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 150.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 197.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 429.03, 1, 1, 1, 1, 1, 1, 41.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 491.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 97.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 101.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 194.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 183.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 166.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 216.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1228.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 447.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1308.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.55, 280.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1322.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 455.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 187.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 173.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 412.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.5, 1, 1, 1, 1, 1, 1, 1, 1, 37.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 116.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.66, 1, 1, 1, 1, 1, 266.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1556.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 56.45, 1, 1, 1, 250.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 465.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 67.19, 1, 1, 1, 1, 1, 1, 151.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 249.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.09, 1, 1, 1, 1, 1, 1, 1, 1, 45.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 190.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.21, 1, 1, 1, 45.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.3, 1, 1, 1, 1, 42.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 183.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 74.33, 1, 1, 1, 1, 1, 1, 22.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 201.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 289.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 200.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 28.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 124.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 199.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 202.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 241.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 211.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 21.36, 1, 1, 236.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.18, 1, 1, 236.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 228.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 446.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 53.54, 322.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 56.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 209.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 244.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 95.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 225.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 477.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 119.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 434.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.0, 270.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 468.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 533.11, 1, 1, 1, 1, 31.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 162.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 405.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.73, 1, 1, 1, 1, 1, 347.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.39, 1, 1, 1, 1, 1, 244.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 226.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 25.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 233.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 437.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 419.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 120.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 507.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 213.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 413.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 436.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 242.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 57.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.29, 1, 1, 1, 1, 431.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 442.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 232.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 470.27, 1, 1, 1, 1, 1, 1, 1, 38.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 476.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 470.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.19, 1, 1, 1, 1, 1, 1, 1, 246.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 449.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 289.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 454.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 105.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 93.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.48, 1.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 423.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 491.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 452.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 428.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 453.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 411.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 481.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 108.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 414.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 176.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 428.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 431.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 38.7, 1, 1, 1, 1, 1, 424.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 33.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 454.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 464.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 15.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.87, 1, 1, 1, 1, 271.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 55.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 696.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 773.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 585.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 485.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.85, 1, 1, 1, 1, 49.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 208.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.8, 1, 1, 1, 1, 1, 1, 1, 1, 46.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 214.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 225.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 133.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 300.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 223.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 241.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 204.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 136.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 119.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 546.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 410.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.27, 1, 1, 1, 1, 1, 1, 1, 1, 731.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 469.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 62.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 527.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 463.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 638.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 55.27, 1, 1, 1, 1, 1, 1, 1, 1, 273.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 457.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 435.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 580.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 107.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 424.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 57.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 438.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 599.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 554.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 405.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 426.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 484.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 411.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 407.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 449.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 393.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 502.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 75.51, 1, 1, 1, 521.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.65, 1, 1, 1, 1, 1, 44.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 441.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 441.74, 1, 1, 1, 42.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 500.95, 1, 1, 1, 44.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 76.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 440.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 435.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 559.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.13, 1, 1, 1, 1, 1, 1, 1, 318.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 473.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 429.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 134.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.29, 1, 1, 1, 1, 1, 1, 1, 1, 287.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 40.78, 1, 1, 1, 1, 259.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 40.43, 1, 1, 1, 1, 1, 1, 363.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1236.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 515.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 542.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 465.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 520.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 468.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 448.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 109.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 417.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 434.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 389.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 438.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 117.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 657.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 502.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 297.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 242.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 507.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 234.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 462.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 169.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 189.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 489.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 466.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 600.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 622.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 389.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 445.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 480.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 411.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 503.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 542.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 213.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 455.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 465.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 502.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 277.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.42, 273.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 436.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 203.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 277.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 434.98, 1, 1, 1, 1, 52.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.21, 1, 1, 1, 1, 1, 456.79, 1, 1, 1, 1, 1, 55.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 458.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 52.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 414.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 414.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 479.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 471.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 435.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 412.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 508.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 85.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 174.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 244.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 146.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 218.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 233.2, 1, 1, 1, 6.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 314.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 241.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 407.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 300.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 419.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 479.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 19.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1427.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 172.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 420.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 235.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 470.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 53.22, 1, 1, 1, 1, 1, 1, 1, 1, 430.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 379.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.79, 1, 1, 1, 1, 1, 1, 350.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 484.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 504.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 524.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 498.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 184.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 201.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 408.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 65.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 358.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 61.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 218.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 492.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 190.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 443.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.7, 1, 1, 1, 1, 1, 1.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 253.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 189.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 396.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 554.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 486.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 546.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 476.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 557.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 623.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 517.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 451.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 563.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3755.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 32.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 30.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 15.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 219.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 163.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 369.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 577.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 228.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 141.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 210.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 112.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 291.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 150.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 409.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 197.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 429.03, 1, 1, 1, 1, 1, 1, 41.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 491.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 97.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 101.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 194.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 183.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 359.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 352.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 245.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 346.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 166.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 216.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.49, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1228.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 447.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 258.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1308.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 315.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.55, 280.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 366.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 350.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1322.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 455.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 187.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 173.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 360.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 412.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 330.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.5, 1, 1, 1, 1, 1, 1, 1, 1, 37.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 116.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 243.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.66, 1, 1, 1, 1, 1, 266.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1556.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 56.45, 1, 1, 1, 250.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 465.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 67.19, 1, 1, 1, 1, 1, 1, 151.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 260.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 249.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.09, 1, 1, 1, 1, 1, 1, 1, 1, 45.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 190.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.21, 1, 1, 1, 45.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 340.3, 1, 1, 1, 1, 42.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 183.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 318.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 394.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 74.33, 1, 1, 1, 1, 1, 1, 22.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 421.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 201.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 289.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 270.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 317.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 200.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 255.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 28.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 124.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 199.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 250.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 266.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 236.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 202.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 241.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 211.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 21.36, 1, 1, 236.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.18, 1, 1, 236.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 228.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 343.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 329.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 446.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 53.54, 322.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.35, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 51.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 261.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 56.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 209.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 244.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 239.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 95.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 225.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 477.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 248.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 259.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 257.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 119.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 434.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.0, 270.34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 256.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 468.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 319.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 533.11, 1, 1, 1, 1, 31.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 162.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 405.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.73, 1, 1, 1, 1, 1, 347.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50.39, 1, 1, 1, 1, 1, 244.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 295.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 226.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 294.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 386.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 25.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 292.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 238.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 44.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.33, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 233.83, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 437.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 313.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 419.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 342.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 247.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 120.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 416.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 378.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 507.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 399.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 213.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 267.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 281.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 265.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 391.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 388.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 413.06, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 436.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 271.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 242.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 278.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.75, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 246.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 337.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 279.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 240.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 276.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 327.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 57.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 237.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.29, 1, 1, 1, 1, 431.66, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 263.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 442.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 362.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 387.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 385.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 290.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 383.12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280.71, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 285.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 232.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 470.27, 1, 1, 1, 1, 1, 1, 1, 38.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 301.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 345.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 476.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41.77, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 373.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 355.59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 331.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 470.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1.19, 1, 1, 1, 1, 1, 1, 1, 246.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 361.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 304.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.63, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 308.17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 449.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 392.65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 289.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 380.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 454.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 105.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 288.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 328.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 324.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 403.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.79, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 93.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 425.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 303.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.84, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 406.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.44, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.04, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 349.48, 1.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 357.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 311.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.97, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 262.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 338.98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 423.67, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 351.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 348.23, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 491.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 336.16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 286.32, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 452.22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 404.42, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 374.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 428.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 390.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 453.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 43.02, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 411.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 48.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 481.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54.25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 49.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 370.56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 108.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 402.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 299.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 333.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 320.1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 298.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.08, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 354.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 414.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.64, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 384.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 427.93, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.91, 1, 1, 1, 1, 1, 1, 1, 1, 1, 353.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 332.72, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 377.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 400.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 176.57, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 428.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 371.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 376.55, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 368.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 323.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 401.99, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 382.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 433.51, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 431.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 344.69, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 381.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 335.46, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46.7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307.45, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 302.52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 310.14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 274.48, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 326.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 269.19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 39.81, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 347.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.43, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 461.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 38.7, 1, 1, 1, 1, 1, 424.76, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 33.78, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 397.09, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 415.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 395.8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 454.11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 464.82, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 15.6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 372.37, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 341.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 365.31, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.87, 1, 1, 1, 1, 271.95, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.53, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 364.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 321.94, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 325.86, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 55.15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 287.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 696.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 773.3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 45.73, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 585.68, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 47.38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 485.18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339.39, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 251.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 275.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 363.85, 1, 1, 1, 1, 49.85, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 208.21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 283.89, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 312.8, 1, 1, 1, 1, 1, 1, 1, 1, 46.74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 214.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 225.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 133.4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 322.36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 282.29, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 306.61, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 284.05, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 293.87, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 300.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 398.92, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 367.9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 223.03, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 316.41, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 305.47, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 241.96, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 334.24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 42.88, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 204.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 296.07, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 356.54, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 136.26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2024-06-29 13:30:33,416 - micro - MainProcess - INFO     Data converted to numpy array: [1. 1. 1. ... 1. 1. 1.] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [1. 1. 1. ... 1. 1. 1.]\n",
      "2024-06-29 13:30:33,422 - micro - MainProcess - INFO     Calculated median: 1.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 1.0\n",
      "2024-06-29 13:30:33,428 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-29 13:30:33,432 - micro - MainProcess - INFO     Calculated 95th percentile: 1.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 1.0\n",
      "2024-06-29 13:30:33,435 - micro - MainProcess - INFO     Calculated 99th percentile: 378.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 378.0\n",
      "2024-06-29 13:30:33,445 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 5.19 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 5.19\n",
      "2024-06-29 13:30:33,448 - micro - MainProcess - INFO     Result: (1.0, 0.0, 1.0, 378.0, 5.19) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (1.0, 0.0, 1.0, 378.0, 5.19)\n",
      "2024-06-29 13:30:33,451 - micro - MainProcess - INFO     Calculating statistics for data: [1.28, 1.2, 1.19, 1.97, 1.16, 0.67, 0.9, 1.13, 0.78, 0.8, 1.17, 1.28, 1.11, 2.94, 0.76, 0.65, 0.71, 0.67, 0.92, 1.09, 0.86, 0.69, 1.95, 0.75, 0.71, 0.66, 0.77, 0.87, 1.16, 0.78, 0.74, 0.71, 0.65, 0.69, 0.94, 0.63, 0.66, 0.53, 0.55, 0.72, 0.67, 0.61, 0.78, 0.59, 0.71, 0.59, 0.73, 0.68, 0.83, 0.64, 0.67, 0.56, 0.74, 0.76, 0.68, 0.62, 0.8, 0.74, 0.66, 0.77] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [1.28, 1.2, 1.19, 1.97, 1.16, 0.67, 0.9, 1.13, 0.78, 0.8, 1.17, 1.28, 1.11, 2.94, 0.76, 0.65, 0.71, 0.67, 0.92, 1.09, 0.86, 0.69, 1.95, 0.75, 0.71, 0.66, 0.77, 0.87, 1.16, 0.78, 0.74, 0.71, 0.65, 0.69, 0.94, 0.63, 0.66, 0.53, 0.55, 0.72, 0.67, 0.61, 0.78, 0.59, 0.71, 0.59, 0.73, 0.68, 0.83, 0.64, 0.67, 0.56, 0.74, 0.76, 0.68, 0.62, 0.8, 0.74, 0.66, 0.77]\n",
      "2024-06-29 13:30:33,460 - micro - MainProcess - INFO     Data converted to numpy array: [1.28 1.2  1.19 1.97 1.16 0.67 0.9  1.13 0.78 0.8  1.17 1.28 1.11 2.94\n",
      " 0.76 0.65 0.71 0.67 0.92 1.09 0.86 0.69 1.95 0.75 0.71 0.66 0.77 0.87\n",
      " 1.16 0.78 0.74 0.71 0.65 0.69 0.94 0.63 0.66 0.53 0.55 0.72 0.67 0.61\n",
      " 0.78 0.59 0.71 0.59 0.73 0.68 0.83 0.64 0.67 0.56 0.74 0.76 0.68 0.62\n",
      " 0.8  0.74 0.66 0.77] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [1.28 1.2  1.19 1.97 1.16 0.67 0.9  1.13 0.78 0.8  1.17 1.28 1.11 2.94\n",
      " 0.76 0.65 0.71 0.67 0.92 1.09 0.86 0.69 1.95 0.75 0.71 0.66 0.77 0.87\n",
      " 1.16 0.78 0.74 0.71 0.65 0.69 0.94 0.63 0.66 0.53 0.55 0.72 0.67 0.61\n",
      " 0.78 0.59 0.71 0.59 0.73 0.68 0.83 0.64 0.67 0.56 0.74 0.76 0.68 0.62\n",
      " 0.8  0.74 0.66 0.77]\n",
      "2024-06-29 13:30:33,466 - micro - MainProcess - INFO     Calculated median: 0.74 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.74\n",
      "2024-06-29 13:30:33,490 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.24 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.24\n",
      "2024-06-29 13:30:33,495 - micro - MainProcess - INFO     Calculated 95th percentile: 1.31 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 1.31\n",
      "2024-06-29 13:30:33,500 - micro - MainProcess - INFO     Calculated 99th percentile: 2.37 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 2.37\n",
      "2024-06-29 13:30:33,507 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.45 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.45\n",
      "2024-06-29 13:30:33,511 - micro - MainProcess - INFO     Result: (0.74, 0.24, 1.31, 2.37, 0.45) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.74, 0.24, 1.31, 2.37, 0.45)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------+------------+---------+--------------+-------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+---------+---------------------+---------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    Model_MaxTokens    | is_Streaming | Iterations | Regions | Average TTLT | Median TTLT | IQR TTLT | 95th Percentile TTLT | 99th Percentile TTLT | CV TTLT | Median Prompt Tokens | IQR Prompt Tokens | Median Completion Tokens | IQR Completion Tokens | 95th Percentile Completion Tokens | 99th Percentile Completion Tokens | CV Completion Tokens | Average TBT | Median TBT | IQR TBT | 95th Percentile TBT | 99th Percentile TBT | Average TTFT | Median TTFT | IQR TTFT | 95th Percentile TTFT | 99th Percentile TTFT | Error Rate | Error Types | Successful Runs | Unsuccessful Runs | Throttle Count | Throttle Rate |                                                                  Best Run                                                                  |                                                                 Worst Run                                                                  |\n",
      "+-----------------------+--------------+------------+---------+--------------+-------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+---------+---------------------+---------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| gpt-4o-2024-05-13_250 |     True     |     60     |   N/A   |     3.74     |     3.7     |   0.48   |         4.5          |         5.02         |  0.12   |         72.0         |        6.0        |          251.0           |          3.0          |              255.05               |              257.82               |         0.01         |    12.58    |    1.0     |   0.0   |         1.0         |       355.42        |     0.79     |    0.73     |   0.16   |         1.06         |         1.48         |    0.0     |     []      |       60        |         0         |       0        |      0.0      | {\"ttlt\": 2.88, \"completion_tokens\": 250, \"prompt_tokens\": 75, \"region\": \"N/A\", \"utilization\": \"N/A\", \"local_time\": \"2024-06-29 13:24:26 \"} | {\"ttlt\": 5.48, \"completion_tokens\": 249, \"prompt_tokens\": 72, \"region\": \"N/A\", \"utilization\": \"N/A\", \"local_time\": \"2024-06-29 13:26:40 \"} |\n",
      "| gpt-4o-2024-05-13_500 |     True     |     60     |   N/A   |     7.25     |    7.12     |   0.76   |         8.86         |         9.76         |  0.11   |         72.0         |        6.0        |          501.5           |          6.0          |               509.0               |              511.64               |         0.01         |    13.62    |    1.0     |   0.0   |         1.0         |        378.0        |     0.87     |    0.74     |   0.24   |         1.31         |         2.37         |    0.0     |     []      |       60        |         0         |       0        |      0.0      | {\"ttlt\": 5.67, \"completion_tokens\": 498, \"prompt_tokens\": 80, \"region\": \"N/A\", \"utilization\": \"N/A\", \"local_time\": \"2024-06-29 13:26:47 \"} | {\"ttlt\": 9.94, \"completion_tokens\": 500, \"prompt_tokens\": 74, \"region\": \"N/A\", \"utilization\": \"N/A\", \"local_time\": \"2024-06-29 13:27:07 \"} |\n",
      "+-----------------------+--------------+------------+---------+--------------+-------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+---------+---------------------+---------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-2024-05-13_250': {'median_ttlt': 3.7,\n",
       "  'is_Streaming': True,\n",
       "  'regions': ['N/A'],\n",
       "  'iqr_ttlt': 0.48,\n",
       "  'percentile_95_ttlt': 4.5,\n",
       "  'percentile_99_ttlt': 5.02,\n",
       "  'cv_ttlt': 0.12,\n",
       "  'median_completion_tokens': 251.0,\n",
       "  'iqr_completion_tokens': 3.0,\n",
       "  'percentile_95_completion_tokens': 255.05,\n",
       "  'percentile_99_completion_tokens': 257.82,\n",
       "  'cv_completion_tokens': 0.01,\n",
       "  'median_prompt_tokens': 72.0,\n",
       "  'iqr_prompt_tokens': 6.0,\n",
       "  'percentile_95_prompt_tokens': 77.05,\n",
       "  'percentile_99_prompt_tokens': 80.0,\n",
       "  'cv_prompt_tokens': 0.06,\n",
       "  'average_ttlt': 3.74,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 60,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 60,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 1.0,\n",
       "  'iqr_tbt': 0.0,\n",
       "  'percentile_95_tbt': 1.0,\n",
       "  'percentile_99_tbt': 355.42,\n",
       "  'cv_tbt': 5.17,\n",
       "  'average_tbt': 12.58,\n",
       "  'median_ttft': 0.73,\n",
       "  'iqr_ttft': 0.16,\n",
       "  'percentile_95_ttft': 1.06,\n",
       "  'percentile_99_ttft': 1.48,\n",
       "  'cv_ttft': 0.24,\n",
       "  'average_ttft': 0.79,\n",
       "  'best_run': {'ttlt': 2.88,\n",
       "   'completion_tokens': 250,\n",
       "   'prompt_tokens': 75,\n",
       "   'region': 'N/A',\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-29 13:24:26 '},\n",
       "  'worst_run': {'ttlt': 5.48,\n",
       "   'completion_tokens': 249,\n",
       "   'prompt_tokens': 72,\n",
       "   'region': 'N/A',\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-29 13:26:40 '}},\n",
       " 'gpt-4o-2024-05-13_500': {'median_ttlt': 7.12,\n",
       "  'is_Streaming': True,\n",
       "  'regions': ['N/A'],\n",
       "  'iqr_ttlt': 0.76,\n",
       "  'percentile_95_ttlt': 8.86,\n",
       "  'percentile_99_ttlt': 9.76,\n",
       "  'cv_ttlt': 0.11,\n",
       "  'median_completion_tokens': 501.5,\n",
       "  'iqr_completion_tokens': 6.0,\n",
       "  'percentile_95_completion_tokens': 509.0,\n",
       "  'percentile_99_completion_tokens': 511.64,\n",
       "  'cv_completion_tokens': 0.01,\n",
       "  'median_prompt_tokens': 72.0,\n",
       "  'iqr_prompt_tokens': 6.0,\n",
       "  'percentile_95_prompt_tokens': 77.05,\n",
       "  'percentile_99_prompt_tokens': 80.0,\n",
       "  'cv_prompt_tokens': 0.06,\n",
       "  'average_ttlt': 7.25,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 60,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 60,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 1.0,\n",
       "  'iqr_tbt': 0.0,\n",
       "  'percentile_95_tbt': 1.0,\n",
       "  'percentile_99_tbt': 378.0,\n",
       "  'cv_tbt': 5.19,\n",
       "  'average_tbt': 13.62,\n",
       "  'median_ttft': 0.74,\n",
       "  'iqr_ttft': 0.24,\n",
       "  'percentile_95_ttft': 1.31,\n",
       "  'percentile_99_ttft': 2.37,\n",
       "  'cv_ttft': 0.45,\n",
       "  'average_ttft': 0.87,\n",
       "  'best_run': {'ttlt': 5.67,\n",
       "   'completion_tokens': 498,\n",
       "   'prompt_tokens': 80,\n",
       "   'region': 'N/A',\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-29 13:26:47 '},\n",
       "  'worst_run': {'ttlt': 9.94,\n",
       "   'completion_tokens': 500,\n",
       "   'prompt_tokens': 74,\n",
       "   'region': 'N/A',\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-29 13:27:07 '}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_streaming.calculate_and_show_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 13:30:39,124 - micro - MainProcess - INFO     Calculating statistics for data: [3.1645096999999964, 9.53728030000002, 9.49380670000005, 9.645866000000012, 5.206454399999984, 4.915370999999993, 6.470371999999998, 6.383262000000002, 2.8779327999999964, 4.251231100000041, 4.203759200000036, 3.9812952999999993, 3.2857230000000186, 4.659821099999988, 4.43161200000003, 4.516898200000014, 3.562340199999994, 5.0621997999999735, 4.885093100000006, 5.122121499999992, 3.705830600000013, 3.0054256999999893, 4.076218400000016, 4.148935199999983, 2.8454222999999956, 7.439819900000032, 7.737992799999972, 7.204791, 5.045396500000038, 3.859495200000026, 3.836447899999996, 3.9097689000000173, 4.10140530000001, 4.093308400000012, 10.632616299999995, 4.173042100000032, 4.2076693000000205, 3.780752199999995, 3.6338537000000315, 4.349870199999998, 4.3156627000000185, 4.04470189999995, 4.133800199999996, 6.409432400000014, 3.8213812999999845, 4.213982999999985, 4.902133399999968, 4.436455299999977, 4.253569599999992, 5.737172999999984] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [3.1645096999999964, 9.53728030000002, 9.49380670000005, 9.645866000000012, 5.206454399999984, 4.915370999999993, 6.470371999999998, 6.383262000000002, 2.8779327999999964, 4.251231100000041, 4.203759200000036, 3.9812952999999993, 3.2857230000000186, 4.659821099999988, 4.43161200000003, 4.516898200000014, 3.562340199999994, 5.0621997999999735, 4.885093100000006, 5.122121499999992, 3.705830600000013, 3.0054256999999893, 4.076218400000016, 4.148935199999983, 2.8454222999999956, 7.439819900000032, 7.737992799999972, 7.204791, 5.045396500000038, 3.859495200000026, 3.836447899999996, 3.9097689000000173, 4.10140530000001, 4.093308400000012, 10.632616299999995, 4.173042100000032, 4.2076693000000205, 3.780752199999995, 3.6338537000000315, 4.349870199999998, 4.3156627000000185, 4.04470189999995, 4.133800199999996, 6.409432400000014, 3.8213812999999845, 4.213982999999985, 4.902133399999968, 4.436455299999977, 4.253569599999992, 5.737172999999984]\n",
      "2024-06-29 13:30:39,128 - micro - MainProcess - INFO     Data converted to numpy array: [ 3.1645097  9.5372803  9.4938067  9.645866   5.2064544  4.915371\n",
      "  6.470372   6.383262   2.8779328  4.2512311  4.2037592  3.9812953\n",
      "  3.285723   4.6598211  4.431612   4.5168982  3.5623402  5.0621998\n",
      "  4.8850931  5.1221215  3.7058306  3.0054257  4.0762184  4.1489352\n",
      "  2.8454223  7.4398199  7.7379928  7.204791   5.0453965  3.8594952\n",
      "  3.8364479  3.9097689  4.1014053  4.0933084 10.6326163  4.1730421\n",
      "  4.2076693  3.7807522  3.6338537  4.3498702  4.3156627  4.0447019\n",
      "  4.1338002  6.4094324  3.8213813  4.213983   4.9021334  4.4364553\n",
      "  4.2535696  5.737173 ] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [ 3.1645097  9.5372803  9.4938067  9.645866   5.2064544  4.915371\n",
      "  6.470372   6.383262   2.8779328  4.2512311  4.2037592  3.9812953\n",
      "  3.285723   4.6598211  4.431612   4.5168982  3.5623402  5.0621998\n",
      "  4.8850931  5.1221215  3.7058306  3.0054257  4.0762184  4.1489352\n",
      "  2.8454223  7.4398199  7.7379928  7.204791   5.0453965  3.8594952\n",
      "  3.8364479  3.9097689  4.1014053  4.0933084 10.6326163  4.1730421\n",
      "  4.2076693  3.7807522  3.6338537  4.3498702  4.3156627  4.0447019\n",
      "  4.1338002  6.4094324  3.8213813  4.213983   4.9021334  4.4364553\n",
      "  4.2535696  5.737173 ]\n",
      "2024-06-29 13:30:39,132 - micro - MainProcess - INFO     Calculated median: 4.25 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 4.25\n",
      "2024-06-29 13:30:39,137 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 1.18 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 1.18\n",
      "2024-06-29 13:30:39,141 - micro - MainProcess - INFO     Calculated 95th percentile: 9.52 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 9.52\n",
      "2024-06-29 13:30:39,147 - micro - MainProcess - INFO     Calculated 99th percentile: 10.15 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 10.15\n",
      "2024-06-29 13:30:39,152 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.36 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.36\n",
      "2024-06-29 13:30:39,155 - micro - MainProcess - INFO     Result: (4.25, 1.18, 9.52, 10.15, 0.36) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (4.25, 1.18, 9.52, 10.15, 0.36)\n",
      "2024-06-29 13:30:39,158 - micro - MainProcess - INFO     Calculating statistics for data: [250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250]\n",
      "2024-06-29 13:30:39,162 - micro - MainProcess - INFO     Data converted to numpy array: [250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250\n",
      " 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250\n",
      " 250 250 250 250 250 250 250 250 250 250 250 250 250 250] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250\n",
      " 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250 250\n",
      " 250 250 250 250 250 250 250 250 250 250 250 250 250 250]\n",
      "2024-06-29 13:30:39,165 - micro - MainProcess - INFO     Calculated median: 250.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 250.0\n",
      "2024-06-29 13:30:39,170 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-29 13:30:39,174 - micro - MainProcess - INFO     Calculated 95th percentile: 250.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 250.0\n",
      "2024-06-29 13:30:39,177 - micro - MainProcess - INFO     Calculated 99th percentile: 250.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 250.0\n",
      "2024-06-29 13:30:39,181 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-29 13:30:39,183 - micro - MainProcess - INFO     Result: (250.0, 0.0, 250.0, 250.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (250.0, 0.0, 250.0, 250.0, 0.0)\n",
      "2024-06-29 13:30:39,185 - micro - MainProcess - INFO     Calculating statistics for data: [74, 67, 66, 76, 73, 66, 77, 72, 63, 71, 73, 79, 74, 68, 74, 64, 71, 65, 74, 70, 75, 68, 75, 66, 74, 77, 72, 71, 74, 74, 70, 77, 72, 67, 75, 68, 71, 71, 74, 74, 66, 63, 76, 66, 69, 70, 69, 71, 72, 67] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [74, 67, 66, 76, 73, 66, 77, 72, 63, 71, 73, 79, 74, 68, 74, 64, 71, 65, 74, 70, 75, 68, 75, 66, 74, 77, 72, 71, 74, 74, 70, 77, 72, 67, 75, 68, 71, 71, 74, 74, 66, 63, 76, 66, 69, 70, 69, 71, 72, 67]\n",
      "2024-06-29 13:30:39,189 - micro - MainProcess - INFO     Data converted to numpy array: [74 67 66 76 73 66 77 72 63 71 73 79 74 68 74 64 71 65 74 70 75 68 75 66\n",
      " 74 77 72 71 74 74 70 77 72 67 75 68 71 71 74 74 66 63 76 66 69 70 69 71\n",
      " 72 67] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [74 67 66 76 73 66 77 72 63 71 73 79 74 68 74 64 71 65 74 70 75 68 75 66\n",
      " 74 77 72 71 74 74 70 77 72 67 75 68 71 71 74 74 66 63 76 66 69 70 69 71\n",
      " 72 67]\n",
      "2024-06-29 13:30:39,194 - micro - MainProcess - INFO     Calculated median: 71.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 71.0\n",
      "2024-06-29 13:30:39,201 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 6.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 6.0\n",
      "2024-06-29 13:30:39,207 - micro - MainProcess - INFO     Calculated 95th percentile: 77.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 77.0\n",
      "2024-06-29 13:30:39,210 - micro - MainProcess - INFO     Calculated 99th percentile: 78.02 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 78.02\n",
      "2024-06-29 13:30:39,218 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.06 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.06\n",
      "2024-06-29 13:30:39,221 - micro - MainProcess - INFO     Result: (71.0, 6.0, 77.0, 78.02, 0.06) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (71.0, 6.0, 77.0, 78.02, 0.06)\n",
      "2024-06-29 13:30:39,227 - micro - MainProcess - INFO     Calculating statistics for data: [0.01, 0.04, 0.04, 0.04, 0.02, 0.02, 0.03, 0.03, 0.01, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.02, 0.01, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.04, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [0.01, 0.04, 0.04, 0.04, 0.02, 0.02, 0.03, 0.03, 0.01, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.02, 0.01, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.04, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]\n",
      "2024-06-29 13:30:39,231 - micro - MainProcess - INFO     Data converted to numpy array: [0.01 0.04 0.04 0.04 0.02 0.02 0.03 0.03 0.01 0.02 0.02 0.02 0.01 0.02\n",
      " 0.02 0.02 0.01 0.02 0.02 0.02 0.01 0.01 0.02 0.02 0.01 0.03 0.03 0.03\n",
      " 0.02 0.02 0.02 0.02 0.02 0.02 0.04 0.02 0.02 0.02 0.01 0.02 0.02 0.02\n",
      " 0.02 0.03 0.02 0.02 0.02 0.02 0.02 0.02] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [0.01 0.04 0.04 0.04 0.02 0.02 0.03 0.03 0.01 0.02 0.02 0.02 0.01 0.02\n",
      " 0.02 0.02 0.01 0.02 0.02 0.02 0.01 0.01 0.02 0.02 0.01 0.03 0.03 0.03\n",
      " 0.02 0.02 0.02 0.02 0.02 0.02 0.04 0.02 0.02 0.02 0.01 0.02 0.02 0.02\n",
      " 0.02 0.03 0.02 0.02 0.02 0.02 0.02 0.02]\n",
      "2024-06-29 13:30:39,235 - micro - MainProcess - INFO     Calculated median: 0.02 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.02\n",
      "2024-06-29 13:30:39,240 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-29 13:30:39,244 - micro - MainProcess - INFO     Calculated 95th percentile: 0.04 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 0.04\n",
      "2024-06-29 13:30:39,253 - micro - MainProcess - INFO     Calculated 99th percentile: 0.04 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 0.04\n",
      "2024-06-29 13:30:39,261 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.36 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.36\n",
      "2024-06-29 13:30:39,265 - micro - MainProcess - INFO     Result: (0.02, 0.0, 0.04, 0.04, 0.36) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.02, 0.0, 0.04, 0.04, 0.36)\n",
      "2024-06-29 13:30:39,268 - micro - MainProcess - INFO     Calculating statistics for data: [3.16, 9.54, 9.49, 9.65, 5.21, 4.92, 6.47, 6.38, 2.88, 4.25, 4.2, 3.98, 3.29, 4.66, 4.43, 4.52, 3.56, 5.06, 4.89, 5.12, 3.71, 3.01, 4.08, 4.15, 2.85, 7.44, 7.74, 7.2, 5.05, 3.86, 3.84, 3.91, 4.1, 4.09, 10.63, 4.17, 4.21, 3.78, 3.63, 4.35, 4.32, 4.04, 4.13, 6.41, 3.82, 4.21, 4.9, 4.44, 4.25, 5.74] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [3.16, 9.54, 9.49, 9.65, 5.21, 4.92, 6.47, 6.38, 2.88, 4.25, 4.2, 3.98, 3.29, 4.66, 4.43, 4.52, 3.56, 5.06, 4.89, 5.12, 3.71, 3.01, 4.08, 4.15, 2.85, 7.44, 7.74, 7.2, 5.05, 3.86, 3.84, 3.91, 4.1, 4.09, 10.63, 4.17, 4.21, 3.78, 3.63, 4.35, 4.32, 4.04, 4.13, 6.41, 3.82, 4.21, 4.9, 4.44, 4.25, 5.74]\n",
      "2024-06-29 13:30:39,275 - micro - MainProcess - INFO     Data converted to numpy array: [ 3.16  9.54  9.49  9.65  5.21  4.92  6.47  6.38  2.88  4.25  4.2   3.98\n",
      "  3.29  4.66  4.43  4.52  3.56  5.06  4.89  5.12  3.71  3.01  4.08  4.15\n",
      "  2.85  7.44  7.74  7.2   5.05  3.86  3.84  3.91  4.1   4.09 10.63  4.17\n",
      "  4.21  3.78  3.63  4.35  4.32  4.04  4.13  6.41  3.82  4.21  4.9   4.44\n",
      "  4.25  5.74] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [ 3.16  9.54  9.49  9.65  5.21  4.92  6.47  6.38  2.88  4.25  4.2   3.98\n",
      "  3.29  4.66  4.43  4.52  3.56  5.06  4.89  5.12  3.71  3.01  4.08  4.15\n",
      "  2.85  7.44  7.74  7.2   5.05  3.86  3.84  3.91  4.1   4.09 10.63  4.17\n",
      "  4.21  3.78  3.63  4.35  4.32  4.04  4.13  6.41  3.82  4.21  4.9   4.44\n",
      "  4.25  5.74]\n",
      "2024-06-29 13:30:39,279 - micro - MainProcess - INFO     Calculated median: 4.25 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 4.25\n",
      "2024-06-29 13:30:39,284 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 1.18 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 1.18\n",
      "2024-06-29 13:30:39,288 - micro - MainProcess - INFO     Calculated 95th percentile: 9.52 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 9.52\n",
      "2024-06-29 13:30:39,291 - micro - MainProcess - INFO     Calculated 99th percentile: 10.15 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 10.15\n",
      "2024-06-29 13:30:39,296 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.36 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.36\n",
      "2024-06-29 13:30:39,299 - micro - MainProcess - INFO     Result: (4.25, 1.18, 9.52, 10.15, 0.36) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (4.25, 1.18, 9.52, 10.15, 0.36)\n",
      "2024-06-29 13:30:39,302 - micro - MainProcess - INFO     Calculating statistics for data: [9.750699300000008, 9.702598899999998, 9.899967700000047, 9.823248800000044, 6.511075099999971, 8.49161939999999, 8.49168149999997, 8.397500999999977, 7.016990600000042, 7.124224200000015, 7.592838400000005, 9.059844999999996, 6.300760499999967, 8.905212199999994, 7.920472899999993, 7.467701000000034, 5.546355500000004, 6.755018500000006, 11.314878700000008, 10.584204999999997, 8.412597300000016, 7.387469699999997, 7.76785430000001, 6.031563100000028, 7.963663599999961, 8.16012440000003, 8.11550470000003, 8.15943390000001, 7.900277700000004, 7.8119102999999654, 5.950271799999996, 7.981589699999972, 7.681413599999985, 7.617659199999991, 7.527652199999977, 7.785452399999997, 6.666113999999993, 6.329974100000015, 6.8424547999999845, 8.54312490000001, 9.741795100000047, 5.913496899999984, 10.315681700000027, 6.5509442000000035, 10.717728300000033, 10.666689900000051, 10.596295499999997, 10.385106300000075, 6.486483500000077, 5.720544199999949] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [9.750699300000008, 9.702598899999998, 9.899967700000047, 9.823248800000044, 6.511075099999971, 8.49161939999999, 8.49168149999997, 8.397500999999977, 7.016990600000042, 7.124224200000015, 7.592838400000005, 9.059844999999996, 6.300760499999967, 8.905212199999994, 7.920472899999993, 7.467701000000034, 5.546355500000004, 6.755018500000006, 11.314878700000008, 10.584204999999997, 8.412597300000016, 7.387469699999997, 7.76785430000001, 6.031563100000028, 7.963663599999961, 8.16012440000003, 8.11550470000003, 8.15943390000001, 7.900277700000004, 7.8119102999999654, 5.950271799999996, 7.981589699999972, 7.681413599999985, 7.617659199999991, 7.527652199999977, 7.785452399999997, 6.666113999999993, 6.329974100000015, 6.8424547999999845, 8.54312490000001, 9.741795100000047, 5.913496899999984, 10.315681700000027, 6.5509442000000035, 10.717728300000033, 10.666689900000051, 10.596295499999997, 10.385106300000075, 6.486483500000077, 5.720544199999949]\n",
      "2024-06-29 13:30:39,305 - micro - MainProcess - INFO     Data converted to numpy array: [ 9.7506993  9.7025989  9.8999677  9.8232488  6.5110751  8.4916194\n",
      "  8.4916815  8.397501   7.0169906  7.1242242  7.5928384  9.059845\n",
      "  6.3007605  8.9052122  7.9204729  7.467701   5.5463555  6.7550185\n",
      " 11.3148787 10.584205   8.4125973  7.3874697  7.7678543  6.0315631\n",
      "  7.9636636  8.1601244  8.1155047  8.1594339  7.9002777  7.8119103\n",
      "  5.9502718  7.9815897  7.6814136  7.6176592  7.5276522  7.7854524\n",
      "  6.666114   6.3299741  6.8424548  8.5431249  9.7417951  5.9134969\n",
      " 10.3156817  6.5509442 10.7177283 10.6666899 10.5962955 10.3851063\n",
      "  6.4864835  5.7205442] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [ 9.7506993  9.7025989  9.8999677  9.8232488  6.5110751  8.4916194\n",
      "  8.4916815  8.397501   7.0169906  7.1242242  7.5928384  9.059845\n",
      "  6.3007605  8.9052122  7.9204729  7.467701   5.5463555  6.7550185\n",
      " 11.3148787 10.584205   8.4125973  7.3874697  7.7678543  6.0315631\n",
      "  7.9636636  8.1601244  8.1155047  8.1594339  7.9002777  7.8119103\n",
      "  5.9502718  7.9815897  7.6814136  7.6176592  7.5276522  7.7854524\n",
      "  6.666114   6.3299741  6.8424548  8.5431249  9.7417951  5.9134969\n",
      " 10.3156817  6.5509442 10.7177283 10.6666899 10.5962955 10.3851063\n",
      "  6.4864835  5.7205442]\n",
      "2024-06-29 13:30:39,310 - micro - MainProcess - INFO     Calculated median: 7.91 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 7.91\n",
      "2024-06-29 13:30:39,315 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 2.14 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 2.14\n",
      "2024-06-29 13:30:39,319 - micro - MainProcess - INFO     Calculated 95th percentile: 10.64 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 10.64\n",
      "2024-06-29 13:30:39,323 - micro - MainProcess - INFO     Calculated 99th percentile: 11.02 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 11.02\n",
      "2024-06-29 13:30:39,329 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.19 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.19\n",
      "2024-06-29 13:30:39,333 - micro - MainProcess - INFO     Result: (7.91, 2.14, 10.64, 11.02, 0.19) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (7.91, 2.14, 10.64, 11.02, 0.19)\n",
      "2024-06-29 13:30:39,335 - micro - MainProcess - INFO     Calculating statistics for data: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]\n",
      "2024-06-29 13:30:39,339 - micro - MainProcess - INFO     Data converted to numpy array: [500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
      " 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
      " 500 500 500 500 500 500 500 500 500 500 500 500 500 500] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
      " 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500 500\n",
      " 500 500 500 500 500 500 500 500 500 500 500 500 500 500]\n",
      "2024-06-29 13:30:39,343 - micro - MainProcess - INFO     Calculated median: 500.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 500.0\n",
      "2024-06-29 13:30:39,350 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-29 13:30:39,354 - micro - MainProcess - INFO     Calculated 95th percentile: 500.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 500.0\n",
      "2024-06-29 13:30:39,358 - micro - MainProcess - INFO     Calculated 99th percentile: 500.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 500.0\n",
      "2024-06-29 13:30:39,363 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-29 13:30:39,367 - micro - MainProcess - INFO     Result: (500.0, 0.0, 500.0, 500.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (500.0, 0.0, 500.0, 500.0, 0.0)\n",
      "2024-06-29 13:30:39,369 - micro - MainProcess - INFO     Calculating statistics for data: [68, 66, 77, 73, 73, 78, 67, 73, 79, 63, 73, 71, 68, 74, 74, 64, 70, 71, 65, 73, 66, 75, 75, 68, 71, 77, 74, 73, 70, 73, 74, 74, 67, 72, 74, 77, 70, 70, 65, 69, 63, 72, 67, 74, 69, 67, 69, 75, 71, 68] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [68, 66, 77, 73, 73, 78, 67, 73, 79, 63, 73, 71, 68, 74, 74, 64, 70, 71, 65, 73, 66, 75, 75, 68, 71, 77, 74, 73, 70, 73, 74, 74, 67, 72, 74, 77, 70, 70, 65, 69, 63, 72, 67, 74, 69, 67, 69, 75, 71, 68]\n",
      "2024-06-29 13:30:39,375 - micro - MainProcess - INFO     Data converted to numpy array: [68 66 77 73 73 78 67 73 79 63 73 71 68 74 74 64 70 71 65 73 66 75 75 68\n",
      " 71 77 74 73 70 73 74 74 67 72 74 77 70 70 65 69 63 72 67 74 69 67 69 75\n",
      " 71 68] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [68 66 77 73 73 78 67 73 79 63 73 71 68 74 74 64 70 71 65 73 66 75 75 68\n",
      " 71 77 74 73 70 73 74 74 67 72 74 77 70 70 65 69 63 72 67 74 69 67 69 75\n",
      " 71 68]\n",
      "2024-06-29 13:30:39,378 - micro - MainProcess - INFO     Calculated median: 71.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 71.0\n",
      "2024-06-29 13:30:39,386 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 6.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 6.0\n",
      "2024-06-29 13:30:39,391 - micro - MainProcess - INFO     Calculated 95th percentile: 77.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 77.0\n",
      "2024-06-29 13:30:39,395 - micro - MainProcess - INFO     Calculated 99th percentile: 78.51 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 78.51\n",
      "2024-06-29 13:30:39,400 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.06 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.06\n",
      "2024-06-29 13:30:39,403 - micro - MainProcess - INFO     Result: (71.0, 6.0, 77.0, 78.51, 0.06) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (71.0, 6.0, 77.0, 78.51, 0.06)\n",
      "2024-06-29 13:30:39,408 - micro - MainProcess - INFO     Calculating statistics for data: [0.02, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.02, 0.01, 0.02, 0.02, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.01, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [0.02, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.02, 0.01, 0.02, 0.02, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.01, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01]\n",
      "2024-06-29 13:30:39,413 - micro - MainProcess - INFO     Data converted to numpy array: [0.02 0.02 0.02 0.02 0.01 0.02 0.02 0.02 0.01 0.01 0.02 0.02 0.01 0.02\n",
      " 0.02 0.01 0.01 0.01 0.02 0.02 0.02 0.01 0.02 0.01 0.02 0.02 0.02 0.02\n",
      " 0.02 0.02 0.01 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.02 0.02 0.01\n",
      " 0.02 0.01 0.02 0.02 0.02 0.02 0.01 0.01] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [0.02 0.02 0.02 0.02 0.01 0.02 0.02 0.02 0.01 0.01 0.02 0.02 0.01 0.02\n",
      " 0.02 0.01 0.01 0.01 0.02 0.02 0.02 0.01 0.02 0.01 0.02 0.02 0.02 0.02\n",
      " 0.02 0.02 0.01 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.02 0.02 0.01\n",
      " 0.02 0.01 0.02 0.02 0.02 0.02 0.01 0.01]\n",
      "2024-06-29 13:30:39,418 - micro - MainProcess - INFO     Calculated median: 0.02 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.02\n",
      "2024-06-29 13:30:39,425 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.01 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.01\n",
      "2024-06-29 13:30:39,428 - micro - MainProcess - INFO     Calculated 95th percentile: 0.02 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 0.02\n",
      "2024-06-29 13:30:39,432 - micro - MainProcess - INFO     Calculated 99th percentile: 0.02 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 0.02\n",
      "2024-06-29 13:30:39,437 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.29 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.29\n",
      "2024-06-29 13:30:39,440 - micro - MainProcess - INFO     Result: (0.02, 0.01, 0.02, 0.02, 0.29) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.02, 0.01, 0.02, 0.02, 0.29)\n",
      "2024-06-29 13:30:39,443 - micro - MainProcess - INFO     Calculating statistics for data: [9.75, 9.7, 9.9, 9.82, 6.51, 8.49, 8.49, 8.4, 7.02, 7.12, 7.59, 9.06, 6.3, 8.91, 7.92, 7.47, 5.55, 6.76, 11.31, 10.58, 8.41, 7.39, 7.77, 6.03, 7.96, 8.16, 8.12, 8.16, 7.9, 7.81, 5.95, 7.98, 7.68, 7.62, 7.53, 7.79, 6.67, 6.33, 6.84, 8.54, 9.74, 5.91, 10.32, 6.55, 10.72, 10.67, 10.6, 10.39, 6.49, 5.72] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [9.75, 9.7, 9.9, 9.82, 6.51, 8.49, 8.49, 8.4, 7.02, 7.12, 7.59, 9.06, 6.3, 8.91, 7.92, 7.47, 5.55, 6.76, 11.31, 10.58, 8.41, 7.39, 7.77, 6.03, 7.96, 8.16, 8.12, 8.16, 7.9, 7.81, 5.95, 7.98, 7.68, 7.62, 7.53, 7.79, 6.67, 6.33, 6.84, 8.54, 9.74, 5.91, 10.32, 6.55, 10.72, 10.67, 10.6, 10.39, 6.49, 5.72]\n",
      "2024-06-29 13:30:39,454 - micro - MainProcess - INFO     Data converted to numpy array: [ 9.75  9.7   9.9   9.82  6.51  8.49  8.49  8.4   7.02  7.12  7.59  9.06\n",
      "  6.3   8.91  7.92  7.47  5.55  6.76 11.31 10.58  8.41  7.39  7.77  6.03\n",
      "  7.96  8.16  8.12  8.16  7.9   7.81  5.95  7.98  7.68  7.62  7.53  7.79\n",
      "  6.67  6.33  6.84  8.54  9.74  5.91 10.32  6.55 10.72 10.67 10.6  10.39\n",
      "  6.49  5.72] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [ 9.75  9.7   9.9   9.82  6.51  8.49  8.49  8.4   7.02  7.12  7.59  9.06\n",
      "  6.3   8.91  7.92  7.47  5.55  6.76 11.31 10.58  8.41  7.39  7.77  6.03\n",
      "  7.96  8.16  8.12  8.16  7.9   7.81  5.95  7.98  7.68  7.62  7.53  7.79\n",
      "  6.67  6.33  6.84  8.54  9.74  5.91 10.32  6.55 10.72 10.67 10.6  10.39\n",
      "  6.49  5.72]\n",
      "2024-06-29 13:30:39,459 - micro - MainProcess - INFO     Calculated median: 7.91 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 7.91\n",
      "2024-06-29 13:30:39,463 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 2.14 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 2.14\n",
      "2024-06-29 13:30:39,468 - micro - MainProcess - INFO     Calculated 95th percentile: 10.64 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 10.64\n",
      "2024-06-29 13:30:39,473 - micro - MainProcess - INFO     Calculated 99th percentile: 11.02 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 11.02\n",
      "2024-06-29 13:30:39,478 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.19 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.19\n",
      "2024-06-29 13:30:39,480 - micro - MainProcess - INFO     Result: (7.91, 2.14, 10.64, 11.02, 0.19) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (7.91, 2.14, 10.64, 11.02, 0.19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------+------------+-----------+--------------+-------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+---------+---------------------+---------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    Model_MaxTokens    | is_Streaming | Iterations |  Regions  | Average TTLT | Median TTLT | IQR TTLT | 95th Percentile TTLT | 99th Percentile TTLT | CV TTLT | Median Prompt Tokens | IQR Prompt Tokens | Median Completion Tokens | IQR Completion Tokens | 95th Percentile Completion Tokens | 99th Percentile Completion Tokens | CV Completion Tokens | Average TBT | Median TBT | IQR TBT | 95th Percentile TBT | 99th Percentile TBT | Average TTFT | Median TTFT | IQR TTFT | 95th Percentile TTFT | 99th Percentile TTFT | Error Rate | Error Types | Successful Runs | Unsuccessful Runs | Throttle Count | Throttle Rate |                                                                      Best Run                                                                      |                                                                      Worst Run                                                                      |\n",
      "+-----------------------+--------------+------------+-----------+--------------+-------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+---------+---------------------+---------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| gpt-4o-2024-05-13_250 |    False     |     50     | East US 2 |     4.95     |    4.25     |   1.18   |         9.52         |        10.15         |  0.36   |         71.0         |        6.0        |          250.0           |          0.0          |               250.0               |               250.0               |         0.0          |    0.02     |    0.02    |   0.0   |        0.04         |        0.04         |     4.95     |    4.25     |   1.18   |         9.52         |        10.15         |    0.0     |     []      |       50        |         0         |       0        |      0.0      | {\"ttlt\": 2.85, \"completion_tokens\": 250, \"prompt_tokens\": 74, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-29 14:28:58 EDT\"} | {\"ttlt\": 10.63, \"completion_tokens\": 250, \"prompt_tokens\": 75, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-29 14:29:15 EDT\"} |\n",
      "| gpt-4o-2024-05-13_500 |    False     |     50     | East US 2 |     8.09     |    7.91     |   2.14   |        10.64         |        11.02         |  0.19   |         71.0         |        6.0        |          500.0           |          0.0          |               500.0               |               500.0               |         0.0          |    0.02     |    0.02    |  0.01   |        0.02         |        0.02         |     8.09     |    7.91     |   2.14   |        10.64         |        11.02         |    0.0     |     []      |       50        |         0         |       0        |      0.0      | {\"ttlt\": 5.55, \"completion_tokens\": 500, \"prompt_tokens\": 70, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-29 14:28:59 EDT\"} | {\"ttlt\": 11.31, \"completion_tokens\": 500, \"prompt_tokens\": 65, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-29 14:29:05 EDT\"} |\n",
      "+-----------------------+--------------+------------+-----------+--------------+-------------+----------+----------------------+----------------------+---------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+---------+---------------------+---------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-2024-05-13_250': {'median_ttlt': 4.25,\n",
       "  'is_Streaming': False,\n",
       "  'regions': ['East US 2'],\n",
       "  'iqr_ttlt': 1.18,\n",
       "  'percentile_95_ttlt': 9.52,\n",
       "  'percentile_99_ttlt': 10.15,\n",
       "  'cv_ttlt': 0.36,\n",
       "  'median_completion_tokens': 250.0,\n",
       "  'iqr_completion_tokens': 0.0,\n",
       "  'percentile_95_completion_tokens': 250.0,\n",
       "  'percentile_99_completion_tokens': 250.0,\n",
       "  'cv_completion_tokens': 0.0,\n",
       "  'median_prompt_tokens': 71.0,\n",
       "  'iqr_prompt_tokens': 6.0,\n",
       "  'percentile_95_prompt_tokens': 77.0,\n",
       "  'percentile_99_prompt_tokens': 78.02,\n",
       "  'cv_prompt_tokens': 0.06,\n",
       "  'average_ttlt': 4.95,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 50,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 50,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 0.02,\n",
       "  'iqr_tbt': 0.0,\n",
       "  'percentile_95_tbt': 0.04,\n",
       "  'percentile_99_tbt': 0.04,\n",
       "  'cv_tbt': 0.36,\n",
       "  'average_tbt': 0.02,\n",
       "  'median_ttft': 4.25,\n",
       "  'iqr_ttft': 1.18,\n",
       "  'percentile_95_ttft': 9.52,\n",
       "  'percentile_99_ttft': 10.15,\n",
       "  'cv_ttft': 0.36,\n",
       "  'average_ttft': 4.95,\n",
       "  'best_run': {'ttlt': 2.85,\n",
       "   'completion_tokens': 250,\n",
       "   'prompt_tokens': 74,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-29 14:28:58 EDT'},\n",
       "  'worst_run': {'ttlt': 10.63,\n",
       "   'completion_tokens': 250,\n",
       "   'prompt_tokens': 75,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-29 14:29:15 EDT'}},\n",
       " 'gpt-4o-2024-05-13_500': {'median_ttlt': 7.91,\n",
       "  'is_Streaming': False,\n",
       "  'regions': ['East US 2'],\n",
       "  'iqr_ttlt': 2.14,\n",
       "  'percentile_95_ttlt': 10.64,\n",
       "  'percentile_99_ttlt': 11.02,\n",
       "  'cv_ttlt': 0.19,\n",
       "  'median_completion_tokens': 500.0,\n",
       "  'iqr_completion_tokens': 0.0,\n",
       "  'percentile_95_completion_tokens': 500.0,\n",
       "  'percentile_99_completion_tokens': 500.0,\n",
       "  'cv_completion_tokens': 0.0,\n",
       "  'median_prompt_tokens': 71.0,\n",
       "  'iqr_prompt_tokens': 6.0,\n",
       "  'percentile_95_prompt_tokens': 77.0,\n",
       "  'percentile_99_prompt_tokens': 78.51,\n",
       "  'cv_prompt_tokens': 0.06,\n",
       "  'average_ttlt': 8.09,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 50,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 50,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 0.02,\n",
       "  'iqr_tbt': 0.01,\n",
       "  'percentile_95_tbt': 0.02,\n",
       "  'percentile_99_tbt': 0.02,\n",
       "  'cv_tbt': 0.29,\n",
       "  'average_tbt': 0.02,\n",
       "  'median_ttft': 7.91,\n",
       "  'iqr_ttft': 2.14,\n",
       "  'percentile_95_ttft': 10.64,\n",
       "  'percentile_99_ttft': 11.02,\n",
       "  'cv_ttft': 0.19,\n",
       "  'average_ttft': 8.09,\n",
       "  'best_run': {'ttlt': 5.55,\n",
       "   'completion_tokens': 500,\n",
       "   'prompt_tokens': 70,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-29 14:28:59 EDT'},\n",
       "  'worst_run': {'ttlt': 11.31,\n",
       "   'completion_tokens': 500,\n",
       "   'prompt_tokens': 65,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-29 14:29:05 EDT'}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_non_streaming.calculate_and_show_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning, a subset of artificial intelligence, has revolutionized numerous fields by enabling systems to learn from data and improve their performance over time without being explicitly programmed. This technology leverages algorithms to parse data, learn from it, and make informed decisions. The core idea is to build models that can generalize from specific examples to broader applications.\\n\\nOne of the most significant impacts of machine learning is in the realm of data analysis. Traditional data analysis methods often require manual intervention and are limited by human capacity to'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 22:12:59,804 - micro - MainProcess - INFO     CPU usage: 12.7% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.7%\n",
      "2024-06-27 22:12:59,822 - micro - MainProcess - INFO     RAM usage: 94.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 94.3%\n",
      "2024-06-27 22:12:59,843 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:12:59,845 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:12:59.845862, (GMT): 2024-06-28 03:12:59.845862+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:12:59.845862, (GMT): 2024-06-28 03:12:59.845862+00:00\n",
      "2024-06-27 22:12:59,849 - micro - MainProcess - INFO     CPU usage: 6.9% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 6.9%\n",
      "2024-06-27 22:12:59,862 - micro - MainProcess - INFO     RAM usage: 94.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 94.3%\n",
      "2024-06-27 22:12:59,887 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:12:59,891 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:12:59.890828, (GMT): 2024-06-28 03:12:59.890828+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:12:59.890828, (GMT): 2024-06-28 03:12:59.890828+00:00\n",
      "2024-06-27 22:13:00,321 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 426.36, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 426.36, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:00,349 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 501.44, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 501.44, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:00,625 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 303.89, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 303.89, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:00,850 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' development', 'Token size': 1, 'Time taken (ms)': 224.24, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' development', 'Token size': 1, 'Time taken (ms)': 224.24, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:00,854 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 504.75, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 504.75, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:01,091 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' are', 'Token size': 1, 'Time taken (ms)': 242.19, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' are', 'Token size': 1, 'Time taken (ms)': 242.19, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:01,379 - micro - MainProcess - INFO     {'Count': 5, 'Content': ' be', 'Token size': 1, 'Time taken (ms)': 287.44, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': ' be', 'Token size': 1, 'Time taken (ms)': 287.44, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:01,386 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 532.28, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 532.28, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:01,655 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' computational', 'Token size': 1, 'Time taken (ms)': 275.03, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' computational', 'Token size': 1, 'Time taken (ms)': 275.03, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:01,897 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 511.32, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 511.32, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:01,901 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' programmed', 'Token size': 1, 'Time taken (ms)': 247.32, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' programmed', 'Token size': 1, 'Time taken (ms)': 247.32, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:02,176 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 274.24, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 274.24, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:02,372 - micro - MainProcess - INFO     {'Count': 5, 'Content': ' its', 'Token size': 1, 'Time taken (ms)': 474.96, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': ' its', 'Token size': 1, 'Time taken (ms)': 474.96, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:02,472 - micro - MainProcess - INFO     {'Count': 9, 'Content': ' used', 'Token size': 1, 'Time taken (ms)': 296.24, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': ' used', 'Token size': 1, 'Time taken (ms)': 296.24, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:02,785 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' labeled', 'Token size': 1, 'Time taken (ms)': 312.75, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' labeled', 'Token size': 1, 'Time taken (ms)': 312.75, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:02,834 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' identify', 'Token size': 1, 'Time taken (ms)': 49.27, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' identify', 'Token size': 1, 'Time taken (ms)': 49.27, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:02,851 - micro - MainProcess - INFO     {'Count': 12, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 16.68, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 16.68, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:02,878 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 2.98 seconds or 2982.86 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 2.98 seconds or 2982.86 milliseconds.\n",
      "2024-06-27 22:13:02,941 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:02,981 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 608.74, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 608.74, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:03,035 - micro - MainProcess - INFO     {'Count': 7, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 53.21, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 53.21, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:03,551 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' dataset', 'Token size': 1, 'Time taken (ms)': 517.66, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' dataset', 'Token size': 1, 'Time taken (ms)': 517.66, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:03,602 - micro - MainProcess - INFO     {'Count': 9, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 50.11, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 50.11, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:03,954 - micro - MainProcess - INFO     CPU usage: 20.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 20.5%\n",
      "2024-06-27 22:13:03,968 - micro - MainProcess - INFO     RAM usage: 94.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 94.3%\n",
      "2024-06-27 22:13:04,000 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:04,002 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:04.002970, (GMT): 2024-06-28 03:13:04.002970+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:04.002970, (GMT): 2024-06-28 03:13:04.002970+00:00\n",
      "2024-06-27 22:13:04,053 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 450.86, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 450.86, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:04,325 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 319.17, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 319.17, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:04,745 - micro - MainProcess - INFO     {'Count': 11, 'Content': '.\\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 692.99, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': '.\\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 692.99, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:04,841 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 516.4, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 516.4, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:04,896 - micro - MainProcess - INFO     {'Count': 3, 'Content': 'ized', 'Token size': 1, 'Time taken (ms)': 55.57, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': 'ized', 'Token size': 1, 'Time taken (ms)': 55.57, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:05,153 - micro - MainProcess - INFO     {'Count': 12, 'Content': ' present', 'Token size': 1, 'Time taken (ms)': 406.79, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ' present', 'Token size': 1, 'Time taken (ms)': 406.79, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:05,233 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 336.8, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 336.8, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:05,614 - micro - MainProcess - INFO     {'Count': 5, 'Content': ' technology', 'Token size': 1, 'Time taken (ms)': 381.09, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': ' technology', 'Token size': 1, 'Time taken (ms)': 381.09, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:05,710 - micro - MainProcess - INFO     {'Count': 13, 'Content': ' principal', 'Token size': 1, 'Time taken (ms)': 558.46, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 13, 'Content': ' principal', 'Token size': 1, 'Time taken (ms)': 558.46, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:05,758 - micro - MainProcess - INFO     {'Count': 14, 'Content': ' typical', 'Token size': 1, 'Time taken (ms)': 47.37, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 14, 'Content': ' typical', 'Token size': 1, 'Time taken (ms)': 47.37, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:06,011 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' concepts', 'Token size': 1, 'Time taken (ms)': 396.35, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' concepts', 'Token size': 1, 'Time taken (ms)': 396.35, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:06,174 - micro - MainProcess - INFO     {'Count': 15, 'Content': ' by', 'Token size': 1, 'Time taken (ms)': 415.61, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 15, 'Content': ' by', 'Token size': 1, 'Time taken (ms)': 415.61, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:06,509 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 498.96, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 498.96, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:06,594 - micro - MainProcess - INFO     {'Count': 16, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 419.81, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 16, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 419.81, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:06,882 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' during', 'Token size': 1, 'Time taken (ms)': 372.26, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' during', 'Token size': 1, 'Time taken (ms)': 372.26, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:07,078 - micro - MainProcess - INFO     {'Count': 17, 'Content': ' (', 'Token size': 1, 'Time taken (ms)': 484.7, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 17, 'Content': ' (', 'Token size': 1, 'Time taken (ms)': 484.7, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:07,128 - micro - MainProcess - INFO     {'Count': 18, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 49.3, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 18, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 49.3, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:07,377 - micro - MainProcess - INFO     {'Count': 9, 'Content': ' images', 'Token size': 1, 'Time taken (ms)': 495.59, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': ' images', 'Token size': 1, 'Time taken (ms)': 495.59, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:07,500 - micro - MainProcess - INFO     {'Count': 19, 'Content': ' industries', 'Token size': 1, 'Time taken (ms)': 372.92, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 19, 'Content': ' industries', 'Token size': 1, 'Time taken (ms)': 372.92, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:07,781 - micro - MainProcess - INFO     {'Count': 10, 'Content': 'abeled', 'Token size': 1, 'Time taken (ms)': 403.62, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': 'abeled', 'Token size': 1, 'Time taken (ms)': 403.62, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:07,935 - micro - MainProcess - INFO     {'Count': 20, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 434.28, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 20, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 434.28, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:08,179 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 397.73, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 397.73, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:08,424 - micro - MainProcess - INFO     {'Count': 21, 'Content': ' high', 'Token size': 1, 'Time taken (ms)': 488.67, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 21, 'Content': ' high', 'Token size': 1, 'Time taken (ms)': 488.67, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:08,470 - micro - MainProcess - INFO     {'Count': 22, 'Content': ' for', 'Token size': 1, 'Time taken (ms)': 46.77, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 22, 'Content': ' for', 'Token size': 1, 'Time taken (ms)': 46.77, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:08,556 - micro - MainProcess - INFO     {'Count': 12, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 377.06, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 377.06, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:08,589 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.58 seconds or 4583.76 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.58 seconds or 4583.76 milliseconds.\n",
      "2024-06-27 22:13:08,642 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:08,823 - micro - MainProcess - INFO     {'Count': 23, 'Content': ' management', 'Token size': 1, 'Time taken (ms)': 352.69, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 23, 'Content': ' management', 'Token size': 1, 'Time taken (ms)': 352.69, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:09,129 - micro - MainProcess - INFO     {'Count': 24, 'Content': '**', 'Token size': 1, 'Time taken (ms)': 305.58, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 24, 'Content': '**', 'Token size': 1, 'Time taken (ms)': 305.58, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:09,260 - micro - MainProcess - INFO     {'Count': 25, 'Content': ' based', 'Token size': 1, 'Time taken (ms)': 130.28, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 25, 'Content': ' based', 'Token size': 1, 'Time taken (ms)': 130.28, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:09,294 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.45 seconds or 9446.67 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.45 seconds or 9446.67 milliseconds.\n",
      "2024-06-27 22:13:09,456 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:09,648 - micro - MainProcess - INFO     CPU usage: 15.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 15.8%\n",
      "2024-06-27 22:13:09,688 - micro - MainProcess - INFO     RAM usage: 94.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 94.3%\n",
      "2024-06-27 22:13:09,709 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:09,712 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:09.712534, (GMT): 2024-06-28 03:13:09.712534+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:09.712534, (GMT): 2024-06-28 03:13:09.712534+00:00\n",
      "2024-06-27 22:13:10,050 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 331.96, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 331.96, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:10,461 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 411.97, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 411.97, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:10,467 - micro - MainProcess - INFO     CPU usage: 10.1% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 10.1%\n",
      "2024-06-27 22:13:10,480 - micro - MainProcess - INFO     RAM usage: 94.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 94.3%\n",
      "2024-06-27 22:13:10,504 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:10,508 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:10.508699, (GMT): 2024-06-28 03:13:10.508699+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:10.508699, (GMT): 2024-06-28 03:13:10.508699+00:00\n",
      "2024-06-27 22:13:10,812 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 300.73, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 300.73, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:10,893 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 431.34, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 431.34, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:11,241 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' technology', 'Token size': 1, 'Time taken (ms)': 347.81, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' technology', 'Token size': 1, 'Time taken (ms)': 347.81, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:11,244 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 433.01, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 433.01, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:11,636 - micro - MainProcess - INFO     {'Count': 5, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 394.4, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 394.4, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:11,698 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' development', 'Token size': 1, 'Time taken (ms)': 454.35, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' development', 'Token size': 1, 'Time taken (ms)': 454.35, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:12,175 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' that', 'Token size': 1, 'Time taken (ms)': 539.77, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' that', 'Token size': 1, 'Time taken (ms)': 539.77, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:12,236 - micro - MainProcess - INFO     {'Count': 4, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 536.79, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 536.79, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:12,524 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 348.29, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 348.29, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:12,680 - micro - MainProcess - INFO     {'Count': 5, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 444.38, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 444.38, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:12,974 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 450.98, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 450.98, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:13,089 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' begins', 'Token size': 1, 'Time taken (ms)': 408.81, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' begins', 'Token size': 1, 'Time taken (ms)': 408.81, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:13,366 - micro - MainProcess - INFO     {'Count': 9, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 390.86, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 390.86, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:13,371 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' without', 'Token size': 1, 'Time taken (ms)': 4.64, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' without', 'Token size': 1, 'Time taken (ms)': 4.64, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:13,516 - micro - MainProcess - INFO     {'Count': 7, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 427.19, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 427.19, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:13,788 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' tasks', 'Token size': 1, 'Time taken (ms)': 417.61, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' tasks', 'Token size': 1, 'Time taken (ms)': 417.61, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:13,970 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' applications', 'Token size': 1, 'Time taken (ms)': 453.46, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' applications', 'Token size': 1, 'Time taken (ms)': 453.46, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:14,073 - micro - MainProcess - INFO     {'Count': 12, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 285.21, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 285.21, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:14,102 - micro - MainProcess - INFO     {'Count': 13, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 29.07, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 13, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 29.07, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:14,105 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.39 seconds or 4387.32 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.39 seconds or 4387.32 milliseconds.\n",
      "2024-06-27 22:13:14,170 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:14,336 - micro - MainProcess - INFO     {'Count': 9, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 366.38, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 366.38, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:14,748 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' linear', 'Token size': 1, 'Time taken (ms)': 412.26, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' linear', 'Token size': 1, 'Time taken (ms)': 412.26, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:14,803 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' on', 'Token size': 1, 'Time taken (ms)': 54.87, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' on', 'Token size': 1, 'Time taken (ms)': 54.87, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:15,185 - micro - MainProcess - INFO     CPU usage: 17.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 17.6%\n",
      "2024-06-27 22:13:15,196 - micro - MainProcess - INFO     RAM usage: 94.3% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 94.3%\n",
      "2024-06-27 22:13:15,215 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:15,218 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:15.218357, (GMT): 2024-06-28 03:13:15.218357+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:15.218357, (GMT): 2024-06-28 03:13:15.218357+00:00\n",
      "2024-06-27 22:13:15,387 - micro - MainProcess - INFO     {'Count': 12, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 583.96, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 583.96, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:15,512 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 287.57, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 287.57, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:15,792 - micro - MainProcess - INFO     {'Count': 13, 'Content': 'uper', 'Token size': 1, 'Time taken (ms)': 404.99, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 13, 'Content': 'uper', 'Token size': 1, 'Time taken (ms)': 404.99, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:16,092 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 580.66, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 580.66, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:16,154 - micro - MainProcess - INFO     {'Count': 14, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 361.75, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 14, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 361.75, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:16,404 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 311.6, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 311.6, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:16,576 - micro - MainProcess - INFO     {'Count': 15, 'Content': ' decisions', 'Token size': 1, 'Time taken (ms)': 422.22, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 15, 'Content': ' decisions', 'Token size': 1, 'Time taken (ms)': 422.22, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:16,778 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' from', 'Token size': 1, 'Time taken (ms)': 374.03, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' from', 'Token size': 1, 'Time taken (ms)': 374.03, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:16,914 - micro - MainProcess - INFO     {'Count': 16, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 338.04, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 16, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 338.04, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:17,140 - micro - MainProcess - INFO     {'Count': 5, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 361.06, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 361.06, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:17,290 - micro - MainProcess - INFO     {'Count': 17, 'Content': '.\\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 375.96, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 17, 'Content': '.\\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 375.96, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:17,411 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 272.1, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 272.1, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:17,772 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 361.85, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 361.85, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:17,875 - micro - MainProcess - INFO     {'Count': 18, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 585.13, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 18, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 585.13, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:18,110 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' is', 'Token size': 1, 'Time taken (ms)': 337.19, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' is', 'Token size': 1, 'Time taken (ms)': 337.19, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:18,474 - micro - MainProcess - INFO     {'Count': 9, 'Content': ' improved', 'Token size': 1, 'Time taken (ms)': 363.16, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': ' improved', 'Token size': 1, 'Time taken (ms)': 363.16, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:18,768 - micro - MainProcess - INFO     {'Count': 10, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 294.47, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 294.47, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:18,889 - micro - MainProcess - INFO     {'Count': 19, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1013.82, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 19, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1013.82, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:19,030 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 261.68, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 261.68, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:19,059 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.83 seconds or 3834.88 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.83 seconds or 3834.88 milliseconds.\n",
      "2024-06-27 22:13:19,119 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:19,482 - micro - MainProcess - INFO     {'Count': 20, 'Content': ' experience', 'Token size': 1, 'Time taken (ms)': 592.45, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 20, 'Content': ' experience', 'Token size': 1, 'Time taken (ms)': 592.45, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:19,885 - micro - MainProcess - INFO     {'Count': 21, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 403.21, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 21, 'Content': ' data', 'Token size': 1, 'Time taken (ms)': 403.21, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:19,914 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.4 seconds or 9404.0 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.4 seconds or 9404.0 milliseconds.\n",
      "2024-06-27 22:13:19,967 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:20,120 - micro - MainProcess - INFO     CPU usage: 19.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.6%\n",
      "2024-06-27 22:13:20,131 - micro - MainProcess - INFO     RAM usage: 93.9% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 93.9%\n",
      "2024-06-27 22:13:20,154 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:20,156 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:20.156569, (GMT): 2024-06-28 03:13:20.156569+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 250 at (Local time): 2024-06-27 22:13:20.156569, (GMT): 2024-06-28 03:13:20.156569+00:00\n",
      "2024-06-27 22:13:20,551 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 392.78, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 392.78, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:20,989 - micro - MainProcess - INFO     CPU usage: 25.8% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 25.8%\n",
      "2024-06-27 22:13:21,007 - micro - MainProcess - INFO     RAM usage: 93.0% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 93.0%\n",
      "2024-06-27 22:13:21,040 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:21,043 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:21.043877, (GMT): 2024-06-28 03:13:21.043877+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:21.043877, (GMT): 2024-06-28 03:13:21.043877+00:00\n",
      "2024-06-27 22:13:21,277 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 726.39, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 726.39, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:21,366 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 317.02, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 317.02, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:21,694 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 416.04, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 416.04, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:21,859 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 493.14, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 493.14, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:22,051 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' programming', 'Token size': 1, 'Time taken (ms)': 356.89, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' programming', 'Token size': 1, 'Time taken (ms)': 356.89, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:22,341 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' development', 'Token size': 1, 'Time taken (ms)': 482.6, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' development', 'Token size': 1, 'Time taken (ms)': 482.6, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:22,587 - micro - MainProcess - INFO     {'Count': 5, 'Content': '.\\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 536.17, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': '.\\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 536.17, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:22,806 - micro - MainProcess - INFO     {'Count': 4, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 465.19, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 465.19, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:23,028 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 440.75, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 440.75, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:23,342 - micro - MainProcess - INFO     {'Count': 5, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 535.92, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 535.92, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:23,369 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' applications', 'Token size': 1, 'Time taken (ms)': 342.37, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' applications', 'Token size': 1, 'Time taken (ms)': 342.37, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:23,394 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' goal', 'Token size': 1, 'Time taken (ms)': 51.9, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' goal', 'Token size': 1, 'Time taken (ms)': 51.9, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:23,736 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 342.07, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 342.07, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:23,828 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 457.52, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 457.52, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:24,231 - micro - MainProcess - INFO     {'Count': 8, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 494.4, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 494.4, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:24,282 - micro - MainProcess - INFO     {'Count': 9, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 454.93, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 454.93, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:24,770 - micro - MainProcess - INFO     {'Count': 9, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 539.06, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 539.06, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:24,777 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' neural', 'Token size': 1, 'Time taken (ms)': 494.73, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' neural', 'Token size': 1, 'Time taken (ms)': 494.73, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:24,824 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 53.94, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 53.94, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:25,124 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' technique', 'Token size': 1, 'Time taken (ms)': 346.27, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' technique', 'Token size': 1, 'Time taken (ms)': 346.27, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:25,155 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.0 seconds or 4996.1 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 5.0 seconds or 4996.1 milliseconds.\n",
      "2024-06-27 22:13:25,208 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:25,211 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' applications', 'Token size': 1, 'Time taken (ms)': 387.96, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' applications', 'Token size': 1, 'Time taken (ms)': 387.96, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:25,563 - micro - MainProcess - INFO     {'Count': 12, 'Content': ' images', 'Token size': 1, 'Time taken (ms)': 351.89, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ' images', 'Token size': 1, 'Time taken (ms)': 351.89, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:25,965 - micro - MainProcess - INFO     {'Count': 13, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 401.4, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 13, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 401.4, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:26,306 - micro - MainProcess - INFO     {'Count': 14, 'Content': ' find', 'Token size': 1, 'Time taken (ms)': 341.56, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 14, 'Content': ' find', 'Token size': 1, 'Time taken (ms)': 341.56, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:26,720 - micro - MainProcess - INFO     {'Count': 15, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 413.75, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 15, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 413.75, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:26,773 - micro - MainProcess - INFO     {'Count': 16, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 52.74, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 16, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 52.74, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:27,088 - micro - MainProcess - INFO     {'Count': 17, 'Content': 'ity', 'Token size': 1, 'Time taken (ms)': 314.15, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 17, 'Content': 'ity', 'Token size': 1, 'Time taken (ms)': 314.15, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:27,422 - micro - MainProcess - INFO     {'Count': 18, 'Content': ' interpret', 'Token size': 1, 'Time taken (ms)': 335.55, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 18, 'Content': ' interpret', 'Token size': 1, 'Time taken (ms)': 335.55, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:27,873 - micro - MainProcess - INFO     {'Count': 19, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 450.04, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 19, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 450.04, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:28,372 - micro - MainProcess - INFO     {'Count': 20, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 499.2, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 20, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 499.2, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:28,823 - micro - MainProcess - INFO     {'Count': 21, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 450.56, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 21, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 450.56, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:29,263 - micro - MainProcess - INFO     {'Count': 22, 'Content': ' faces', 'Token size': 1, 'Time taken (ms)': 440.59, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 22, 'Content': ' faces', 'Token size': 1, 'Time taken (ms)': 440.59, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:29,466 - micro - MainProcess - INFO     {'Count': 23, 'Content': ' can', 'Token size': 1, 'Time taken (ms)': 202.79, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 23, 'Content': ' can', 'Token size': 1, 'Time taken (ms)': 202.79, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:29,495 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.45 seconds or 8446.44 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 8.45 seconds or 8446.44 milliseconds.\n",
      "2024-06-27 22:13:29,552 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:30,566 - micro - MainProcess - INFO     CPU usage: 19.6% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 19.6%\n",
      "2024-06-27 22:13:30,579 - micro - MainProcess - INFO     RAM usage: 86.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.8%\n",
      "2024-06-27 22:13:30,607 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:30,609 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:30.609616, (GMT): 2024-06-28 03:13:30.609616+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:30.609616, (GMT): 2024-06-28 03:13:30.609616+00:00\n",
      "2024-06-27 22:13:30,973 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 361.33, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 361.33, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:31,376 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 403.12, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 403.12, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:31,803 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 427.35, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 427.35, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:32,278 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 474.25, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 474.25, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:32,710 - micro - MainProcess - INFO     {'Count': 5, 'Content': ' its', 'Token size': 1, 'Time taken (ms)': 433.11, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': ' its', 'Token size': 1, 'Time taken (ms)': 433.11, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:33,101 - micro - MainProcess - INFO     {'Count': 6, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 390.57, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 390.57, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:33,491 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 389.97, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 389.97, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:33,965 - micro - MainProcess - INFO     {'Count': 8, 'Content': ' are', 'Token size': 1, 'Time taken (ms)': 473.68, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ' are', 'Token size': 1, 'Time taken (ms)': 473.68, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:34,436 - micro - MainProcess - INFO     {'Count': 9, 'Content': ':', 'Token size': 1, 'Time taken (ms)': 470.58, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': ':', 'Token size': 1, 'Time taken (ms)': 470.58, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:34,958 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' that', 'Token size': 1, 'Time taken (ms)': 522.65, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' that', 'Token size': 1, 'Time taken (ms)': 522.65, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:35,006 - micro - MainProcess - INFO     {'Count': 11, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 48.02, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 48.02, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:35,636 - micro - MainProcess - INFO     {'Count': 12, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 629.48, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 629.48, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:36,074 - micro - MainProcess - INFO     {'Count': 13, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 437.75, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 13, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 437.75, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:36,498 - micro - MainProcess - INFO     {'Count': 14, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 424.44, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 14, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 424.44, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:36,983 - micro - MainProcess - INFO     {'Count': 15, 'Content': ' are', 'Token size': 1, 'Time taken (ms)': 484.96, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 15, 'Content': ' are', 'Token size': 1, 'Time taken (ms)': 484.96, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:37,419 - micro - MainProcess - INFO     {'Count': 16, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 435.81, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 16, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 435.81, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:37,955 - micro - MainProcess - INFO     {'Count': 17, 'Content': ' over', 'Token size': 1, 'Time taken (ms)': 536.26, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 17, 'Content': ' over', 'Token size': 1, 'Time taken (ms)': 536.26, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:38,496 - micro - MainProcess - INFO     {'Count': 18, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 540.63, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 18, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 540.63, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:39,107 - micro - MainProcess - INFO     {'Count': 19, 'Content': ' outbreaks', 'Token size': 1, 'Time taken (ms)': 611.61, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 19, 'Content': ' outbreaks', 'Token size': 1, 'Time taken (ms)': 611.61, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:39,561 - micro - MainProcess - INFO     {'Count': 20, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 454.29, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 20, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 454.29, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:39,968 - micro - MainProcess - INFO     {'Count': 21, 'Content': 'ic', 'Token size': 1, 'Time taken (ms)': 406.24, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 21, 'Content': 'ic', 'Token size': 1, 'Time taken (ms)': 406.24, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:40,115 - micro - MainProcess - INFO     {'Count': 22, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 147.57, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 22, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 147.57, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:40,148 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.54 seconds or 9535.73 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 9.54 seconds or 9535.73 milliseconds.\n",
      "2024-06-27 22:13:40,257 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n",
      "2024-06-27 22:13:41,264 - micro - MainProcess - INFO     CPU usage: 12.5% (utils.py:log_system_info:233)\n",
      "INFO:micro:CPU usage: 12.5%\n",
      "2024-06-27 22:13:41,277 - micro - MainProcess - INFO     RAM usage: 86.8% (utils.py:log_system_info:235)\n",
      "INFO:micro:RAM usage: 86.8%\n",
      "2024-06-27 22:13:41,304 - micro - MainProcess - INFO     Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90. (utils.py:detect_model_encoding:81)\n",
      "INFO:micro:Fuzzy match found: 'gpt-4o-2024-05-13' matched to 'gpt-4' with a score of 90.\n",
      "2024-06-27 22:13:41,307 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:41.307750, (GMT): 2024-06-28 03:13:41.307750+00:00 (latencytest.py:make_call:722)\n",
      "INFO:micro:Starting call to model gpt-4o-2024-05-13 with max tokens 500 at (Local time): 2024-06-27 22:13:41.307750, (GMT): 2024-06-28 03:13:41.307750+00:00\n",
      "2024-06-27 22:13:41,673 - micro - MainProcess - INFO     {'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 363.75, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 363.75, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:42,212 - micro - MainProcess - INFO     {'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 538.71, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 2, 'Content': 'Machine', 'Token size': 1, 'Time taken (ms)': 538.71, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:42,769 - micro - MainProcess - INFO     {'Count': 3, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 557.01, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 3, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 557.01, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:43,351 - micro - MainProcess - INFO     {'Count': 4, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 582.38, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 4, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 582.38, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:43,871 - micro - MainProcess - INFO     {'Count': 5, 'Content': ' its', 'Token size': 1, 'Time taken (ms)': 519.65, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 5, 'Content': ' its', 'Token size': 1, 'Time taken (ms)': 519.65, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:45,497 - micro - MainProcess - INFO     {'Count': 6, 'Content': ' is', 'Token size': 1, 'Time taken (ms)': 1625.05, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 6, 'Content': ' is', 'Token size': 1, 'Time taken (ms)': 1625.05, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:46,098 - micro - MainProcess - INFO     {'Count': 7, 'Content': ' steps', 'Token size': 1, 'Time taken (ms)': 602.36, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 7, 'Content': ' steps', 'Token size': 1, 'Time taken (ms)': 602.36, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:46,146 - micro - MainProcess - INFO     {'Count': 8, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 47.98, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 8, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 47.98, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:46,656 - micro - MainProcess - INFO     {'Count': 9, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 509.07, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 9, 'Content': ' learning', 'Token size': 1, 'Time taken (ms)': 509.07, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:47,204 - micro - MainProcess - INFO     {'Count': 10, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 547.95, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 10, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 547.95, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:47,665 - micro - MainProcess - INFO     {'Count': 11, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 461.1, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 11, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 461.1, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:48,250 - micro - MainProcess - INFO     {'Count': 12, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 585.54, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 12, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 585.54, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:48,813 - micro - MainProcess - INFO     {'Count': 13, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 562.89, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 13, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 562.89, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:49,387 - micro - MainProcess - INFO     {'Count': 14, 'Content': ' trees', 'Token size': 1, 'Time taken (ms)': 574.39, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 14, 'Content': ' trees', 'Token size': 1, 'Time taken (ms)': 574.39, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:49,449 - micro - MainProcess - INFO     {'Count': 15, 'Content': ' During', 'Token size': 1, 'Time taken (ms)': 61.47, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 15, 'Content': ' During', 'Token size': 1, 'Time taken (ms)': 61.47, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:49,927 - micro - MainProcess - INFO     {'Count': 16, 'Content': ' patterns', 'Token size': 1, 'Time taken (ms)': 478.2, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 16, 'Content': ' patterns', 'Token size': 1, 'Time taken (ms)': 478.2, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:50,500 - micro - MainProcess - INFO     {'Count': 17, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 572.39, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 17, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 572.39, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:51,125 - micro - MainProcess - INFO     {'Count': 18, 'Content': ' dataset', 'Token size': 1, 'Time taken (ms)': 625.19, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 18, 'Content': ' dataset', 'Token size': 1, 'Time taken (ms)': 625.19, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:51,175 - micro - MainProcess - INFO     {'Count': 19, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 49.44, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 19, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 49.44, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:51,929 - micro - MainProcess - INFO     {'Count': 20, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 754.54, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 20, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 754.54, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:52,388 - micro - MainProcess - INFO     {'Count': 21, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 459.93, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 21, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 459.93, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:52,901 - micro - MainProcess - INFO     {'Count': 22, 'Content': ' categorized', 'Token size': 1, 'Time taken (ms)': 512.98, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 22, 'Content': ' categorized', 'Token size': 1, 'Time taken (ms)': 512.98, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:53,025 - micro - MainProcess - INFO     {'Count': 23, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 123.13, 'utilization': 'NA'} (latencytest.py:make_call:751)\n",
      "INFO:micro:{'Count': 23, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 123.13, 'utilization': 'NA'}\n",
      "2024-06-27 22:13:53,055 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 11.75 seconds or 11745.29 milliseconds. (latencytest.py:make_call:768)\n",
      "INFO:micro:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 11.75 seconds or 11745.29 milliseconds.\n",
      "2024-06-27 22:13:53,125 - micro - MainProcess - INFO     Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US (utils.py:get_local_time_in_azure_region:148)\n",
      "INFO:micro:Target region None not found, assuming local deployment. \n",
      "                    Machine location during test: Berwyn, US\n"
     ]
    }
   ],
   "source": [
    "await benchmark_streaming.run_latency_benchmark_bulk(deployment_names=[DEPLOYMENT_ID], max_tokens_list=[500,250], iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-2024-05-13_100': {'ttlt_successful': [2.6457064],\n",
       "  'ttlt_unsuccessful': [],\n",
       "  'tbt': [336.87],\n",
       "  'ttft': [0.95],\n",
       "  'regions': [None],\n",
       "  'number_of_iterations': 1,\n",
       "  'completion_tokens': [6],\n",
       "  'prompt_tokens': [1008],\n",
       "  'errors': {'count': 0, 'codes': []},\n",
       "  'best_run': {'ttlt': 2.6457064,\n",
       "   'completion_tokens': 6,\n",
       "   'prompt_tokens': 1008,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:12:01 '},\n",
       "  'worst_run': {'ttlt': 2.6457064,\n",
       "   'completion_tokens': 6,\n",
       "   'prompt_tokens': 1008,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:12:01 '}},\n",
       " 'gpt-4o-2024-05-13_250': {'ttlt_successful': [2.9828582000000097,\n",
       "   4.583761499999994,\n",
       "   4.387315100000009,\n",
       "   3.834885,\n",
       "   4.996095199999999],\n",
       "  'ttlt_unsuccessful': [],\n",
       "  'tbt': [229.94, 384.68, 337.66, 351.78, 457.21],\n",
       "  'ttft': [8.53, 6.38, 6.64, 5.75, 7.86],\n",
       "  'regions': [None, None, None, None, None],\n",
       "  'number_of_iterations': 5,\n",
       "  'completion_tokens': [12, 12, 13, 11, 11],\n",
       "  'prompt_tokens': [50, 50, 50, 50, 50],\n",
       "  'errors': {'count': 0, 'codes': []},\n",
       "  'best_run': {'ttlt': 2.9828582000000097,\n",
       "   'completion_tokens': 12,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:02 '},\n",
       "  'worst_run': {'ttlt': 4.996095199999999,\n",
       "   'completion_tokens': 11,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:25 '}},\n",
       " 'gpt-4o-2024-05-13_500': {'ttlt_successful': [9.446672300000003,\n",
       "   9.403997600000011,\n",
       "   8.446443699999989,\n",
       "   9.535728200000008,\n",
       "   11.745287300000001],\n",
       "  'ttlt_unsuccessful': [],\n",
       "  'tbt': [371.25, 453.65, 368.2, 435.35, 515.97],\n",
       "  'ttft': [10.03, 6.01, 6.34, 7.23, 7.28],\n",
       "  'regions': [None, None, None, None, None],\n",
       "  'number_of_iterations': 5,\n",
       "  'completion_tokens': [25, 21, 23, 22, 23],\n",
       "  'prompt_tokens': [50, 50, 50, 50, 50],\n",
       "  'errors': {'count': 0, 'codes': []},\n",
       "  'best_run': {'ttlt': 8.446443699999989,\n",
       "   'completion_tokens': 23,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:29 '},\n",
       "  'worst_run': {'ttlt': 11.745287300000001,\n",
       "   'completion_tokens': 23,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:53 '}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_streaming.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 22:15:06,079 - micro - MainProcess - INFO     Calculating statistics for data: [2.6457064] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [2.6457064]\n",
      "2024-06-27 22:15:06,088 - micro - MainProcess - INFO     Data converted to numpy array: [2.6457064] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [2.6457064]\n",
      "2024-06-27 22:15:06,095 - micro - MainProcess - INFO     Calculated median: 2.6457064 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 2.6457064\n",
      "2024-06-27 22:15:06,102 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 22:15:06,107 - micro - MainProcess - INFO     Calculated 95th percentile: 2.6457064 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 2.6457064\n",
      "2024-06-27 22:15:06,108 - micro - MainProcess - INFO     Calculated 99th percentile: 2.6457064 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 2.6457064\n",
      "2024-06-27 22:15:06,117 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 22:15:06,119 - micro - MainProcess - INFO     Result: (2.6457064, 0.0, 2.6457064, 2.6457064, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (2.6457064, 0.0, 2.6457064, 2.6457064, 0.0)\n",
      "2024-06-27 22:15:06,121 - micro - MainProcess - INFO     Calculating statistics for data: [6] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [6]\n",
      "2024-06-27 22:15:06,123 - micro - MainProcess - INFO     Data converted to numpy array: [6] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [6]\n",
      "2024-06-27 22:15:06,125 - micro - MainProcess - INFO     Calculated median: 6.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 6.0\n",
      "2024-06-27 22:15:06,129 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 22:15:06,132 - micro - MainProcess - INFO     Calculated 95th percentile: 6.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 6.0\n",
      "2024-06-27 22:15:06,133 - micro - MainProcess - INFO     Calculated 99th percentile: 6.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 6.0\n",
      "2024-06-27 22:15:06,136 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 22:15:06,137 - micro - MainProcess - INFO     Result: (6.0, 0.0, 6.0, 6.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (6.0, 0.0, 6.0, 6.0, 0.0)\n",
      "2024-06-27 22:15:06,139 - micro - MainProcess - INFO     Calculating statistics for data: [1008] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [1008]\n",
      "2024-06-27 22:15:06,142 - micro - MainProcess - INFO     Data converted to numpy array: [1008] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [1008]\n",
      "2024-06-27 22:15:06,144 - micro - MainProcess - INFO     Calculated median: 1008.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 1008.0\n",
      "2024-06-27 22:15:06,147 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 22:15:06,149 - micro - MainProcess - INFO     Calculated 95th percentile: 1008.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 1008.0\n",
      "2024-06-27 22:15:06,152 - micro - MainProcess - INFO     Calculated 99th percentile: 1008.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 1008.0\n",
      "2024-06-27 22:15:06,155 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 22:15:06,160 - micro - MainProcess - INFO     Result: (1008.0, 0.0, 1008.0, 1008.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (1008.0, 0.0, 1008.0, 1008.0, 0.0)\n",
      "2024-06-27 22:15:06,176 - micro - MainProcess - INFO     Calculating statistics for data: [336.87] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [336.87]\n",
      "2024-06-27 22:15:06,179 - micro - MainProcess - INFO     Data converted to numpy array: [336.87] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [336.87]\n",
      "2024-06-27 22:15:06,183 - micro - MainProcess - INFO     Calculated median: 336.87 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 336.87\n",
      "2024-06-27 22:15:06,187 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 22:15:06,190 - micro - MainProcess - INFO     Calculated 95th percentile: 336.87 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 336.87\n",
      "2024-06-27 22:15:06,196 - micro - MainProcess - INFO     Calculated 99th percentile: 336.87 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 336.87\n",
      "2024-06-27 22:15:06,200 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 22:15:06,204 - micro - MainProcess - INFO     Result: (336.87, 0.0, 336.87, 336.87, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (336.87, 0.0, 336.87, 336.87, 0.0)\n",
      "2024-06-27 22:15:06,206 - micro - MainProcess - INFO     Calculating statistics for data: [0.95] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [0.95]\n",
      "2024-06-27 22:15:06,208 - micro - MainProcess - INFO     Data converted to numpy array: [0.95] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [0.95]\n",
      "2024-06-27 22:15:06,212 - micro - MainProcess - INFO     Calculated median: 0.95 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.95\n",
      "2024-06-27 22:15:06,215 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 22:15:06,219 - micro - MainProcess - INFO     Calculated 95th percentile: 0.95 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 0.95\n",
      "2024-06-27 22:15:06,221 - micro - MainProcess - INFO     Calculated 99th percentile: 0.95 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 0.95\n",
      "2024-06-27 22:15:06,226 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 22:15:06,229 - micro - MainProcess - INFO     Result: (0.95, 0.0, 0.95, 0.95, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.95, 0.0, 0.95, 0.95, 0.0)\n",
      "2024-06-27 22:15:06,233 - micro - MainProcess - INFO     Calculating statistics for data: [2.9828582000000097, 4.583761499999994, 4.387315100000009, 3.834885, 4.996095199999999] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [2.9828582000000097, 4.583761499999994, 4.387315100000009, 3.834885, 4.996095199999999]\n",
      "2024-06-27 22:15:06,237 - micro - MainProcess - INFO     Data converted to numpy array: [2.9828582 4.5837615 4.3873151 3.834885  4.9960952] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [2.9828582 4.5837615 4.3873151 3.834885  4.9960952]\n",
      "2024-06-27 22:15:06,241 - micro - MainProcess - INFO     Calculated median: 4.387315100000009 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 4.387315100000009\n",
      "2024-06-27 22:15:06,245 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.7488764999999944 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.7488764999999944\n",
      "2024-06-27 22:15:06,249 - micro - MainProcess - INFO     Calculated 95th percentile: 4.913628459999998 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 4.913628459999998\n",
      "2024-06-27 22:15:06,251 - micro - MainProcess - INFO     Calculated 99th percentile: 4.979601851999999 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 4.979601851999999\n",
      "2024-06-27 22:15:06,254 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.16741315722934866 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.16741315722934866\n",
      "2024-06-27 22:15:06,256 - micro - MainProcess - INFO     Result: (4.387315100000009, 0.7488764999999944, 4.913628459999998, 4.979601851999999, 0.16741315722934866) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (4.387315100000009, 0.7488764999999944, 4.913628459999998, 4.979601851999999, 0.16741315722934866)\n",
      "2024-06-27 22:15:06,258 - micro - MainProcess - INFO     Calculating statistics for data: [12, 12, 13, 11, 11] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [12, 12, 13, 11, 11]\n",
      "2024-06-27 22:15:06,261 - micro - MainProcess - INFO     Data converted to numpy array: [12 12 13 11 11] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [12 12 13 11 11]\n",
      "2024-06-27 22:15:06,263 - micro - MainProcess - INFO     Calculated median: 12.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 12.0\n",
      "2024-06-27 22:15:06,267 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 1.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 1.0\n",
      "2024-06-27 22:15:06,270 - micro - MainProcess - INFO     Calculated 95th percentile: 12.8 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 12.8\n",
      "2024-06-27 22:15:06,273 - micro - MainProcess - INFO     Calculated 99th percentile: 12.96 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 12.96\n",
      "2024-06-27 22:15:06,278 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.06341792180972781 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.06341792180972781\n",
      "2024-06-27 22:15:06,280 - micro - MainProcess - INFO     Result: (12.0, 1.0, 12.8, 12.96, 0.06341792180972781) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (12.0, 1.0, 12.8, 12.96, 0.06341792180972781)\n",
      "2024-06-27 22:15:06,283 - micro - MainProcess - INFO     Calculating statistics for data: [50, 50, 50, 50, 50] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [50, 50, 50, 50, 50]\n",
      "2024-06-27 22:15:06,287 - micro - MainProcess - INFO     Data converted to numpy array: [50 50 50 50 50] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [50 50 50 50 50]\n",
      "2024-06-27 22:15:06,291 - micro - MainProcess - INFO     Calculated median: 50.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 50.0\n",
      "2024-06-27 22:15:06,295 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 22:15:06,300 - micro - MainProcess - INFO     Calculated 95th percentile: 50.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 50.0\n",
      "2024-06-27 22:15:06,303 - micro - MainProcess - INFO     Calculated 99th percentile: 50.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 50.0\n",
      "2024-06-27 22:15:06,307 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 22:15:06,310 - micro - MainProcess - INFO     Result: (50.0, 0.0, 50.0, 50.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (50.0, 0.0, 50.0, 50.0, 0.0)\n",
      "2024-06-27 22:15:06,313 - micro - MainProcess - INFO     Calculating statistics for data: [229.94, 384.68, 337.66, 351.78, 457.21] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [229.94, 384.68, 337.66, 351.78, 457.21]\n",
      "2024-06-27 22:15:06,316 - micro - MainProcess - INFO     Data converted to numpy array: [229.94 384.68 337.66 351.78 457.21] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [229.94 384.68 337.66 351.78 457.21]\n",
      "2024-06-27 22:15:06,322 - micro - MainProcess - INFO     Calculated median: 351.78 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 351.78\n",
      "2024-06-27 22:15:06,326 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 47.01999999999998 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 47.01999999999998\n",
      "2024-06-27 22:15:06,330 - micro - MainProcess - INFO     Calculated 95th percentile: 442.70399999999995 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 442.70399999999995\n",
      "2024-06-27 22:15:06,333 - micro - MainProcess - INFO     Calculated 99th percentile: 454.30879999999996 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 454.30879999999996\n",
      "2024-06-27 22:15:06,339 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.20954226591739528 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.20954226591739528\n",
      "2024-06-27 22:15:06,341 - micro - MainProcess - INFO     Result: (351.78, 47.01999999999998, 442.70399999999995, 454.30879999999996, 0.20954226591739528) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (351.78, 47.01999999999998, 442.70399999999995, 454.30879999999996, 0.20954226591739528)\n",
      "2024-06-27 22:15:06,346 - micro - MainProcess - INFO     Calculating statistics for data: [8.53, 6.38, 6.64, 5.75, 7.86] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [8.53, 6.38, 6.64, 5.75, 7.86]\n",
      "2024-06-27 22:15:06,350 - micro - MainProcess - INFO     Data converted to numpy array: [8.53 6.38 6.64 5.75 7.86] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [8.53 6.38 6.64 5.75 7.86]\n",
      "2024-06-27 22:15:06,353 - micro - MainProcess - INFO     Calculated median: 6.64 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 6.64\n",
      "2024-06-27 22:15:06,357 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 1.4800000000000004 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 1.4800000000000004\n",
      "2024-06-27 22:15:06,360 - micro - MainProcess - INFO     Calculated 95th percentile: 8.395999999999999 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 8.395999999999999\n",
      "2024-06-27 22:15:06,363 - micro - MainProcess - INFO     Calculated 99th percentile: 8.5032 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 8.5032\n",
      "2024-06-27 22:15:06,368 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.14435073684741606 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.14435073684741606\n",
      "2024-06-27 22:15:06,371 - micro - MainProcess - INFO     Result: (6.64, 1.4800000000000004, 8.395999999999999, 8.5032, 0.14435073684741606) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (6.64, 1.4800000000000004, 8.395999999999999, 8.5032, 0.14435073684741606)\n",
      "2024-06-27 22:15:06,379 - micro - MainProcess - INFO     Calculating statistics for data: [9.446672300000003, 9.403997600000011, 8.446443699999989, 9.535728200000008, 11.745287300000001] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [9.446672300000003, 9.403997600000011, 8.446443699999989, 9.535728200000008, 11.745287300000001]\n",
      "2024-06-27 22:15:06,383 - micro - MainProcess - INFO     Data converted to numpy array: [ 9.4466723  9.4039976  8.4464437  9.5357282 11.7452873] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [ 9.4466723  9.4039976  8.4464437  9.5357282 11.7452873]\n",
      "2024-06-27 22:15:06,387 - micro - MainProcess - INFO     Calculated median: 9.446672300000003 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 9.446672300000003\n",
      "2024-06-27 22:15:06,392 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.13173059999999737 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.13173059999999737\n",
      "2024-06-27 22:15:06,395 - micro - MainProcess - INFO     Calculated 95th percentile: 11.303375480000001 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 11.303375480000001\n",
      "2024-06-27 22:15:06,401 - micro - MainProcess - INFO     Calculated 99th percentile: 11.656904936000002 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 11.656904936000002\n",
      "2024-06-27 22:15:06,406 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.11211162708448552 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.11211162708448552\n",
      "2024-06-27 22:15:06,409 - micro - MainProcess - INFO     Result: (9.446672300000003, 0.13173059999999737, 11.303375480000001, 11.656904936000002, 0.11211162708448552) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (9.446672300000003, 0.13173059999999737, 11.303375480000001, 11.656904936000002, 0.11211162708448552)\n",
      "2024-06-27 22:15:06,412 - micro - MainProcess - INFO     Calculating statistics for data: [25, 21, 23, 22, 23] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [25, 21, 23, 22, 23]\n",
      "2024-06-27 22:15:06,415 - micro - MainProcess - INFO     Data converted to numpy array: [25 21 23 22 23] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [25 21 23 22 23]\n",
      "2024-06-27 22:15:06,418 - micro - MainProcess - INFO     Calculated median: 23.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 23.0\n",
      "2024-06-27 22:15:06,422 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 1.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 1.0\n",
      "2024-06-27 22:15:06,425 - micro - MainProcess - INFO     Calculated 95th percentile: 24.6 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 24.6\n",
      "2024-06-27 22:15:06,430 - micro - MainProcess - INFO     Calculated 99th percentile: 24.92 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 24.92\n",
      "2024-06-27 22:15:06,434 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.05818639983079649 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.05818639983079649\n",
      "2024-06-27 22:15:06,437 - micro - MainProcess - INFO     Result: (23.0, 1.0, 24.6, 24.92, 0.05818639983079649) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (23.0, 1.0, 24.6, 24.92, 0.05818639983079649)\n",
      "2024-06-27 22:15:06,441 - micro - MainProcess - INFO     Calculating statistics for data: [50, 50, 50, 50, 50] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [50, 50, 50, 50, 50]\n",
      "2024-06-27 22:15:06,444 - micro - MainProcess - INFO     Data converted to numpy array: [50 50 50 50 50] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [50 50 50 50 50]\n",
      "2024-06-27 22:15:06,449 - micro - MainProcess - INFO     Calculated median: 50.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 50.0\n",
      "2024-06-27 22:15:06,453 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 22:15:06,459 - micro - MainProcess - INFO     Calculated 95th percentile: 50.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 50.0\n",
      "2024-06-27 22:15:06,463 - micro - MainProcess - INFO     Calculated 99th percentile: 50.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 50.0\n",
      "2024-06-27 22:15:06,468 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 22:15:06,471 - micro - MainProcess - INFO     Result: (50.0, 0.0, 50.0, 50.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (50.0, 0.0, 50.0, 50.0, 0.0)\n",
      "2024-06-27 22:15:06,474 - micro - MainProcess - INFO     Calculating statistics for data: [371.25, 453.65, 368.2, 435.35, 515.97] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [371.25, 453.65, 368.2, 435.35, 515.97]\n",
      "2024-06-27 22:15:06,478 - micro - MainProcess - INFO     Data converted to numpy array: [371.25 453.65 368.2  435.35 515.97] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [371.25 453.65 368.2  435.35 515.97]\n",
      "2024-06-27 22:15:06,482 - micro - MainProcess - INFO     Calculated median: 435.35 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 435.35\n",
      "2024-06-27 22:15:06,488 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 82.39999999999998 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 82.39999999999998\n",
      "2024-06-27 22:15:06,492 - micro - MainProcess - INFO     Calculated 95th percentile: 503.50600000000003 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 503.50600000000003\n",
      "2024-06-27 22:15:06,495 - micro - MainProcess - INFO     Calculated 99th percentile: 513.4772 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 513.4772\n",
      "2024-06-27 22:15:06,499 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.12874059805977855 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.12874059805977855\n",
      "2024-06-27 22:15:06,502 - micro - MainProcess - INFO     Result: (435.35, 82.39999999999998, 503.50600000000003, 513.4772, 0.12874059805977855) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (435.35, 82.39999999999998, 503.50600000000003, 513.4772, 0.12874059805977855)\n",
      "2024-06-27 22:15:06,505 - micro - MainProcess - INFO     Calculating statistics for data: [10.03, 6.01, 6.34, 7.23, 7.28] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [10.03, 6.01, 6.34, 7.23, 7.28]\n",
      "2024-06-27 22:15:06,508 - micro - MainProcess - INFO     Data converted to numpy array: [10.03  6.01  6.34  7.23  7.28] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [10.03  6.01  6.34  7.23  7.28]\n",
      "2024-06-27 22:15:06,511 - micro - MainProcess - INFO     Calculated median: 7.23 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 7.23\n",
      "2024-06-27 22:15:06,516 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.9400000000000004 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.9400000000000004\n",
      "2024-06-27 22:15:06,526 - micro - MainProcess - INFO     Calculated 95th percentile: 9.479999999999999 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 9.479999999999999\n",
      "2024-06-27 22:15:06,530 - micro - MainProcess - INFO     Calculated 99th percentile: 9.92 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 9.92\n",
      "2024-06-27 22:15:06,562 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.1918089484018454 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.1918089484018454\n",
      "2024-06-27 22:15:06,568 - micro - MainProcess - INFO     Result: (7.23, 0.9400000000000004, 9.479999999999999, 9.92, 0.1918089484018454) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (7.23, 0.9400000000000004, 9.479999999999999, 9.92, 0.1918089484018454)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------+---------+-------------------+-------------------+---------------------+----------------------+----------------------+---------------------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+-------------------+---------------------+---------------------+-------------------+-------------+--------------------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    Model_MaxTokens    | Iterations | Regions |   Average TTLT    |    Median TTLT    |      IQR TTLT       | 95th Percentile TTLT | 99th Percentile TTLT |       CV TTLT       | Median Prompt Tokens | IQR Prompt Tokens | Median Completion Tokens | IQR Completion Tokens | 95th Percentile Completion Tokens | 99th Percentile Completion Tokens | CV Completion Tokens | Average TBT | Median TBT |      IQR TBT      | 95th Percentile TBT | 99th Percentile TBT |   Average TTFT    | Median TTFT |      IQR TTFT      | 95th Percentile TTFT | 99th Percentile TTFT | Error Rate | Error Types | Successful Runs | Unsuccessful Runs | Throttle Count | Throttle Rate |                                                                        Best Run                                                                        |                                                                       Worst Run                                                                        |\n",
      "+-----------------------+------------+---------+-------------------+-------------------+---------------------+----------------------+----------------------+---------------------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+-------------------+---------------------+---------------------+-------------------+-------------+--------------------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| gpt-4o-2024-05-13_100 |     1      |   N/A   |     2.6457064     |     2.6457064     |         0.0         |      2.6457064       |      2.6457064       |         0.0         |        1008.0        |        0.0        |           6.0            |          0.0          |                6.0                |                6.0                |         0.0          |   336.87    |   336.87   |        0.0        |       336.87        |       336.87        |       0.95        |    0.95     |        0.0         |         0.95         |         0.95         |    0.0     |     []      |        1        |         0         |       0        |      0.0      |     {\"ttlt\": 2.6457064, \"completion_tokens\": 6, \"prompt_tokens\": 1008, \"region\": null, \"utilization\": \"N/A\", \"local_time\": \"2024-06-27 22:12:01 \"}     |     {\"ttlt\": 2.6457064, \"completion_tokens\": 6, \"prompt_tokens\": 1008, \"region\": null, \"utilization\": \"N/A\", \"local_time\": \"2024-06-27 22:12:01 \"}     |\n",
      "| gpt-4o-2024-05-13_250 |     5      |   N/A   | 4.156983000000002 | 4.387315100000009 | 0.7488764999999944  |  4.913628459999998   |  4.979601851999999   | 0.16741315722934866 |         50.0         |        0.0        |           12.0           |          1.0          |               12.8                |               12.96               | 0.06341792180972781  |   352.254   |   351.78   | 47.01999999999998 | 442.70399999999995  | 454.30879999999996  | 7.032000000000001 |    6.64     | 1.4800000000000004 |  8.395999999999999   |        8.5032        |    0.0     |     []      |        5        |         0         |       0        |      0.0      | {\"ttlt\": 2.9828582000000097, \"completion_tokens\": 12, \"prompt_tokens\": 50, \"region\": null, \"utilization\": \"N/A\", \"local_time\": \"2024-06-27 22:13:02 \"} | {\"ttlt\": 4.996095199999999, \"completion_tokens\": 11, \"prompt_tokens\": 50, \"region\": null, \"utilization\": \"N/A\", \"local_time\": \"2024-06-27 22:13:25 \"}  |\n",
      "| gpt-4o-2024-05-13_500 |     5      |   N/A   | 9.715625820000003 | 9.446672300000003 | 0.13173059999999737 |  11.303375480000001  |  11.656904936000002  | 0.11211162708448552 |         50.0         |        0.0        |           23.0           |          1.0          |               24.6                |               24.92               | 0.05818639983079649  |   428.884   |   435.35   | 82.39999999999998 | 503.50600000000003  |      513.4772       |       7.378       |    7.23     | 0.9400000000000004 |  9.479999999999999   |         9.92         |    0.0     |     []      |        5        |         0         |       0        |      0.0      | {\"ttlt\": 8.446443699999989, \"completion_tokens\": 23, \"prompt_tokens\": 50, \"region\": null, \"utilization\": \"N/A\", \"local_time\": \"2024-06-27 22:13:29 \"}  | {\"ttlt\": 11.745287300000001, \"completion_tokens\": 23, \"prompt_tokens\": 50, \"region\": null, \"utilization\": \"N/A\", \"local_time\": \"2024-06-27 22:13:53 \"} |\n",
      "+-----------------------+------------+---------+-------------------+-------------------+---------------------+----------------------+----------------------+---------------------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+-------------+------------+-------------------+---------------------+---------------------+-------------------+-------------+--------------------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-2024-05-13_100': {'median_ttlt': 2.6457064,\n",
       "  'regions': [None],\n",
       "  'iqr_ttlt': 0.0,\n",
       "  'percentile_95_ttlt': 2.6457064,\n",
       "  'percentile_99_ttlt': 2.6457064,\n",
       "  'cv_ttlt': 0.0,\n",
       "  'median_completion_tokens': 6.0,\n",
       "  'iqr_completion_tokens': 0.0,\n",
       "  'percentile_95_completion_tokens': 6.0,\n",
       "  'percentile_99_completion_tokens': 6.0,\n",
       "  'cv_completion_tokens': 0.0,\n",
       "  'median_prompt_tokens': 1008.0,\n",
       "  'iqr_prompt_tokens': 0.0,\n",
       "  'percentile_95_prompt_tokens': 1008.0,\n",
       "  'percentile_99_prompt_tokens': 1008.0,\n",
       "  'cv_prompt_tokens': 0.0,\n",
       "  'average_ttlt': 2.6457064,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 1,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 1,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 336.87,\n",
       "  'iqr_tbt': 0.0,\n",
       "  'percentile_95_tbt': 336.87,\n",
       "  'percentile_99_tbt': 336.87,\n",
       "  'cv_tbt': 0.0,\n",
       "  'average_tbt': 336.87,\n",
       "  'median_ttft': 0.95,\n",
       "  'iqr_ttft': 0.0,\n",
       "  'percentile_95_ttft': 0.95,\n",
       "  'percentile_99_ttft': 0.95,\n",
       "  'cv_ttft': 0.0,\n",
       "  'average_ttft': 0.95,\n",
       "  'best_run': {'ttlt': 2.6457064,\n",
       "   'completion_tokens': 6,\n",
       "   'prompt_tokens': 1008,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:12:01 '},\n",
       "  'worst_run': {'ttlt': 2.6457064,\n",
       "   'completion_tokens': 6,\n",
       "   'prompt_tokens': 1008,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:12:01 '}},\n",
       " 'gpt-4o-2024-05-13_250': {'median_ttlt': 4.387315100000009,\n",
       "  'regions': [None],\n",
       "  'iqr_ttlt': 0.7488764999999944,\n",
       "  'percentile_95_ttlt': 4.913628459999998,\n",
       "  'percentile_99_ttlt': 4.979601851999999,\n",
       "  'cv_ttlt': 0.16741315722934866,\n",
       "  'median_completion_tokens': 12.0,\n",
       "  'iqr_completion_tokens': 1.0,\n",
       "  'percentile_95_completion_tokens': 12.8,\n",
       "  'percentile_99_completion_tokens': 12.96,\n",
       "  'cv_completion_tokens': 0.06341792180972781,\n",
       "  'median_prompt_tokens': 50.0,\n",
       "  'iqr_prompt_tokens': 0.0,\n",
       "  'percentile_95_prompt_tokens': 50.0,\n",
       "  'percentile_99_prompt_tokens': 50.0,\n",
       "  'cv_prompt_tokens': 0.0,\n",
       "  'average_ttlt': 4.156983000000002,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 5,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 5,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 351.78,\n",
       "  'iqr_tbt': 47.01999999999998,\n",
       "  'percentile_95_tbt': 442.70399999999995,\n",
       "  'percentile_99_tbt': 454.30879999999996,\n",
       "  'cv_tbt': 0.20954226591739528,\n",
       "  'average_tbt': 352.254,\n",
       "  'median_ttft': 6.64,\n",
       "  'iqr_ttft': 1.4800000000000004,\n",
       "  'percentile_95_ttft': 8.395999999999999,\n",
       "  'percentile_99_ttft': 8.5032,\n",
       "  'cv_ttft': 0.14435073684741606,\n",
       "  'average_ttft': 7.032000000000001,\n",
       "  'best_run': {'ttlt': 2.9828582000000097,\n",
       "   'completion_tokens': 12,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:02 '},\n",
       "  'worst_run': {'ttlt': 4.996095199999999,\n",
       "   'completion_tokens': 11,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:25 '}},\n",
       " 'gpt-4o-2024-05-13_500': {'median_ttlt': 9.446672300000003,\n",
       "  'regions': [None],\n",
       "  'iqr_ttlt': 0.13173059999999737,\n",
       "  'percentile_95_ttlt': 11.303375480000001,\n",
       "  'percentile_99_ttlt': 11.656904936000002,\n",
       "  'cv_ttlt': 0.11211162708448552,\n",
       "  'median_completion_tokens': 23.0,\n",
       "  'iqr_completion_tokens': 1.0,\n",
       "  'percentile_95_completion_tokens': 24.6,\n",
       "  'percentile_99_completion_tokens': 24.92,\n",
       "  'cv_completion_tokens': 0.05818639983079649,\n",
       "  'median_prompt_tokens': 50.0,\n",
       "  'iqr_prompt_tokens': 0.0,\n",
       "  'percentile_95_prompt_tokens': 50.0,\n",
       "  'percentile_99_prompt_tokens': 50.0,\n",
       "  'cv_prompt_tokens': 0.0,\n",
       "  'average_ttlt': 9.715625820000003,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 5,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 5,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 435.35,\n",
       "  'iqr_tbt': 82.39999999999998,\n",
       "  'percentile_95_tbt': 503.50600000000003,\n",
       "  'percentile_99_tbt': 513.4772,\n",
       "  'cv_tbt': 0.12874059805977855,\n",
       "  'average_tbt': 428.884,\n",
       "  'median_ttft': 7.23,\n",
       "  'iqr_ttft': 0.9400000000000004,\n",
       "  'percentile_95_ttft': 9.479999999999999,\n",
       "  'percentile_99_ttft': 9.92,\n",
       "  'cv_ttft': 0.1918089484018454,\n",
       "  'average_ttft': 7.378,\n",
       "  'best_run': {'ttlt': 8.446443699999989,\n",
       "   'completion_tokens': 23,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:29 '},\n",
       "  'worst_run': {'ttlt': 11.745287300000001,\n",
       "   'completion_tokens': 23,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': None,\n",
       "   'utilization': 'N/A',\n",
       "   'local_time': '2024-06-27 22:13:53 '}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_streaming.calculate_and_show_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_nonstreaming = AzureOpenAIBenchmarkNonStreaming(api_key=OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=DEPLOYMENT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 11:44:43,273 - micro - MainProcess - INFO     As no context was provided, 1000 tokens were added as average workloads. (latencytest.py:make_call:602)\n",
      "INFO:micro:As no context was provided, 1000 tokens were added as average workloads.\n",
      "2024-06-27 11:44:43,384 - micro - MainProcess - INFO     Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 1000 (latencytest.py:make_call:625)\n",
      "INFO:micro:Initiating call for Model: gpt-4o-2024-05-13, Max Tokens: 1000\n",
      "2024-06-27 11:45:14,868 - micro - MainProcess - INFO     Succesful Run - Time taken: 31.48 seconds. (latencytest.py:make_call:648)\n",
      "INFO:micro:Succesful Run - Time taken: 31.48 seconds.\n"
     ]
    }
   ],
   "source": [
    "await benchmark_nonstreaming.make_call(deployment_name=DEPLOYMENT_ID, max_tokens=1000)\n",
    "await benchmark_nonstreaming.run_latency_benchmark_bulk(deployment_names=[DEPLOYMENT_ID], max_tokens_list=[1000,500,250], iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-2024-05-13_1000': {'ttlt_successful': [31.47952219999999,\n",
       "   18.452650199999994,\n",
       "   23.416592199999968,\n",
       "   45.44973810000005,\n",
       "   14.786188600000003,\n",
       "   26.359097200000008],\n",
       "  'ttlt_unsuccessful': [],\n",
       "  'tbt': [0.03147952219999999,\n",
       "   0.018452650199999993,\n",
       "   0.023416592199999968,\n",
       "   0.045449738100000046,\n",
       "   0.014786188600000003,\n",
       "   0.026359097200000008],\n",
       "  'ttft': [None, None, None, None, None, None],\n",
       "  'regions': ['East US 2',\n",
       "   'East US 2',\n",
       "   'East US 2',\n",
       "   'East US 2',\n",
       "   'East US 2',\n",
       "   'East US 2'],\n",
       "  'number_of_iterations': 6,\n",
       "  'completion_tokens': [1000, 1000, 1000, 1000, 1000, 1000],\n",
       "  'prompt_tokens': [986, 52, 50, 52, 50, 52],\n",
       "  'errors': {'count': 0, 'codes': []},\n",
       "  'best_run': {'ttlt': 14.786188600000003,\n",
       "   'completion_tokens': 1000,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:48 EDT'},\n",
       "  'worst_run': {'ttlt': 45.44973810000005,\n",
       "   'completion_tokens': 1000,\n",
       "   'prompt_tokens': 52,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:32 EDT'}},\n",
       " 'gpt-4o-2024-05-13_250': {'ttlt_successful': [6.5350243999999975,\n",
       "   7.10248039999999,\n",
       "   8.431264599999963,\n",
       "   2.9964409000000387,\n",
       "   7.201789399999996],\n",
       "  'ttlt_unsuccessful': [],\n",
       "  'tbt': [0.02614009759999999,\n",
       "   0.02840992159999996,\n",
       "   0.033725058399999855,\n",
       "   0.011985763600000155,\n",
       "   0.028807157599999984],\n",
       "  'ttft': [None, None, None, None, None],\n",
       "  'regions': ['East US 2', 'East US 2', 'East US 2', 'East US 2', 'East US 2'],\n",
       "  'number_of_iterations': 5,\n",
       "  'completion_tokens': [250, 250, 250, 250, 250],\n",
       "  'prompt_tokens': [51, 51, 49, 49, 51],\n",
       "  'errors': {'count': 0, 'codes': []},\n",
       "  'best_run': {'ttlt': 2.9964409000000387,\n",
       "   'completion_tokens': 250,\n",
       "   'prompt_tokens': 49,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:46:31 EDT'},\n",
       "  'worst_run': {'ttlt': 8.431264599999963,\n",
       "   'completion_tokens': 250,\n",
       "   'prompt_tokens': 49,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:46:27 EDT'}},\n",
       " 'gpt-4o-2024-05-13_500': {'ttlt_successful': [13.453411500000016,\n",
       "   10.463891999999987,\n",
       "   17.72546410000001,\n",
       "   20.02827719999999,\n",
       "   7.849451299999998],\n",
       "  'ttlt_unsuccessful': [],\n",
       "  'tbt': [0.02690682300000003,\n",
       "   0.020927783999999974,\n",
       "   0.03545092820000002,\n",
       "   0.04005655439999998,\n",
       "   0.0156989026],\n",
       "  'ttft': [None, None, None, None, None],\n",
       "  'regions': ['East US 2', 'East US 2', 'East US 2', 'East US 2', 'East US 2'],\n",
       "  'number_of_iterations': 5,\n",
       "  'completion_tokens': [500, 500, 500, 500, 500],\n",
       "  'prompt_tokens': [49, 51, 51, 51, 51],\n",
       "  'errors': {'count': 0, 'codes': []},\n",
       "  'best_run': {'ttlt': 7.849451299999998,\n",
       "   'completion_tokens': 500,\n",
       "   'prompt_tokens': 51,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:16 EDT'},\n",
       "  'worst_run': {'ttlt': 20.02827719999999,\n",
       "   'completion_tokens': 500,\n",
       "   'prompt_tokens': 51,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:07 EDT'}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_nonstreaming.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 11:48:53,157 - micro - MainProcess - INFO     Calculating statistics for data: [31.47952219999999, 18.452650199999994, 23.416592199999968, 45.44973810000005, 14.786188600000003, 26.359097200000008] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [31.47952219999999, 18.452650199999994, 23.416592199999968, 45.44973810000005, 14.786188600000003, 26.359097200000008]\n",
      "2024-06-27 11:48:53,164 - micro - MainProcess - INFO     Data converted to numpy array: [31.4795222 18.4526502 23.4165922 45.4497381 14.7861886 26.3590972] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [31.4795222 18.4526502 23.4165922 45.4497381 14.7861886 26.3590972]\n",
      "2024-06-27 11:48:53,167 - micro - MainProcess - INFO     Calculated median: 24.887844699999988 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 24.887844699999988\n",
      "2024-06-27 11:48:53,172 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 10.505780250000008 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 10.505780250000008\n",
      "2024-06-27 11:48:53,175 - micro - MainProcess - INFO     Calculated 95th percentile: 41.95718412500003 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 41.95718412500003\n",
      "2024-06-27 11:48:53,177 - micro - MainProcess - INFO     Calculated 99th percentile: 44.75122730500005 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 44.75122730500005\n",
      "2024-06-27 11:48:53,179 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.37364087930274653 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.37364087930274653\n",
      "2024-06-27 11:48:53,182 - micro - MainProcess - INFO     Result: (24.887844699999988, 10.505780250000008, 41.95718412500003, 44.75122730500005, 0.37364087930274653) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (24.887844699999988, 10.505780250000008, 41.95718412500003, 44.75122730500005, 0.37364087930274653)\n",
      "2024-06-27 11:48:53,184 - micro - MainProcess - INFO     Calculating statistics for data: [1000, 1000, 1000, 1000, 1000, 1000] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [1000, 1000, 1000, 1000, 1000, 1000]\n",
      "2024-06-27 11:48:53,186 - micro - MainProcess - INFO     Data converted to numpy array: [1000 1000 1000 1000 1000 1000] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [1000 1000 1000 1000 1000 1000]\n",
      "2024-06-27 11:48:53,188 - micro - MainProcess - INFO     Calculated median: 1000.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 1000.0\n",
      "2024-06-27 11:48:53,191 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 11:48:53,193 - micro - MainProcess - INFO     Calculated 95th percentile: 1000.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 1000.0\n",
      "2024-06-27 11:48:53,195 - micro - MainProcess - INFO     Calculated 99th percentile: 1000.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 1000.0\n",
      "2024-06-27 11:48:53,198 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 11:48:53,200 - micro - MainProcess - INFO     Result: (1000.0, 0.0, 1000.0, 1000.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (1000.0, 0.0, 1000.0, 1000.0, 0.0)\n",
      "2024-06-27 11:48:53,204 - micro - MainProcess - INFO     Calculating statistics for data: [986, 52, 50, 52, 50, 52] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [986, 52, 50, 52, 50, 52]\n",
      "2024-06-27 11:48:53,207 - micro - MainProcess - INFO     Data converted to numpy array: [986  52  50  52  50  52] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [986  52  50  52  50  52]\n",
      "2024-06-27 11:48:53,209 - micro - MainProcess - INFO     Calculated median: 52.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 52.0\n",
      "2024-06-27 11:48:53,212 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 1.5 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 1.5\n",
      "2024-06-27 11:48:53,215 - micro - MainProcess - INFO     Calculated 95th percentile: 752.5 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 752.5\n",
      "2024-06-27 11:48:53,218 - micro - MainProcess - INFO     Calculated 99th percentile: 939.3000000000002 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 939.3000000000002\n",
      "2024-06-27 11:48:53,222 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 1.6829977732662775 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 1.6829977732662775\n",
      "2024-06-27 11:48:53,223 - micro - MainProcess - INFO     Result: (52.0, 1.5, 752.5, 939.3000000000002, 1.6829977732662775) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (52.0, 1.5, 752.5, 939.3000000000002, 1.6829977732662775)\n",
      "2024-06-27 11:48:53,226 - micro - MainProcess - INFO     Calculating statistics for data: [0.03147952219999999, 0.018452650199999993, 0.023416592199999968, 0.045449738100000046, 0.014786188600000003, 0.026359097200000008] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [0.03147952219999999, 0.018452650199999993, 0.023416592199999968, 0.045449738100000046, 0.014786188600000003, 0.026359097200000008]\n",
      "2024-06-27 11:48:53,228 - micro - MainProcess - INFO     Data converted to numpy array: [0.03147952 0.01845265 0.02341659 0.04544974 0.01478619 0.0263591 ] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [0.03147952 0.01845265 0.02341659 0.04544974 0.01478619 0.0263591 ]\n",
      "2024-06-27 11:48:53,232 - micro - MainProcess - INFO     Calculated median: 0.024887844699999988 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.024887844699999988\n",
      "2024-06-27 11:48:53,236 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.01050578025000001 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.01050578025000001\n",
      "2024-06-27 11:48:53,241 - micro - MainProcess - INFO     Calculated 95th percentile: 0.04195718412500003 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 0.04195718412500003\n",
      "2024-06-27 11:48:53,244 - micro - MainProcess - INFO     Calculated 99th percentile: 0.044751227305000044 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 0.044751227305000044\n",
      "2024-06-27 11:48:53,251 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.3736408793027465 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.3736408793027465\n",
      "2024-06-27 11:48:53,252 - micro - MainProcess - INFO     Result: (0.024887844699999988, 0.01050578025000001, 0.04195718412500003, 0.044751227305000044, 0.3736408793027465) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.024887844699999988, 0.01050578025000001, 0.04195718412500003, 0.044751227305000044, 0.3736408793027465)\n",
      "2024-06-27 11:48:53,255 - micro - MainProcess - INFO     Calculating statistics for data: [6.5350243999999975, 7.10248039999999, 8.431264599999963, 2.9964409000000387, 7.201789399999996] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [6.5350243999999975, 7.10248039999999, 8.431264599999963, 2.9964409000000387, 7.201789399999996]\n",
      "2024-06-27 11:48:53,258 - micro - MainProcess - INFO     Data converted to numpy array: [6.5350244 7.1024804 8.4312646 2.9964409 7.2017894] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [6.5350244 7.1024804 8.4312646 2.9964409 7.2017894]\n",
      "2024-06-27 11:48:53,261 - micro - MainProcess - INFO     Calculated median: 7.10248039999999 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 7.10248039999999\n",
      "2024-06-27 11:48:53,265 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.666764999999998 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.666764999999998\n",
      "2024-06-27 11:48:53,269 - micro - MainProcess - INFO     Calculated 95th percentile: 8.18536955999997 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 8.18536955999997\n",
      "2024-06-27 11:48:53,272 - micro - MainProcess - INFO     Calculated 99th percentile: 8.382085591999964 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 8.382085591999964\n",
      "2024-06-27 11:48:53,277 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.2844681870071126 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.2844681870071126\n",
      "2024-06-27 11:48:53,279 - micro - MainProcess - INFO     Result: (7.10248039999999, 0.666764999999998, 8.18536955999997, 8.382085591999964, 0.2844681870071126) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (7.10248039999999, 0.666764999999998, 8.18536955999997, 8.382085591999964, 0.2844681870071126)\n",
      "2024-06-27 11:48:53,281 - micro - MainProcess - INFO     Calculating statistics for data: [250, 250, 250, 250, 250] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [250, 250, 250, 250, 250]\n",
      "2024-06-27 11:48:53,284 - micro - MainProcess - INFO     Data converted to numpy array: [250 250 250 250 250] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [250 250 250 250 250]\n",
      "2024-06-27 11:48:53,286 - micro - MainProcess - INFO     Calculated median: 250.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 250.0\n",
      "2024-06-27 11:48:53,288 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 11:48:53,294 - micro - MainProcess - INFO     Calculated 95th percentile: 250.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 250.0\n",
      "2024-06-27 11:48:53,298 - micro - MainProcess - INFO     Calculated 99th percentile: 250.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 250.0\n",
      "2024-06-27 11:48:53,301 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 11:48:53,304 - micro - MainProcess - INFO     Result: (250.0, 0.0, 250.0, 250.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (250.0, 0.0, 250.0, 250.0, 0.0)\n",
      "2024-06-27 11:48:53,305 - micro - MainProcess - INFO     Calculating statistics for data: [51, 51, 49, 49, 51] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [51, 51, 49, 49, 51]\n",
      "2024-06-27 11:48:53,309 - micro - MainProcess - INFO     Data converted to numpy array: [51 51 49 49 51] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [51 51 49 49 51]\n",
      "2024-06-27 11:48:53,315 - micro - MainProcess - INFO     Calculated median: 51.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 51.0\n",
      "2024-06-27 11:48:53,318 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 2.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 2.0\n",
      "2024-06-27 11:48:53,321 - micro - MainProcess - INFO     Calculated 95th percentile: 51.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 51.0\n",
      "2024-06-27 11:48:53,324 - micro - MainProcess - INFO     Calculated 99th percentile: 51.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 51.0\n",
      "2024-06-27 11:48:53,328 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.019517846556041257 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.019517846556041257\n",
      "2024-06-27 11:48:53,330 - micro - MainProcess - INFO     Result: (51.0, 2.0, 51.0, 51.0, 0.019517846556041257) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (51.0, 2.0, 51.0, 51.0, 0.019517846556041257)\n",
      "2024-06-27 11:48:53,333 - micro - MainProcess - INFO     Calculating statistics for data: [0.02614009759999999, 0.02840992159999996, 0.033725058399999855, 0.011985763600000155, 0.028807157599999984] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [0.02614009759999999, 0.02840992159999996, 0.033725058399999855, 0.011985763600000155, 0.028807157599999984]\n",
      "2024-06-27 11:48:53,335 - micro - MainProcess - INFO     Data converted to numpy array: [0.0261401  0.02840992 0.03372506 0.01198576 0.02880716] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [0.0261401  0.02840992 0.03372506 0.01198576 0.02880716]\n",
      "2024-06-27 11:48:53,338 - micro - MainProcess - INFO     Calculated median: 0.02840992159999996 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.02840992159999996\n",
      "2024-06-27 11:48:53,342 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.002667059999999992 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.002667059999999992\n",
      "2024-06-27 11:48:53,346 - micro - MainProcess - INFO     Calculated 95th percentile: 0.03274147823999988 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 0.03274147823999988\n",
      "2024-06-27 11:48:53,349 - micro - MainProcess - INFO     Calculated 99th percentile: 0.03352834236799986 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 0.03352834236799986\n",
      "2024-06-27 11:48:53,352 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.28446818700711257 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.28446818700711257\n",
      "2024-06-27 11:48:53,355 - micro - MainProcess - INFO     Result: (0.02840992159999996, 0.002667059999999992, 0.03274147823999988, 0.03352834236799986, 0.28446818700711257) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.02840992159999996, 0.002667059999999992, 0.03274147823999988, 0.03352834236799986, 0.28446818700711257)\n",
      "2024-06-27 11:48:53,360 - micro - MainProcess - INFO     Calculating statistics for data: [13.453411500000016, 10.463891999999987, 17.72546410000001, 20.02827719999999, 7.849451299999998] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [13.453411500000016, 10.463891999999987, 17.72546410000001, 20.02827719999999, 7.849451299999998]\n",
      "2024-06-27 11:48:53,363 - micro - MainProcess - INFO     Data converted to numpy array: [13.4534115 10.463892  17.7254641 20.0282772  7.8494513] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [13.4534115 10.463892  17.7254641 20.0282772  7.8494513]\n",
      "2024-06-27 11:48:53,368 - micro - MainProcess - INFO     Calculated median: 13.453411500000016 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 13.453411500000016\n",
      "2024-06-27 11:48:53,372 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 7.2615721000000235 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 7.2615721000000235\n",
      "2024-06-27 11:48:53,376 - micro - MainProcess - INFO     Calculated 95th percentile: 19.567714579999993 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 19.567714579999993\n",
      "2024-06-27 11:48:53,379 - micro - MainProcess - INFO     Calculated 99th percentile: 19.93616467599999 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 19.93616467599999\n",
      "2024-06-27 11:48:53,383 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.3229340250559107 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.3229340250559107\n",
      "2024-06-27 11:48:53,387 - micro - MainProcess - INFO     Result: (13.453411500000016, 7.2615721000000235, 19.567714579999993, 19.93616467599999, 0.3229340250559107) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (13.453411500000016, 7.2615721000000235, 19.567714579999993, 19.93616467599999, 0.3229340250559107)\n",
      "2024-06-27 11:48:53,390 - micro - MainProcess - INFO     Calculating statistics for data: [500, 500, 500, 500, 500] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [500, 500, 500, 500, 500]\n",
      "2024-06-27 11:48:53,410 - micro - MainProcess - INFO     Data converted to numpy array: [500 500 500 500 500] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [500 500 500 500 500]\n",
      "2024-06-27 11:48:53,412 - micro - MainProcess - INFO     Calculated median: 500.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 500.0\n",
      "2024-06-27 11:48:53,445 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 11:48:53,472 - micro - MainProcess - INFO     Calculated 95th percentile: 500.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 500.0\n",
      "2024-06-27 11:48:53,476 - micro - MainProcess - INFO     Calculated 99th percentile: 500.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 500.0\n",
      "2024-06-27 11:48:53,481 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.0 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.0\n",
      "2024-06-27 11:48:53,485 - micro - MainProcess - INFO     Result: (500.0, 0.0, 500.0, 500.0, 0.0) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (500.0, 0.0, 500.0, 500.0, 0.0)\n",
      "2024-06-27 11:48:53,489 - micro - MainProcess - INFO     Calculating statistics for data: [49, 51, 51, 51, 51] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [49, 51, 51, 51, 51]\n",
      "2024-06-27 11:48:53,492 - micro - MainProcess - INFO     Data converted to numpy array: [49 51 51 51 51] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [49 51 51 51 51]\n",
      "2024-06-27 11:48:53,496 - micro - MainProcess - INFO     Calculated median: 51.0 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 51.0\n",
      "2024-06-27 11:48:53,500 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.0 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.0\n",
      "2024-06-27 11:48:53,503 - micro - MainProcess - INFO     Calculated 95th percentile: 51.0 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 51.0\n",
      "2024-06-27 11:48:53,508 - micro - MainProcess - INFO     Calculated 99th percentile: 51.0 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 51.0\n",
      "2024-06-27 11:48:53,513 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.015810276679841896 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.015810276679841896\n",
      "2024-06-27 11:48:53,517 - micro - MainProcess - INFO     Result: (51.0, 0.0, 51.0, 51.0, 0.015810276679841896) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (51.0, 0.0, 51.0, 51.0, 0.015810276679841896)\n",
      "2024-06-27 11:48:53,524 - micro - MainProcess - INFO     Calculating statistics for data: [0.02690682300000003, 0.020927783999999974, 0.03545092820000002, 0.04005655439999998, 0.0156989026] (utils.py:calculate_statistics:177)\n",
      "INFO:micro:Calculating statistics for data: [0.02690682300000003, 0.020927783999999974, 0.03545092820000002, 0.04005655439999998, 0.0156989026]\n",
      "2024-06-27 11:48:53,529 - micro - MainProcess - INFO     Data converted to numpy array: [0.02690682 0.02092778 0.03545093 0.04005655 0.0156989 ] (utils.py:calculate_statistics:185)\n",
      "INFO:micro:Data converted to numpy array: [0.02690682 0.02092778 0.03545093 0.04005655 0.0156989 ]\n",
      "2024-06-27 11:48:53,533 - micro - MainProcess - INFO     Calculated median: 0.02690682300000003 (utils.py:calculate_statistics:189)\n",
      "INFO:micro:Calculated median: 0.02690682300000003\n",
      "2024-06-27 11:48:53,539 - micro - MainProcess - INFO     Calculated interquartile range (IQR): 0.01452314420000005 (utils.py:calculate_statistics:193)\n",
      "INFO:micro:Calculated interquartile range (IQR): 0.01452314420000005\n",
      "2024-06-27 11:48:53,545 - micro - MainProcess - INFO     Calculated 95th percentile: 0.039135429159999985 (utils.py:calculate_statistics:197)\n",
      "INFO:micro:Calculated 95th percentile: 0.039135429159999985\n",
      "2024-06-27 11:48:53,548 - micro - MainProcess - INFO     Calculated 99th percentile: 0.03987232935199998 (utils.py:calculate_statistics:201)\n",
      "INFO:micro:Calculated 99th percentile: 0.03987232935199998\n",
      "2024-06-27 11:48:53,552 - micro - MainProcess - INFO     Calculated coefficient of variation (CV): 0.3229340250559106 (utils.py:calculate_statistics:205)\n",
      "INFO:micro:Calculated coefficient of variation (CV): 0.3229340250559106\n",
      "2024-06-27 11:48:53,556 - micro - MainProcess - INFO     Result: (0.02690682300000003, 0.01452314420000005, 0.039135429159999985, 0.03987232935199998, 0.3229340250559106) (utils.py:calculate_statistics:208)\n",
      "INFO:micro:Result: (0.02690682300000003, 0.01452314420000005, 0.039135429159999985, 0.03987232935199998, 0.3229340250559106)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------+-----------+--------------------+--------------------+--------------------+----------------------+----------------------+---------------------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    Model_MaxTokens     | Iterations |  Regions  |    Average TTLT    |    Median TTLT     |      IQR TTLT      | 95th Percentile TTLT | 99th Percentile TTLT |       CV TTLT       | Median Prompt Tokens | IQR Prompt Tokens | Median Completion Tokens | IQR Completion Tokens | 95th Percentile Completion Tokens | 99th Percentile Completion Tokens | CV Completion Tokens |     Average TBT      |      Median TBT      |       IQR TBT        | 95th Percentile TBT  | 99th Percentile TBT  | Average TTFT | Median TTFT | IQR TTFT | 95th Percentile TTFT | 99th Percentile TTFT | Error Rate | Error Types | Successful Runs | Unsuccessful Runs | Throttle Count | Throttle Rate |                                                                             Best Run                                                                              |                                                                            Worst Run                                                                             |\n",
      "+------------------------+------------+-----------+--------------------+--------------------+--------------------+----------------------+----------------------+---------------------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| gpt-4o-2024-05-13_250  |     5      | East US 2 | 6.453399939999997  |  7.10248039999999  | 0.666764999999998  |   8.18536955999997   |  8.382085591999964   | 0.2844681870071126  |         51.0         |        2.0        |          250.0           |          0.0          |               250.0               |               250.0               |         0.0          | 0.02581359975999999  | 0.02840992159999996  | 0.002667059999999992 | 0.03274147823999988  | 0.03352834236799986  |              |             |          |                      |                      |    0.0     |     []      |        5        |         0         |       0        |      0.0      | {\"ttlt\": 2.9964409000000387, \"completion_tokens\": 250, \"prompt_tokens\": 49, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-27 12:46:31 EDT\"}  | {\"ttlt\": 8.431264599999963, \"completion_tokens\": 250, \"prompt_tokens\": 49, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-27 12:46:27 EDT\"}  |\n",
      "| gpt-4o-2024-05-13_500  |     5      | East US 2 |    13.90409922     | 13.453411500000016 | 7.2615721000000235 |  19.567714579999993  |  19.93616467599999   | 0.3229340250559107  |         51.0         |        0.0        |          500.0           |          0.0          |               500.0               |               500.0               |         0.0          | 0.027808198440000004 | 0.02690682300000003  | 0.01452314420000005  | 0.039135429159999985 | 0.03987232935199998  |              |             |          |                      |                      |    0.0     |     []      |        5        |         0         |       0        |      0.0      |  {\"ttlt\": 7.849451299999998, \"completion_tokens\": 500, \"prompt_tokens\": 51, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-27 12:47:16 EDT\"}  | {\"ttlt\": 20.02827719999999, \"completion_tokens\": 500, \"prompt_tokens\": 51, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-27 12:47:07 EDT\"}  |\n",
      "| gpt-4o-2024-05-13_1000 |     6      | East US 2 | 26.657298083333334 | 24.887844699999988 | 10.505780250000008 |  41.95718412500003   |  44.75122730500005   | 0.37364087930274653 |         52.0         |        1.5        |          1000.0          |          0.0          |              1000.0               |              1000.0               |         0.0          | 0.026657298083333336 | 0.024887844699999988 | 0.01050578025000001  | 0.04195718412500003  | 0.044751227305000044 |              |             |          |                      |                      |    0.0     |     []      |        6        |         0         |       0        |      0.0      | {\"ttlt\": 14.786188600000003, \"completion_tokens\": 1000, \"prompt_tokens\": 50, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-27 12:47:48 EDT\"} | {\"ttlt\": 45.44973810000005, \"completion_tokens\": 1000, \"prompt_tokens\": 52, \"region\": \"East US 2\", \"utilization\": \"NA\", \"local_time\": \"2024-06-27 12:47:32 EDT\"} |\n",
      "+------------------------+------------+-----------+--------------------+--------------------+--------------------+----------------------+----------------------+---------------------+----------------------+-------------------+--------------------------+-----------------------+-----------------------------------+-----------------------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+--------------+-------------+----------+----------------------+----------------------+------------+-------------+-----------------+-------------------+----------------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4o-2024-05-13_1000': {'median_ttlt': 24.887844699999988,\n",
       "  'regions': ['East US 2'],\n",
       "  'iqr_ttlt': 10.505780250000008,\n",
       "  'percentile_95_ttlt': 41.95718412500003,\n",
       "  'percentile_99_ttlt': 44.75122730500005,\n",
       "  'cv_ttlt': 0.37364087930274653,\n",
       "  'median_completion_tokens': 1000.0,\n",
       "  'iqr_completion_tokens': 0.0,\n",
       "  'percentile_95_completion_tokens': 1000.0,\n",
       "  'percentile_99_completion_tokens': 1000.0,\n",
       "  'cv_completion_tokens': 0.0,\n",
       "  'median_prompt_tokens': 52.0,\n",
       "  'iqr_prompt_tokens': 1.5,\n",
       "  'percentile_95_prompt_tokens': 752.5,\n",
       "  'percentile_99_prompt_tokens': 939.3000000000002,\n",
       "  'cv_prompt_tokens': 1.6829977732662775,\n",
       "  'average_ttlt': 26.657298083333334,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 6,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 6,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 0.024887844699999988,\n",
       "  'iqr_tbt': 0.01050578025000001,\n",
       "  'percentile_95_tbt': 0.04195718412500003,\n",
       "  'percentile_99_tbt': 0.044751227305000044,\n",
       "  'cv_tbt': 0.3736408793027465,\n",
       "  'average_tbt': 0.026657298083333336,\n",
       "  'median_ttft': None,\n",
       "  'iqr_ttft': None,\n",
       "  'percentile_95_ttft': None,\n",
       "  'percentile_99_ttft': None,\n",
       "  'cv_ttft': None,\n",
       "  'average_ttft': None,\n",
       "  'best_run': {'ttlt': 14.786188600000003,\n",
       "   'completion_tokens': 1000,\n",
       "   'prompt_tokens': 50,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:48 EDT'},\n",
       "  'worst_run': {'ttlt': 45.44973810000005,\n",
       "   'completion_tokens': 1000,\n",
       "   'prompt_tokens': 52,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:32 EDT'}},\n",
       " 'gpt-4o-2024-05-13_250': {'median_ttlt': 7.10248039999999,\n",
       "  'regions': ['East US 2'],\n",
       "  'iqr_ttlt': 0.666764999999998,\n",
       "  'percentile_95_ttlt': 8.18536955999997,\n",
       "  'percentile_99_ttlt': 8.382085591999964,\n",
       "  'cv_ttlt': 0.2844681870071126,\n",
       "  'median_completion_tokens': 250.0,\n",
       "  'iqr_completion_tokens': 0.0,\n",
       "  'percentile_95_completion_tokens': 250.0,\n",
       "  'percentile_99_completion_tokens': 250.0,\n",
       "  'cv_completion_tokens': 0.0,\n",
       "  'median_prompt_tokens': 51.0,\n",
       "  'iqr_prompt_tokens': 2.0,\n",
       "  'percentile_95_prompt_tokens': 51.0,\n",
       "  'percentile_99_prompt_tokens': 51.0,\n",
       "  'cv_prompt_tokens': 0.019517846556041257,\n",
       "  'average_ttlt': 6.453399939999997,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 5,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 5,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 0.02840992159999996,\n",
       "  'iqr_tbt': 0.002667059999999992,\n",
       "  'percentile_95_tbt': 0.03274147823999988,\n",
       "  'percentile_99_tbt': 0.03352834236799986,\n",
       "  'cv_tbt': 0.28446818700711257,\n",
       "  'average_tbt': 0.02581359975999999,\n",
       "  'median_ttft': None,\n",
       "  'iqr_ttft': None,\n",
       "  'percentile_95_ttft': None,\n",
       "  'percentile_99_ttft': None,\n",
       "  'cv_ttft': None,\n",
       "  'average_ttft': None,\n",
       "  'best_run': {'ttlt': 2.9964409000000387,\n",
       "   'completion_tokens': 250,\n",
       "   'prompt_tokens': 49,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:46:31 EDT'},\n",
       "  'worst_run': {'ttlt': 8.431264599999963,\n",
       "   'completion_tokens': 250,\n",
       "   'prompt_tokens': 49,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:46:27 EDT'}},\n",
       " 'gpt-4o-2024-05-13_500': {'median_ttlt': 13.453411500000016,\n",
       "  'regions': ['East US 2'],\n",
       "  'iqr_ttlt': 7.2615721000000235,\n",
       "  'percentile_95_ttlt': 19.567714579999993,\n",
       "  'percentile_99_ttlt': 19.93616467599999,\n",
       "  'cv_ttlt': 0.3229340250559107,\n",
       "  'median_completion_tokens': 500.0,\n",
       "  'iqr_completion_tokens': 0.0,\n",
       "  'percentile_95_completion_tokens': 500.0,\n",
       "  'percentile_99_completion_tokens': 500.0,\n",
       "  'cv_completion_tokens': 0.0,\n",
       "  'median_prompt_tokens': 51.0,\n",
       "  'iqr_prompt_tokens': 0.0,\n",
       "  'percentile_95_prompt_tokens': 51.0,\n",
       "  'percentile_99_prompt_tokens': 51.0,\n",
       "  'cv_prompt_tokens': 0.015810276679841896,\n",
       "  'average_ttlt': 13.90409922,\n",
       "  'error_rate': 0.0,\n",
       "  'number_of_iterations': 5,\n",
       "  'throttle_count': 0,\n",
       "  'throttle_rate': 0.0,\n",
       "  'errors_types': [],\n",
       "  'successful_runs': 5,\n",
       "  'unsuccessful_runs': 0,\n",
       "  'median_tbt': 0.02690682300000003,\n",
       "  'iqr_tbt': 0.01452314420000005,\n",
       "  'percentile_95_tbt': 0.039135429159999985,\n",
       "  'percentile_99_tbt': 0.03987232935199998,\n",
       "  'cv_tbt': 0.3229340250559106,\n",
       "  'average_tbt': 0.027808198440000004,\n",
       "  'median_ttft': None,\n",
       "  'iqr_ttft': None,\n",
       "  'percentile_95_ttft': None,\n",
       "  'percentile_99_ttft': None,\n",
       "  'cv_ttft': None,\n",
       "  'average_ttft': None,\n",
       "  'best_run': {'ttlt': 7.849451299999998,\n",
       "   'completion_tokens': 500,\n",
       "   'prompt_tokens': 51,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:16 EDT'},\n",
       "  'worst_run': {'ttlt': 20.02827719999999,\n",
       "   'completion_tokens': 500,\n",
       "   'prompt_tokens': 51,\n",
       "   'region': 'East US 2',\n",
       "   'utilization': 'NA',\n",
       "   'local_time': '2024-06-27 12:47:07 EDT'}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_nonstreaming.calculate_and_show_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = AzureOpenAIBenchmarkStreaming(api_key=OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=DEPLOYMENT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 23:36:50,322 - micro - MainProcess - INFO     Starting call to model gpt-4o-2024-05-13 with max tokens 100 at (Local time): 2024-06-26 23:36:50.322042, (GMT): 2024-06-27 04:36:50.322042+00:00 (latencytest.py:make_call:529)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 971.27}\n",
      "{'Count': 2, 'Content': 'The', 'Token size': 1, 'Time taken (ms)': 787.63}\n",
      "{'Count': 3, 'Content': ' some', 'Token size': 1, 'Time taken (ms)': 1068.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 23:36:54,501 - micro - MainProcess - INFO     Finished call to model gpt-4o-2024-05-13. Time taken for chat: 4.18 seconds or 4178.88 milliseconds. (latencytest.py:make_call:581)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Count': 4, 'Content': ' by', 'Token size': 1, 'Time taken (ms)': 1264.66}\n",
      "{'Count': 5, 'Content': 'qu', 'Token size': 1, 'Time taken (ms)': 54.4}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m benchmark\u001b[38;5;241m.\u001b[39mmake_call(deployment_name\u001b[38;5;241m=\u001b[39mDEPLOYMENT_ID, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\upgrade-llm\\lib\\site-packages\\backoff\\_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[0;32m    148\u001b[0m }\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    153\u001b[0m     giveup_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m giveup(e)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\gbb-ai-upgrade-llm\\src\\performance\\latencytest.py:595\u001b[0m, in \u001b[0;36mAzureOpenAIBenchmarkStreaming.make_call\u001b[1;34m(self, deployment_name, max_tokens, timeout)\u001b[0m\n\u001b[0;32m    589\u001b[0m     TTFT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(first_token_time \u001b[38;5;241m/\u001b[39m context_tokens_size, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m context_tokens_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    590\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTBT\u001b[39m\u001b[38;5;124m\"\u001b[39m: TBT,\n\u001b[0;32m    592\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTFT\u001b[39m\u001b[38;5;124m\"\u001b[39m: TTFT,\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTLT\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(total_time_taken \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    594\u001b[0m     }\n\u001b[1;32m--> 595\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_time_taken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_results(deployment_name, max_tokens, headers, total_time_taken)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\gbb-ai-upgrade-llm\\src\\performance\\latencytest.py:299\u001b[0m, in \u001b[0;36mAzureOpenAIBenchmarkLatency._store_results\u001b[1;34m(self, deployment_name, max_tokens, headers, time_taken, metrics)\u001b[0m\n\u001b[0;32m    297\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeployment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults:\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimes_succesful\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimes_unsucessfull\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregions\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_of_iterations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: []},\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_run\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m    308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    309\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    311\u001b[0m         },\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworst_run\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    314\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    316\u001b[0m         },\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTBT\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTFT\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTTLT\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     }\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_taken \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_of_iterations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "await benchmark.make_call(deployment_name=DEPLOYMENT_ID, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import aiohttp\n",
    "import backoff\n",
    "from datetime import datetime, timezone\n",
    "from tabulate import tabulate\n",
    "import tiktoken\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def _terminal_http_code(e):\n",
    "    return 400 <= e.status < 500\n",
    "\n",
    "MAX_RETRY_ATTEMPTS = 3\n",
    "MAX_TIMEOUT_SECONDS = 60\n",
    "\n",
    "class AzureOpenAIBenchmarkStreaming(AzureOpenAIBenchmarkLatency):\n",
    "    def __init__(self, api_key, azure_endpoint, api_version=\"2024-02-15-preview\"):\n",
    "        \"\"\"\n",
    "        Initialize the AzureOpenAILatencyBenchmark with the API key, API version, and endpoint.\n",
    "        \"\"\"\n",
    "        super().__init__(api_key, azure_endpoint, api_version)\n",
    "        self.api_key = api_key\n",
    "        self.azure_endpoint = azure_endpoint\n",
    "        self.api_version = api_version\n",
    "        self.results = []\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        aiohttp.ClientError,\n",
    "        jitter=backoff.full_jitter,\n",
    "        max_tries=MAX_RETRY_ATTEMPTS,\n",
    "        giveup=_terminal_http_code,\n",
    "    )\n",
    "    async def make_call(self, deployment_name, max_tokens, timeout=60):\n",
    "        \"\"\"\n",
    "        Make a chat completion call and print the time taken for the call.\n",
    "        \"\"\"\n",
    "        base_url = self.azure_endpoint\n",
    "        url = f\"{base_url}/openai/deployments/{deployment_name}/chat/completions?api-version={self.api_version}\"\n",
    "        start_phrase = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Give me history of New York\"}\n",
    "        ]\n",
    "        payload = {\n",
    "            \"messages\": start_phrase,\n",
    "            \"stream\": True,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        headers = {\n",
    "            \"api-key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        logger.info(\n",
    "            f\"Starting call to model {deployment_name} with max tokens {max_tokens} at (Local time): {datetime.now()}, (GMT): {datetime.now(timezone.utc)}\"\n",
    "        )\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                async with session.post(url, headers=headers, json=payload, timeout=timeout) as response:\n",
    "                    response.raise_for_status()\n",
    "                    prev_end_time = start_time  # Initialize prev_end_time to store the previous end_time\n",
    "                    count = 0  # Initialize a counter for chunks\n",
    "                    token_times = []  # List to store times for each token\n",
    "                    first_token_time = None\n",
    "\n",
    "                    async for chunk in response.content.iter_any():\n",
    "                        if chunk:  # filter out keep-alive new lines\n",
    "                            try:\n",
    "                                response_chunk = json.loads(chunk)\n",
    "                            except json.JSONDecodeError:\n",
    "                                # Extract content from the chunk string\n",
    "                                content_match = re.search(r'\"content\":\"(.*?)\"', chunk.decode('utf-8'))\n",
    "                                if content_match:\n",
    "                                    count += 1  # Increment the counter\n",
    "                                    content = content_match.group(1)\n",
    "\n",
    "                                    token_size = num_tokens_from_string(content, \"cl100k_base\")\n",
    "\n",
    "                                    end_time = time.perf_counter()\n",
    "                                    time_taken = (end_time - prev_end_time) * 1000  # Subtract prev_end_time from start_time\n",
    "                                    time_taken = max(time_taken, 1)  # Set a minimum value of 1 ms for time_taken\n",
    "                                    time_taken = round(time_taken, 2)  # Round time_taken to 2 decimal places\n",
    "\n",
    "                                    result = {\"Count\": count, \"Content\": content, \"Token size\": token_size, \"Time taken (ms)\": time_taken}\n",
    "                                    self.results.append(result)\n",
    "\n",
    "                                    # Print the result as it is added to the list\n",
    "                                    print(result)\n",
    "                                    \n",
    "                                    # Record the time for each token\n",
    "                                    token_times.append(time_taken)\n",
    "                                    if first_token_time is None:\n",
    "                                        first_token_time = time_taken\n",
    "\n",
    "                                    prev_end_time = end_time  # Update prev_end_time for the next iteration\n",
    "\n",
    "            except aiohttp.ClientError as e:\n",
    "                logger.error(f\"Error during API call: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        total_time_taken = end_time - start_time\n",
    "\n",
    "        logger.info(\n",
    "            f\"Finished call to model {deployment_name}. Time taken for chat: {round(total_time_taken, 2)} seconds or {round(total_time_taken * 1000, 2)} milliseconds.\"\n",
    "        )\n",
    "\n",
    "        # Calculate TBT and TTFT\n",
    "        if token_times:\n",
    "            TBT = round(sum(token_times[1:]) / (len(token_times) - 1), 2) if len(token_times) > 1 else 0\n",
    "            context_tokens_size = num_tokens_from_string(\"Give me history of New York\", \"cl100k_base\")\n",
    "            TTFT = round(first_token_time / context_tokens_size, 2) if context_tokens_size > 0 else 0\n",
    "            self.results.append({\"Count\": \"TBT\", \"Content\": \"\", \"Token size\": \"\", \"Time taken (ms)\": TBT})\n",
    "            self.results.append({\"Count\": \"TTFT\", \"Content\": \"\", \"Token size\": \"\", \"Time taken (ms)\": TTFT})\n",
    "\n",
    "        # Calculate the cumulative time\n",
    "        cumulative_time = sum(result[\"Time taken (ms)\"] for result in self.results if isinstance(result[\"Time taken (ms)\"], (int, float)))\n",
    "        cumulative_time = round(cumulative_time, 2)  # Round cumulative_time to 2 decimal places\n",
    "\n",
    "        # Add the cumulative time to the results list as an extra row\n",
    "        self.results.append({\"Count\": \"TTLT\", \"Content\": \"\", \"Token size\": \"\", \"Time taken (ms)\": cumulative_time})\n",
    "\n",
    "        # Display results in tabular form\n",
    "        print(\"\\nResults Table:\")\n",
    "        print(tabulate([result.values() for result in self.results], headers=self.results[0].keys(), tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now you can access the environment variables using os.getenv\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "DEPLOYMENT_ID = os.getenv(\"AZURE_AOAI_CHAT_MODEL_NAME_DEPLOYMENT_ID\")\n",
    "DEPLOYMENT_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:  \n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"  \n",
    "    encoding = tiktoken.get_encoding(encoding_name)  \n",
    "    num_tokens = len(encoding.encode(string))  \n",
    "    return num_tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:openai.ChatCompletion.create\n",
      "END:openai.ChatCompletion.create\n",
      "Time taken: 3.7182253000000856 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "url = AZURE_OPENAI_ENDPOINT + \"/openai/deployments/\" + DEPLOYMENT_ID + \"/chat/completions?api-version=2023-05-15\"\n",
    "start_phrase = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},\n",
    "                {\"role\":\"user\",\"content\":\"Write 5000 word essay on soccer\"}]\n",
    "payload = {        \n",
    "    \"messages\":start_phrase,\n",
    "    #\"stream\":True,\n",
    "    \"max_tokens\":200\n",
    "    }\n",
    "\n",
    "headers = {  \n",
    "    \"api-key\": OPENAI_API_KEY,  \n",
    "    \"Content-Type\": \"application/json\"  \n",
    "} \n",
    "print(\"Begin:openai.ChatCompletion.create\")\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "try:  \n",
    "    r = requests.post(url,  \n",
    "                      headers=headers,  \n",
    "                      json=payload,  \n",
    "                      timeout=10  \n",
    "                      )  \n",
    "except requests.exceptions.Timeout:  \n",
    "    print(\"The request has timed out. Please try again.\")  \n",
    "    \n",
    "print(\"END:openai.ChatCompletion.create\")\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin:openai.ChatCompletion.create\n",
      "{'Count': 1, 'Content': 'Sure', 'Token size': 1, 'Time taken (ms)': 709.08}\n",
      "{'Count': 2, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1.58}\n",
      "{'Count': 3, 'Content': ' here', 'Token size': 1, 'Time taken (ms)': 1.95}\n",
      "{'Count': 4, 'Content': ' is', 'Token size': 1, 'Time taken (ms)': 1.21}\n",
      "{'Count': 5, 'Content': ' an', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 6, 'Content': ' essay', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 7, 'Content': ' on', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 8, 'Content': ' soccer', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 9, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 10, 'Content': ' Due', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 11, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 12, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 13, 'Content': ' platform', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 14, 'Content': \"'s\", 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 15, 'Content': ' constraints', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 16, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 17, 'Content': ' I', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 18, 'Content': \" can't\", 'Token size': 2, 'Time taken (ms)': 1}\n",
      "{'Count': 19, 'Content': ' provide', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 20, 'Content': ' a', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 21, 'Content': ' full', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 22, 'Content': ' ', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 23, 'Content': '500', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 24, 'Content': '0', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 25, 'Content': ' words', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 26, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 27, 'Content': ' one', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 28, 'Content': ' go', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 29, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 30, 'Content': ' but', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 31, 'Content': \" I'll\", 'Token size': 2, 'Time taken (ms)': 1.0}\n",
      "{'Count': 32, 'Content': ' give', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 33, 'Content': ' you', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 34, 'Content': ' a', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 35, 'Content': ' sizeable', 'Token size': 2, 'Time taken (ms)': 3.0}\n",
      "{'Count': 36, 'Content': ' portion', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 37, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 640.83}\n",
      "{'Count': 38, 'Content': ' get', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 39, 'Content': ' started', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 40, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 41, 'Content': ' \\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 1}\n",
      "{'Count': 42, 'Content': '---\\\\n\\\\n', 'Token size': 4, 'Time taken (ms)': 6.95}\n",
      "{'Count': 43, 'Content': '###', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 44, 'Content': ' The', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 45, 'Content': ' Global', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 46, 'Content': ' Phen', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 47, 'Content': 'omen', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 48, 'Content': 'on', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 49, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 50, 'Content': ' Soccer', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 51, 'Content': '\\\\n\\\\n', 'Token size': 2, 'Time taken (ms)': 1.02}\n",
      "{'Count': 52, 'Content': '####', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 53, 'Content': ' Introduction', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 54, 'Content': '\\\\n\\\\n', 'Token size': 2, 'Time taken (ms)': 1}\n",
      "{'Count': 55, 'Content': 'Soc', 'Token size': 2, 'Time taken (ms)': 1}\n",
      "{'Count': 56, 'Content': 'cer', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 57, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1.57}\n",
      "{'Count': 58, 'Content': ' known', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 59, 'Content': ' as', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 60, 'Content': ' football', 'Token size': 1, 'Time taken (ms)': 1.07}\n",
      "{'Count': 61, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 62, 'Content': ' most', 'Token size': 1, 'Time taken (ms)': 1.08}\n",
      "{'Count': 63, 'Content': ' countries', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 64, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 65, 'Content': ' stands', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 66, 'Content': ' as', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 67, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 68, 'Content': ' world', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 69, 'Content': 's', 'Token size': 1, 'Time taken (ms)': 1.07}\n",
      "{'Count': 70, 'Content': ' most', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 71, 'Content': ' popular', 'Token size': 1, 'Time taken (ms)': 472.87}\n",
      "{'Count': 72, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 73, 'Content': ' beloved', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 74, 'Content': ' sport', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 75, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 76, 'Content': ' Its', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 77, 'Content': ' simplicity', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 78, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 79, 'Content': ' accessibility', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 80, 'Content': ' have', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 81, 'Content': ' allowed', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 82, 'Content': ' it', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 83, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 84, 'Content': ' transcend', 'Token size': 1, 'Time taken (ms)': 2.0}\n",
      "{'Count': 85, 'Content': ' cultural', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 86, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 87, 'Content': ' economic', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 88, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 89, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 90, 'Content': ' geographical', 'Token size': 1, 'Time taken (ms)': 1.51}\n",
      "{'Count': 91, 'Content': ' barriers', 'Token size': 1, 'Time taken (ms)': 1.02}\n",
      "{'Count': 92, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 1.01}\n",
      "{'Count': 93, 'Content': ' Whether', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 94, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 95, 'Content': ' urban', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 96, 'Content': ' meg', 'Token size': 1, 'Time taken (ms)': 986.48}\n",
      "{'Count': 97, 'Content': 'ac', 'Token size': 1, 'Time taken (ms)': 1.01}\n",
      "{'Count': 98, 'Content': 'ities', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 99, 'Content': ' or', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 100, 'Content': ' rural', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 101, 'Content': ' communities', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 102, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 103, 'Content': ' soccer', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 104, 'Content': ' has', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 105, 'Content': ' established', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 106, 'Content': ' itself', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 107, 'Content': ' as', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 108, 'Content': ' not', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 109, 'Content': ' merely', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 110, 'Content': ' a', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 111, 'Content': ' sport', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 112, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 113, 'Content': ' but', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 114, 'Content': ' a', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 115, 'Content': ' significant', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 116, 'Content': ' cultural', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 117, 'Content': ' force', 'Token size': 1, 'Time taken (ms)': 1.03}\n",
      "{'Count': 118, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 119, 'Content': ' This', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 120, 'Content': ' essay', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 121, 'Content': ' del', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 122, 'Content': 'ves', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 123, 'Content': ' into', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 124, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 125, 'Content': ' intric', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 126, 'Content': 'acies', 'Token size': 1, 'Time taken (ms)': 618.69}\n",
      "{'Count': 127, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 128, 'Content': ' soccer', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 129, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 130, 'Content': ' its', 'Token size': 1, 'Time taken (ms)': 1.0}\n",
      "{'Count': 131, 'Content': ' history', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 132, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 133, 'Content': ' global', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 134, 'Content': ' influence', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 135, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 136, 'Content': ' economic', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 137, 'Content': ' impact', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 138, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 139, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 140, 'Content': ' social', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 141, 'Content': ' significance', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 142, 'Content': '.\\\\n\\\\n', 'Token size': 3, 'Time taken (ms)': 1}\n",
      "{'Count': 143, 'Content': '####', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 144, 'Content': ' History', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 145, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 146, 'Content': ' Soccer', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 147, 'Content': '\\\\n\\\\n', 'Token size': 2, 'Time taken (ms)': 1}\n",
      "{'Count': 148, 'Content': 'The', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 149, 'Content': ' origins', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 150, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 151, 'Content': ' soccer', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 152, 'Content': ' can', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 153, 'Content': ' be', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 154, 'Content': ' traced', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 155, 'Content': ' back', 'Token size': 1, 'Time taken (ms)': 612.13}\n",
      "{'Count': 156, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 1.4}\n",
      "{'Count': 157, 'Content': ' ancient', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 158, 'Content': ' civilizations', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 159, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 160, 'Content': ' with', 'Token size': 1, 'Time taken (ms)': 1.11}\n",
      "{'Count': 161, 'Content': ' early', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 162, 'Content': ' forms', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 163, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 164, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 165, 'Content': ' game', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 166, 'Content': ' appearing', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 167, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 168, 'Content': ' China', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 169, 'Content': ' during', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 170, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 171, 'Content': ' Han', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 172, 'Content': ' Dynasty', 'Token size': 1, 'Time taken (ms)': 8.96}\n",
      "{'Count': 173, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 174, 'Content': ' Greece', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 175, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1.02}\n",
      "{'Count': 176, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 177, 'Content': ' Rome', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 178, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 179, 'Content': ' However', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 180, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 181, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 182, 'Content': ' modern', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 183, 'Content': ' version', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 184, 'Content': ' of', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 185, 'Content': ' soccer', 'Token size': 1, 'Time taken (ms)': 324.38}\n",
      "{'Count': 186, 'Content': ' began', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 187, 'Content': ' to', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 188, 'Content': ' take', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 189, 'Content': ' shape', 'Token size': 1, 'Time taken (ms)': 1.51}\n",
      "{'Count': 190, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 191, 'Content': ' England', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 192, 'Content': ' in', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 193, 'Content': ' the', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 194, 'Content': ' ', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 195, 'Content': '19', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 196, 'Content': 'th', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 197, 'Content': ' century', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 198, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 199, 'Content': ' Initially', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "{'Count': 200, 'Content': ',', 'Token size': 1, 'Time taken (ms)': 1}\n",
      "END:openai.ChatCompletion.create\n",
      "Total time taken 4.79 seconds to complete.\n",
      "\n",
      "Results Table:\n",
      "+------------+---------------+------------+-----------------+\n",
      "|   Count    |    Content    | Token size | Time taken (ms) |\n",
      "+------------+---------------+------------+-----------------+\n",
      "|     1      |     Sure      |     1      |     709.08      |\n",
      "|     2      |       ,       |     1      |      1.58       |\n",
      "|     3      |     here      |     1      |      1.95       |\n",
      "|     4      |      is       |     1      |      1.21       |\n",
      "|     5      |      an       |     1      |        1        |\n",
      "|     6      |     essay     |     1      |        1        |\n",
      "|     7      |      on       |     1      |        1        |\n",
      "|     8      |    soccer     |     1      |        1        |\n",
      "|     9      |       .       |     1      |       1.0       |\n",
      "|     10     |      Due      |     1      |        1        |\n",
      "|     11     |      to       |     1      |       1.0       |\n",
      "|     12     |      the      |     1      |        1        |\n",
      "|     13     |   platform    |     1      |       1.0       |\n",
      "|     14     |      's       |     1      |        1        |\n",
      "|     15     |  constraints  |     1      |        1        |\n",
      "|     16     |       ,       |     1      |        1        |\n",
      "|     17     |       I       |     1      |        1        |\n",
      "|     18     |     can't     |     2      |        1        |\n",
      "|     19     |    provide    |     1      |       1.0       |\n",
      "|     20     |       a       |     1      |        1        |\n",
      "|     21     |     full      |     1      |        1        |\n",
      "|     22     |               |     1      |        1        |\n",
      "|     23     |      500      |     1      |        1        |\n",
      "|     24     |       0       |     1      |        1        |\n",
      "|     25     |     words     |     1      |        1        |\n",
      "|     26     |      in       |     1      |        1        |\n",
      "|     27     |      one      |     1      |        1        |\n",
      "|     28     |      go       |     1      |        1        |\n",
      "|     29     |       ,       |     1      |        1        |\n",
      "|     30     |      but      |     1      |        1        |\n",
      "|     31     |     I'll      |     2      |       1.0       |\n",
      "|     32     |     give      |     1      |        1        |\n",
      "|     33     |      you      |     1      |        1        |\n",
      "|     34     |       a       |     1      |        1        |\n",
      "|     35     |   sizeable    |     2      |       3.0       |\n",
      "|     36     |    portion    |     1      |        1        |\n",
      "|     37     |      to       |     1      |     640.83      |\n",
      "|     38     |      get      |     1      |        1        |\n",
      "|     39     |    started    |     1      |        1        |\n",
      "|     40     |       .       |     1      |        1        |\n",
      "|     41     |     \\n\\n      |     3      |        1        |\n",
      "|     42     |    ---\\n\\n    |     4      |      6.95       |\n",
      "|     43     |      ###      |     1      |        1        |\n",
      "|     44     |      The      |     1      |       1.0       |\n",
      "|     45     |    Global     |     1      |        1        |\n",
      "|     46     |     Phen      |     1      |        1        |\n",
      "|     47     |     omen      |     1      |        1        |\n",
      "|     48     |      on       |     1      |        1        |\n",
      "|     49     |      of       |     1      |        1        |\n",
      "|     50     |    Soccer     |     1      |        1        |\n",
      "|     51     |     \\n\\n      |     2      |      1.02       |\n",
      "|     52     |     ####      |     1      |        1        |\n",
      "|     53     | Introduction  |     1      |        1        |\n",
      "|     54     |     \\n\\n      |     2      |        1        |\n",
      "|     55     |      Soc      |     2      |        1        |\n",
      "|     56     |      cer      |     1      |        1        |\n",
      "|     57     |       ,       |     1      |      1.57       |\n",
      "|     58     |     known     |     1      |        1        |\n",
      "|     59     |      as       |     1      |        1        |\n",
      "|     60     |   football    |     1      |      1.07       |\n",
      "|     61     |      in       |     1      |        1        |\n",
      "|     62     |     most      |     1      |      1.08       |\n",
      "|     63     |   countries   |     1      |        1        |\n",
      "|     64     |       ,       |     1      |        1        |\n",
      "|     65     |    stands     |     1      |        1        |\n",
      "|     66     |      as       |     1      |        1        |\n",
      "|     67     |      the      |     1      |        1        |\n",
      "|     68     |     world     |     1      |        1        |\n",
      "|     69     |      s       |     1      |      1.07       |\n",
      "|     70     |     most      |     1      |        1        |\n",
      "|     71     |    popular    |     1      |     472.87      |\n",
      "|     72     |      and      |     1      |        1        |\n",
      "|     73     |    beloved    |     1      |       1.0       |\n",
      "|     74     |     sport     |     1      |        1        |\n",
      "|     75     |       .       |     1      |        1        |\n",
      "|     76     |      Its      |     1      |        1        |\n",
      "|     77     |  simplicity   |     1      |        1        |\n",
      "|     78     |      and      |     1      |        1        |\n",
      "|     79     | accessibility |     1      |        1        |\n",
      "|     80     |     have      |     1      |        1        |\n",
      "|     81     |    allowed    |     1      |        1        |\n",
      "|     82     |      it       |     1      |        1        |\n",
      "|     83     |      to       |     1      |        1        |\n",
      "|     84     |   transcend   |     1      |       2.0       |\n",
      "|     85     |   cultural    |     1      |        1        |\n",
      "|     86     |       ,       |     1      |        1        |\n",
      "|     87     |   economic    |     1      |        1        |\n",
      "|     88     |       ,       |     1      |        1        |\n",
      "|     89     |      and      |     1      |        1        |\n",
      "|     90     | geographical  |     1      |      1.51       |\n",
      "|     91     |   barriers    |     1      |      1.02       |\n",
      "|     92     |       .       |     1      |      1.01       |\n",
      "|     93     |    Whether    |     1      |        1        |\n",
      "|     94     |      in       |     1      |        1        |\n",
      "|     95     |     urban     |     1      |        1        |\n",
      "|     96     |      meg      |     1      |     986.48      |\n",
      "|     97     |      ac       |     1      |      1.01       |\n",
      "|     98     |     ities     |     1      |        1        |\n",
      "|     99     |      or       |     1      |        1        |\n",
      "|    100     |     rural     |     1      |        1        |\n",
      "|    101     |  communities  |     1      |        1        |\n",
      "|    102     |       ,       |     1      |        1        |\n",
      "|    103     |    soccer     |     1      |        1        |\n",
      "|    104     |      has      |     1      |        1        |\n",
      "|    105     |  established  |     1      |        1        |\n",
      "|    106     |    itself     |     1      |        1        |\n",
      "|    107     |      as       |     1      |        1        |\n",
      "|    108     |      not      |     1      |        1        |\n",
      "|    109     |    merely     |     1      |        1        |\n",
      "|    110     |       a       |     1      |        1        |\n",
      "|    111     |     sport     |     1      |        1        |\n",
      "|    112     |       ,       |     1      |        1        |\n",
      "|    113     |      but      |     1      |        1        |\n",
      "|    114     |       a       |     1      |        1        |\n",
      "|    115     |  significant  |     1      |        1        |\n",
      "|    116     |   cultural    |     1      |        1        |\n",
      "|    117     |     force     |     1      |      1.03       |\n",
      "|    118     |       .       |     1      |        1        |\n",
      "|    119     |     This      |     1      |        1        |\n",
      "|    120     |     essay     |     1      |        1        |\n",
      "|    121     |      del      |     1      |        1        |\n",
      "|    122     |      ves      |     1      |        1        |\n",
      "|    123     |     into      |     1      |        1        |\n",
      "|    124     |      the      |     1      |        1        |\n",
      "|    125     |    intric     |     1      |        1        |\n",
      "|    126     |     acies     |     1      |     618.69      |\n",
      "|    127     |      of       |     1      |        1        |\n",
      "|    128     |    soccer     |     1      |       1.0       |\n",
      "|    129     |       ,       |     1      |        1        |\n",
      "|    130     |      its      |     1      |       1.0       |\n",
      "|    131     |    history    |     1      |        1        |\n",
      "|    132     |       ,       |     1      |        1        |\n",
      "|    133     |    global     |     1      |        1        |\n",
      "|    134     |   influence   |     1      |        1        |\n",
      "|    135     |       ,       |     1      |        1        |\n",
      "|    136     |   economic    |     1      |        1        |\n",
      "|    137     |    impact     |     1      |        1        |\n",
      "|    138     |       ,       |     1      |        1        |\n",
      "|    139     |      and      |     1      |        1        |\n",
      "|    140     |    social     |     1      |        1        |\n",
      "|    141     | significance  |     1      |        1        |\n",
      "|    142     |     .\\n\\n     |     3      |        1        |\n",
      "|    143     |     ####      |     1      |        1        |\n",
      "|    144     |    History    |     1      |        1        |\n",
      "|    145     |      of       |     1      |        1        |\n",
      "|    146     |    Soccer     |     1      |        1        |\n",
      "|    147     |     \\n\\n      |     2      |        1        |\n",
      "|    148     |      The      |     1      |        1        |\n",
      "|    149     |    origins    |     1      |        1        |\n",
      "|    150     |      of       |     1      |        1        |\n",
      "|    151     |    soccer     |     1      |        1        |\n",
      "|    152     |      can      |     1      |        1        |\n",
      "|    153     |      be       |     1      |        1        |\n",
      "|    154     |    traced     |     1      |        1        |\n",
      "|    155     |     back      |     1      |     612.13      |\n",
      "|    156     |      to       |     1      |       1.4       |\n",
      "|    157     |    ancient    |     1      |        1        |\n",
      "|    158     | civilizations |     1      |        1        |\n",
      "|    159     |       ,       |     1      |        1        |\n",
      "|    160     |     with      |     1      |      1.11       |\n",
      "|    161     |     early     |     1      |        1        |\n",
      "|    162     |     forms     |     1      |        1        |\n",
      "|    163     |      of       |     1      |        1        |\n",
      "|    164     |      the      |     1      |        1        |\n",
      "|    165     |     game      |     1      |        1        |\n",
      "|    166     |   appearing   |     1      |        1        |\n",
      "|    167     |      in       |     1      |        1        |\n",
      "|    168     |     China     |     1      |        1        |\n",
      "|    169     |    during     |     1      |        1        |\n",
      "|    170     |      the      |     1      |        1        |\n",
      "|    171     |      Han      |     1      |        1        |\n",
      "|    172     |    Dynasty    |     1      |      8.96       |\n",
      "|    173     |       ,       |     1      |        1        |\n",
      "|    174     |    Greece     |     1      |        1        |\n",
      "|    175     |       ,       |     1      |      1.02       |\n",
      "|    176     |      and      |     1      |        1        |\n",
      "|    177     |     Rome      |     1      |        1        |\n",
      "|    178     |       .       |     1      |        1        |\n",
      "|    179     |    However    |     1      |        1        |\n",
      "|    180     |       ,       |     1      |        1        |\n",
      "|    181     |      the      |     1      |        1        |\n",
      "|    182     |    modern     |     1      |        1        |\n",
      "|    183     |    version    |     1      |        1        |\n",
      "|    184     |      of       |     1      |        1        |\n",
      "|    185     |    soccer     |     1      |     324.38      |\n",
      "|    186     |     began     |     1      |        1        |\n",
      "|    187     |      to       |     1      |        1        |\n",
      "|    188     |     take      |     1      |        1        |\n",
      "|    189     |     shape     |     1      |      1.51       |\n",
      "|    190     |      in       |     1      |        1        |\n",
      "|    191     |    England    |     1      |        1        |\n",
      "|    192     |      in       |     1      |        1        |\n",
      "|    193     |      the      |     1      |        1        |\n",
      "|    194     |               |     1      |        1        |\n",
      "|    195     |      19       |     1      |        1        |\n",
      "|    196     |      th       |     1      |        1        |\n",
      "|    197     |    century    |     1      |        1        |\n",
      "|    198     |       .       |     1      |        1        |\n",
      "|    199     |   Initially   |     1      |        1        |\n",
      "|    200     |       ,       |     1      |        1        |\n",
      "| Cumulative |               |            |     4578.54     |\n",
      "+------------+---------------+------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import regex as re\n",
    "from tabulate import tabulate  \n",
    "\n",
    "payload = {        \n",
    "    \"messages\":start_phrase,\n",
    "    \"stream\":True,\n",
    "    \"max_tokens\":200\n",
    "    }\n",
    "\n",
    "results = []  \n",
    "\n",
    "try: \n",
    "    print(\"Begin:openai.ChatCompletion.create\")  \n",
    "    start_time_api_call = time.perf_counter()  # Initialize start_time before the loop  \n",
    "    with requests.post(url, headers=headers, json=payload, timeout=10, stream=True) as r:  \n",
    "        start_time = time.time()  # Initialize start_time before the loop  \n",
    "        prev_end_time = start_time  # Initialize prev_end_time to store the previous end_time  \n",
    "        count = 0  # Initialize a counter for chunks  \n",
    "        for chunk in r.iter_lines():  \n",
    "            if chunk:  # filter out keep-alive new lines  \n",
    "                try:  \n",
    "                    response_chunk = json.loads(chunk)  \n",
    "                except json.JSONDecodeError:  \n",
    "                    # Extract content from the chunk string  \n",
    "                    content_match = re.search(r'\"content\":\"(.*?)\"', chunk.decode('utf-8'))  \n",
    "                    if content_match:  \n",
    "                        count += 1  # Increment the counter  \n",
    "                        content = content_match.group(1)  \n",
    "  \n",
    "                        token_size = num_tokens_from_string(content, \"cl100k_base\")  \n",
    "  \n",
    "                        end_time = time.time()  \n",
    "                        time_taken = (end_time - prev_end_time) * 1000  # Subtract prev_end_time from start_time  \n",
    "                        time_taken = max(time_taken, 1)  # Set a minimum value of 0.01 ms for time_taken  \n",
    "                        time_taken = round(time_taken, 2)  # Round time_taken to 2 decimal places  \n",
    "  \n",
    "                        result = {\"Count\": count, \"Content\": content, \"Token size\": token_size, \"Time taken (ms)\": time_taken}  \n",
    "                        results.append(result)  \n",
    "  \n",
    "                        # Print the result as it is added to the list  \n",
    "                        print(result)  \n",
    "                        prev_end_time = end_time  # Update prev_end_time for the next iteration  \n",
    "  \n",
    "                    continue  \n",
    "  \n",
    "except requests.exceptions.Timeout:  \n",
    "    print(\"The request has timed out. Please try again.\")  \n",
    "\n",
    "print(\"END:openai.ChatCompletion.create\")  \n",
    "end_time_api_call = time.perf_counter() \n",
    "  \n",
    "# Calculate the time taken and print the result  \n",
    "time_taken = end_time_api_call - start_time_api_call  \n",
    "print(f'Total time taken {time_taken:.2f} seconds to complete.')  \n",
    "\n",
    "# Calculate the cumulative time  \n",
    "cumulative_time = sum(result[\"Time taken (ms)\"] for result in results)  \n",
    "cumulative_time = round(cumulative_time, 2)  # Round cumulative_time to 2 decimal places  \n",
    "  \n",
    "# Add the cumulative time to the results list as an extra row  \n",
    "results.append({\"Count\": \"Cumulative\", \"Content\": \"\", \"Token size\": \"\", \"Time taken (ms)\": cumulative_time})  \n",
    "  \n",
    "# Display results in tabular form  \n",
    "print(\"\\nResults Table:\")  \n",
    "print(tabulate([result.values() for result in results], headers=results[0].keys(), tablefmt=\"pretty\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The content for this response was already consumed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\upgrade-llm\\lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\upgrade-llm\\lib\\site-packages\\requests\\models.py:926\u001b[0m, in \u001b[0;36mResponse.text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    923\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding\n\u001b[1;32m--> 926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# Fallback to auto-detected encoding.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\upgrade-llm\\lib\\site-packages\\requests\\models.py:897\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;66;03m# Read the contents.\u001b[39;00m\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed:\n\u001b[1;32m--> 897\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe content for this response was already consumed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The content for this response was already consumed"
     ]
    }
   ],
   "source": [
    "response = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sys  \n",
    "\n",
    "\n",
    "\n",
    "base_url = \"https://myopenAI.openai.azure.com/\"\n",
    "deployment_name  = \"mydep\"\n",
    "api_key  =\"******\" \n",
    "\n",
    "\n",
    "url = base_url + \"/openai/deployments/\" + deployment_name + \"/chat/completions?api-version=2023-05-15\"\n",
    "start_phrase = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},\n",
    "                {\"role\":\"user\",\"content\":\"Write 5000 word essay on soccer\"}]\n",
    "payload = {        \n",
    "    \"messages\":start_phrase,\n",
    "    \"stream\":True,\n",
    "    \"max_tokens\":200\n",
    "    }\n",
    "\n",
    "headers = {  \n",
    "    \"api-key\": api_key,  \n",
    "    \"Content-Type\": \"application/json\"  \n",
    "} \n",
    "print(\"Begin:openai.ChatCompletion.create\")\n",
    "start_time_api_call = time.time()\n",
    "\n",
    "  \n",
    "try:  \n",
    "    with requests.post(url, headers=headers, json=payload, timeout=1, stream=True) as r:  \n",
    "        start_time = time.time()  \n",
    "        for chunk in r.iter_content(chunk_size=None, decode_unicode=True):  \n",
    "            end_time = time.time()  \n",
    "            time_taken = (end_time - start_time) * 1000  # Convert to milliseconds  \n",
    "            chunk_size = sys.getsizeof(chunk)  \n",
    "  \n",
    "            print(f\"Time taken for this chunk: {time_taken:.2f} ms\")  \n",
    "            print(f\"Size of this chunk: {chunk_size} bytes\")  \n",
    "            start_time = end_time  \n",
    "  \n",
    "except requests.exceptions.Timeout:  \n",
    "    print(\"The request has timed out. Please try again.\")  \n",
    "        \n",
    "print(\"END:openai.ChatCompletion.create\")\n",
    "end_time_api_call  = time.time()\n",
    "# Calculate the time taken and print the result\n",
    "time_taken = end_time_api_call - start_time_api_call\n",
    "print(f'Total time taken {time_taken:.2f} seconds to complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['usage']['completion_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = AzureOpenAIBenchmarkStreaming(api_key=OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=DEPLOYMENT_VERSION)\n",
    "await benchmark.make_call(deployment_name=DEPLOYMENT_ID, max_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting call to model gpt-4o-2024-05-13 with max tokens 100 at (Local time): 2024-06-26 23:18:31.243877, (GMT): 2024-06-27 04:18:31.243877+00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 471.37}\n",
      "{'Count': 2, 'Content': 'Certainly', 'Token size': 1, 'Time taken (ms)': 936.2}\n",
      "{'Count': 3, 'Content': '.', 'Token size': 1, 'Time taken (ms)': 50.16}\n",
      "{'Count': 4, 'Content': ' before', 'Token size': 1, 'Time taken (ms)': 954.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Finished call to model gpt-4o-2024-05-13. Time taken for chat: 3.29 seconds or 3293.49 milliseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Count': 5, 'Content': 'on', 'Token size': 1, 'Time taken (ms)': 798.28}\n",
      "{'Count': 6, 'Content': ' and', 'Token size': 1, 'Time taken (ms)': 53.11}\n",
      "\n",
      "Results Table:\n",
      "+-------+-----------+------------+-----------------+\n",
      "| Count |  Content  | Token size | Time taken (ms) |\n",
      "+-------+-----------+------------+-----------------+\n",
      "|   1   |           |     0      |     471.37      |\n",
      "|   2   | Certainly |     1      |      936.2      |\n",
      "|   3   |     .     |     1      |      50.16      |\n",
      "|   4   |  before   |     1      |     954.29      |\n",
      "|   5   |    on     |     1      |     798.28      |\n",
      "|   6   |    and    |     1      |      53.11      |\n",
      "|  TBT  |           |            |     558.41      |\n",
      "| TTFT  |           |            |      78.56      |\n",
      "| TTLT  |           |            |     3900.38     |\n",
      "+-------+-----------+------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "await benchmark.make_call(deployment_name=DEPLOYMENT_ID, max_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await benchmark.make_call(deployment_name=DEPLOYMENT_ID, max_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Count': 1, 'Content': '', 'Token size': 0, 'Time taken (ms)': 506.69},\n",
       " {'Count': 2, 'Content': 'The', 'Token size': 1, 'Time taken (ms)': 1280.25},\n",
       " {'Count': 3, 'Content': '###', 'Token size': 1, 'Time taken (ms)': 47.56},\n",
       " {'Count': 4, 'Content': ' Period', 'Token size': 1, 'Time taken (ms)': 804.1},\n",
       " {'Count': 5, 'Content': ' as', 'Token size': 1, 'Time taken (ms)': 47.9},\n",
       " {'Count': 6, 'Content': 'ro', 'Token size': 1, 'Time taken (ms)': 1098.75},\n",
       " {'Count': 7, 'Content': 'acy', 'Token size': 1, 'Time taken (ms)': 47.63},\n",
       " {'Count': 'TBT', 'Content': '', 'Token size': '', 'Time taken (ms)': 554.37},\n",
       " {'Count': 'TTFT', 'Content': '', 'Token size': '', 'Time taken (ms)': 84.45},\n",
       " {'Count': 'Cumulative',\n",
       "  'Content': '',\n",
       "  'Token size': '',\n",
       "  'Time taken (ms)': 4471.7}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upgrade-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
